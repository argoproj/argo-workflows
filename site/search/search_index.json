{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Argo Documentation \u00b6 Getting Started \u00b6 For set-up information and running your first Workflows, please see our Getting Started guide. Examples \u00b6 For detailed examples about what Argo can do, please see our documentation by example page. Fields \u00b6 For a full list of all the fields available in for use in Argo, and a link to examples where each is used, please see Argo Fields .","title":"Overview"},{"location":"#argo-documentation","text":"","title":"Argo Documentation"},{"location":"#getting-started","text":"For set-up information and running your first Workflows, please see our Getting Started guide.","title":"Getting Started"},{"location":"#examples","text":"For detailed examples about what Argo can do, please see our documentation by example page.","title":"Examples"},{"location":"#fields","text":"For a full list of all the fields available in for use in Argo, and a link to examples where each is used, please see Argo Fields .","title":"Fields"},{"location":"CONTRIBUTING/","text":"Contributing \u00b6 How To Provide Feedback \u00b6 Please raise an issue in Github . Code of Conduct \u00b6 See code of conduct . How To Contribute \u00b6 We're always looking for contributors. Documentation - something missing or unclear? Please submit a pull request! Code contribution - investigate a help wanted issue , or anything labelled with \"good first issue\"? Join the #argo-devs channel on our Slack . Running Locally \u00b6 To run Argo Workflows locally for development: running locally . Test Policy \u00b6 Changes without either unit or e2e tests are unlikely to be accepted. See the pull request template . Contributor Workshop \u00b6 We have a 90m video on YouTube show you have to get hands-on contributing.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"","title":"Contributing"},{"location":"CONTRIBUTING/#how-to-provide-feedback","text":"Please raise an issue in Github .","title":"How To Provide Feedback"},{"location":"CONTRIBUTING/#code-of-conduct","text":"See code of conduct .","title":"Code of Conduct"},{"location":"CONTRIBUTING/#how-to-contribute","text":"We're always looking for contributors. Documentation - something missing or unclear? Please submit a pull request! Code contribution - investigate a help wanted issue , or anything labelled with \"good first issue\"? Join the #argo-devs channel on our Slack .","title":"How To Contribute"},{"location":"CONTRIBUTING/#running-locally","text":"To run Argo Workflows locally for development: running locally .","title":"Running Locally"},{"location":"CONTRIBUTING/#test-policy","text":"Changes without either unit or e2e tests are unlikely to be accepted. See the pull request template .","title":"Test Policy"},{"location":"CONTRIBUTING/#contributor-workshop","text":"We have a 90m video on YouTube show you have to get hands-on contributing.","title":"Contributor Workshop"},{"location":"access-token/","text":"Access Token \u00b6 If you want to automate tasks with the Argo Server API or CLI, you will need an access token. Firstly, create a role with minimal permissions. This example role for jenkins only permission to update and list workflows: ```shell script kubectl create role jenkins --verb=list,update --resource=workflows.argoproj.io Create a service account for your service : ``` shell script kubectl create sa jenkins Bind the service account to the role (in this case in the argo namespace): ```shell script kubectl create rolebinding jenkins --role=jenkins --serviceaccount=argo:jenkins You now need to get a token : ``` shell script SECRET = $ ( kubectl - n argo get sa jenkins - o = jsonpath = '{.secrets[0].name}' ) ARGO_TOKEN = \"Bearer $(kubectl -n argo get secret $SECRET -o=jsonpath='{.data.token}' | base64 --decode)\" echo $ ARGO_TOKEN Bearer ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNkltS ... Note The ARGO_TOKEN should always start with \"Bearer \". Use that token with the CLI (you need to set ARGO_SERVER too): ```shell script ARGO_SERVER=http://localhost:2746 argo list Use that token in your API requests , e . g . to list workflows : ``` shell script curl https : // localhost : 2746 / api / v1 / workflows / argo - H \"Authorisation: $ARGO_TOKEN\" # 200 OK You should check you cannot do things you're not allowed! ```shell script curl https://localhost:2746/api/v1/workflow-templates/argo -H \"Authorisation: $ARGO_TOKEN\" 403 error \u00b6 ## Token Revocation Token compromised ? ``` shell script kubectl delete secret $ SECRET A new one will be created.","title":"Access Token"},{"location":"access-token/#access-token","text":"If you want to automate tasks with the Argo Server API or CLI, you will need an access token. Firstly, create a role with minimal permissions. This example role for jenkins only permission to update and list workflows: ```shell script kubectl create role jenkins --verb=list,update --resource=workflows.argoproj.io Create a service account for your service : ``` shell script kubectl create sa jenkins Bind the service account to the role (in this case in the argo namespace): ```shell script kubectl create rolebinding jenkins --role=jenkins --serviceaccount=argo:jenkins You now need to get a token : ``` shell script SECRET = $ ( kubectl - n argo get sa jenkins - o = jsonpath = '{.secrets[0].name}' ) ARGO_TOKEN = \"Bearer $(kubectl -n argo get secret $SECRET -o=jsonpath='{.data.token}' | base64 --decode)\" echo $ ARGO_TOKEN Bearer ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNkltS ... Note The ARGO_TOKEN should always start with \"Bearer \". Use that token with the CLI (you need to set ARGO_SERVER too): ```shell script ARGO_SERVER=http://localhost:2746 argo list Use that token in your API requests , e . g . to list workflows : ``` shell script curl https : // localhost : 2746 / api / v1 / workflows / argo - H \"Authorisation: $ARGO_TOKEN\" # 200 OK You should check you cannot do things you're not allowed! ```shell script curl https://localhost:2746/api/v1/workflow-templates/argo -H \"Authorisation: $ARGO_TOKEN\"","title":"Access Token"},{"location":"access-token/#403-error","text":"## Token Revocation Token compromised ? ``` shell script kubectl delete secret $ SECRET A new one will be created.","title":"403 error"},{"location":"architecture/","text":"Architecture \u00b6 Argo Workflow Overview \u00b6 Workflow controller architecture \u00b6","title":"Architecture"},{"location":"architecture/#architecture","text":"","title":"Architecture"},{"location":"architecture/#argo-workflow-overview","text":"","title":"Argo Workflow Overview"},{"location":"architecture/#workflow-controller-architecture","text":"","title":"Workflow controller architecture"},{"location":"argo-server-auth-mode/","text":"Argo Server Auth Mode \u00b6 You can choose which kube config the Argo Server uses: \"server\" - in hosted mode, use the kube config of service account, in local mode, use your local kube config. \"client\" - requires clients to provide their Kubernetes bearer token and use that. \"hybrid\" - use the client token if provided, fallback to the server token if note. \"sso\" - since v2.9, use single sign-on, this will use the same service account as per \"server\" for RBAC. We expect to change this in the future so that the OAuth claims are mapped to service accounts. By default, the server will start with auth mode of \"server\".","title":"Argo Server Auth Mode"},{"location":"argo-server-auth-mode/#argo-server-auth-mode","text":"You can choose which kube config the Argo Server uses: \"server\" - in hosted mode, use the kube config of service account, in local mode, use your local kube config. \"client\" - requires clients to provide their Kubernetes bearer token and use that. \"hybrid\" - use the client token if provided, fallback to the server token if note. \"sso\" - since v2.9, use single sign-on, this will use the same service account as per \"server\" for RBAC. We expect to change this in the future so that the OAuth claims are mapped to service accounts. By default, the server will start with auth mode of \"server\".","title":"Argo Server Auth Mode"},{"location":"argo-server-sso/","text":"Argo Server SSO \u00b6 v2.9 and after To start Argo Server with SSO. \u00b6 Firstly, configure the settings workflow-controller-configmap.yaml with the correct OAuth 2 values. Then, start the Argo Server using the SSO auth mode : argo server --auth-mode sso --auth-mode ...","title":"Argo Server SSO"},{"location":"argo-server-sso/#argo-server-sso","text":"v2.9 and after","title":"Argo Server SSO"},{"location":"argo-server-sso/#to-start-argo-server-with-sso","text":"Firstly, configure the settings workflow-controller-configmap.yaml with the correct OAuth 2 values. Then, start the Argo Server using the SSO auth mode : argo server --auth-mode sso --auth-mode ...","title":"To start Argo Server with SSO."},{"location":"argo-server/","text":"Argo Server \u00b6 v2.5 and after The Argo Server is a server that exposes an API and UI for workflows. You'll need to use this if you want to offload large workflows or the workflow archive . You can run this in either \"hosted\" or \"local\" mode. It replaces the Argo UI. Hosted Mode \u00b6 Use this mode if: You want a drop-in replacement for the Argo UI. If you need to prevent users from directly accessing the database. Hosted mode is provided as part of the standard manifests , specifically in argo-server-deployment.yaml . Local Mode \u00b6 Use this mode if: You want something that does not require complex set-up. You do not need to run a database. To run locally: argo server This will start a server on port 2746 which you can view at http://localhost:2746 . Options \u00b6 Auth Mode \u00b6 See auth . Managed Namespace \u00b6 See managed namespace . Base href \u00b6 If the server is running behind reverse proxy with a subpath different from / (for example, /argo ), you can set an alternative subpath with the --base-href flag or the BASE_HREF environment variable. Transport Layer Security \u00b6 See TLS . SSO \u00b6 See SSO . Access the Argo Workflows UI \u00b6 kubectl -n argo port-forward deployment/argo-server 2746 :2746 Then visit: http://127.0.0.1:2746 By default, the Argo UI service is not exposed with an external IP. To access the UI, use one of the following: Method 1: kubectl port-forward \u00b6 kubectl - n argo port - forward deployment / argo - server 2746 : 2746 Then visit: http://127.0.0.1:8001 Method 2: kubectl proxy \u00b6 kubectl proxy Then visit: http://127.0.0.1:8001/api/v1/namespaces/argo/services/argo-ui/proxy/ NOTE: artifact download and webconsole is not supported using this method Method 3: Expose a LoadBalancer \u00b6 Update the argo-ui service to be of type LoadBalancer . kubectl patch svc argo - ui - n argo - p '{\"spec\": {\"type\": \"LoadBalancer\"}}' Then wait for the external IP to be made available: kubectl get svc argo - ui - n argo NAME TYPE CLUSTER - IP EXTERNAL - IP PORT ( S ) AGE argo - ui LoadBalancer 10 . 19 . 255 . 205 35 . 197 . 49 . 167 80 : 30999 / TCP 1 m NOTE: On Minikube, you won't get an external IP after updating the service -- it will always show pending . Run the following command to determine the Argo UI URL: minikube service - n argo --url argo-ui","title":"Argo Server"},{"location":"argo-server/#argo-server","text":"v2.5 and after The Argo Server is a server that exposes an API and UI for workflows. You'll need to use this if you want to offload large workflows or the workflow archive . You can run this in either \"hosted\" or \"local\" mode. It replaces the Argo UI.","title":"Argo Server"},{"location":"argo-server/#hosted-mode","text":"Use this mode if: You want a drop-in replacement for the Argo UI. If you need to prevent users from directly accessing the database. Hosted mode is provided as part of the standard manifests , specifically in argo-server-deployment.yaml .","title":"Hosted Mode"},{"location":"argo-server/#local-mode","text":"Use this mode if: You want something that does not require complex set-up. You do not need to run a database. To run locally: argo server This will start a server on port 2746 which you can view at http://localhost:2746 .","title":"Local Mode"},{"location":"argo-server/#options","text":"","title":"Options"},{"location":"argo-server/#auth-mode","text":"See auth .","title":"Auth Mode"},{"location":"argo-server/#managed-namespace","text":"See managed namespace .","title":"Managed Namespace"},{"location":"argo-server/#base-href","text":"If the server is running behind reverse proxy with a subpath different from / (for example, /argo ), you can set an alternative subpath with the --base-href flag or the BASE_HREF environment variable.","title":"Base href"},{"location":"argo-server/#transport-layer-security","text":"See TLS .","title":"Transport Layer Security"},{"location":"argo-server/#sso","text":"See SSO .","title":"SSO"},{"location":"argo-server/#access-the-argo-workflows-ui","text":"kubectl -n argo port-forward deployment/argo-server 2746 :2746 Then visit: http://127.0.0.1:2746 By default, the Argo UI service is not exposed with an external IP. To access the UI, use one of the following:","title":"Access the Argo Workflows UI"},{"location":"argo-server/#method-1-kubectl-port-forward","text":"kubectl - n argo port - forward deployment / argo - server 2746 : 2746 Then visit: http://127.0.0.1:8001","title":"Method 1: kubectl port-forward"},{"location":"argo-server/#method-2-kubectl-proxy","text":"kubectl proxy Then visit: http://127.0.0.1:8001/api/v1/namespaces/argo/services/argo-ui/proxy/ NOTE: artifact download and webconsole is not supported using this method","title":"Method 2: kubectl proxy"},{"location":"argo-server/#method-3-expose-a-loadbalancer","text":"Update the argo-ui service to be of type LoadBalancer . kubectl patch svc argo - ui - n argo - p '{\"spec\": {\"type\": \"LoadBalancer\"}}' Then wait for the external IP to be made available: kubectl get svc argo - ui - n argo NAME TYPE CLUSTER - IP EXTERNAL - IP PORT ( S ) AGE argo - ui LoadBalancer 10 . 19 . 255 . 205 35 . 197 . 49 . 167 80 : 30999 / TCP 1 m NOTE: On Minikube, you won't get an external IP after updating the service -- it will always show pending . Run the following command to determine the Argo UI URL: minikube service - n argo --url argo-ui","title":"Method 3: Expose a LoadBalancer"},{"location":"artifact-repository-ref/","text":"Artifact Repository Ref \u00b6 v2.9 and after You can reduce duplication in your templates by configuring repositories that can be accessed by any workflow. This can also remove sensitive information from your templates. Create a suitable config map in either (a) your workflows namespace or (b) in the Argo's namespace, the default name is artifact-repositories : apiVersion : v1 kind : ConfigMap metadata : name : artifact - repositories data : minio : | s3 : bucket : my - bucket endpoint : minio : 9000 insecure : true accessKeySecret : name : my - minio - cred key : accesskey secretKeySecret : name : my - minio - cred key : secretkey You can override the repository for a workflow as follows: spec : artifactRepositoryRef : key : minio Reference: fields.md#artifactrepositoryref .","title":"Artifact Repository Ref"},{"location":"artifact-repository-ref/#artifact-repository-ref","text":"v2.9 and after You can reduce duplication in your templates by configuring repositories that can be accessed by any workflow. This can also remove sensitive information from your templates. Create a suitable config map in either (a) your workflows namespace or (b) in the Argo's namespace, the default name is artifact-repositories : apiVersion : v1 kind : ConfigMap metadata : name : artifact - repositories data : minio : | s3 : bucket : my - bucket endpoint : minio : 9000 insecure : true accessKeySecret : name : my - minio - cred key : accesskey secretKeySecret : name : my - minio - cred key : secretkey You can override the repository for a workflow as follows: spec : artifactRepositoryRef : key : minio Reference: fields.md#artifactrepositoryref .","title":"Artifact Repository Ref"},{"location":"async-pattern/","text":"Asynchronous Job Pattern \u00b6 Introduction \u00b6 If triggering an external job (eg an Amazon EMR job) from Argo that does not run to completion in a container, there are two options: create a container that polls the external job completion status combine a trigger step that starts the job with a Suspend step that is unsuspended by an API call to Argo when the external job is complete. This document describes the second option in more detail. The pattern \u00b6 The pattern involves two steps - the first step is a short-running step that triggers a long-running job outside Argo (eg an HTTP submission), and the second step is a Suspend step that suspends workflow exection and is ultimately either resumed or stopped (ie failed) via a call to the Argo API when the job outside Argo succeeds or fails. When implemented as a WorkflowTemplate it can look something like this: apiVersion : argoproj . io / v1alpha1 kind : WorkflowTemplate metadata : name : external - job - template spec : templates : - name : run - external - job inputs : parameters : - name : \" job-cmd \" steps : - - name : trigger - job template : trigger - job arguments : parameters : - name : \" job-cmd \" value : \" {{inputs.parameters.job-cmd}} \" - - name : wait - completion template : wait - completion arguments : parameters : - name : uuid value : \" {{steps.trigger-job.outputs.result}} \" - name : trigger - job inputs : parameters : - name : \" job-cmd \" value : \" {{inputs.parameters.job-cmd}} \" image : appropriate / curl : latest command : [ \" /bin/sh \" , \" -c \" ] args : [ \" {{inputs.parameters.cmd}} \" ] - name : wait - completion inputs : parameters : - name : uuid suspend : {} In this case the job-cmd parameter can be a command that makes an http call via curl to an endpoint that returns a job uuid. More sophisticated submission and parsing of submission output could be done with something like a Python script step. On job completion the external job would need to call either resume if successful: You may need an access token . curl -- request PUT \\ -- url http : // localhost : 2746 / api / v1 / workflows /< NAMESPACE >/< WORKFLOWNAME >/ resume -- header ' content-type: application/json ' \\ -- header \" Authorization: Bearer $ARGO_TOKEN \" \\ -- data ' { \" namespace \" : \" <NAMESPACE> \" , \" name \" : \" <WORKFLOWNAME> \" , \" nodeFieldSelector \" : \" inputs.parameters.uuid.value=<UUID> \" } ' ``` or stop if unsuccessful : curl --request PUT \\ --url http://localhost:2746/api/v1/workflows/ / /stop --header 'content-type: application/json' \\ --header \"Authorization: Bearer $ARGO_TOKEN\" \\ --data '{ \"namespace\": \" \", \"name\": \" \", \"nodeFieldSelector\": \"inputs.parameters.uuid.value= \", \"message\": \" \" }' ``` Retrying failed jobs \u00b6 Using argo retry on failed jobs that follow this pattern will cause Argo to re-attempt the Suspend step without re-triggering the job. Instead you need to use the --restart-successful option, eg if using the template from above: argo retry < WORKFLOWNAME > --restart-successful --node-field-selector templateRef.template=run-external-job,phase=Failed See also: access token resuming a workflow via automation submitting a workflow via automation one workflow submitting another","title":"Asynchronous Job Pattern"},{"location":"async-pattern/#asynchronous-job-pattern","text":"","title":"Asynchronous Job Pattern"},{"location":"async-pattern/#introduction","text":"If triggering an external job (eg an Amazon EMR job) from Argo that does not run to completion in a container, there are two options: create a container that polls the external job completion status combine a trigger step that starts the job with a Suspend step that is unsuspended by an API call to Argo when the external job is complete. This document describes the second option in more detail.","title":"Introduction"},{"location":"async-pattern/#the-pattern","text":"The pattern involves two steps - the first step is a short-running step that triggers a long-running job outside Argo (eg an HTTP submission), and the second step is a Suspend step that suspends workflow exection and is ultimately either resumed or stopped (ie failed) via a call to the Argo API when the job outside Argo succeeds or fails. When implemented as a WorkflowTemplate it can look something like this: apiVersion : argoproj . io / v1alpha1 kind : WorkflowTemplate metadata : name : external - job - template spec : templates : - name : run - external - job inputs : parameters : - name : \" job-cmd \" steps : - - name : trigger - job template : trigger - job arguments : parameters : - name : \" job-cmd \" value : \" {{inputs.parameters.job-cmd}} \" - - name : wait - completion template : wait - completion arguments : parameters : - name : uuid value : \" {{steps.trigger-job.outputs.result}} \" - name : trigger - job inputs : parameters : - name : \" job-cmd \" value : \" {{inputs.parameters.job-cmd}} \" image : appropriate / curl : latest command : [ \" /bin/sh \" , \" -c \" ] args : [ \" {{inputs.parameters.cmd}} \" ] - name : wait - completion inputs : parameters : - name : uuid suspend : {} In this case the job-cmd parameter can be a command that makes an http call via curl to an endpoint that returns a job uuid. More sophisticated submission and parsing of submission output could be done with something like a Python script step. On job completion the external job would need to call either resume if successful: You may need an access token . curl -- request PUT \\ -- url http : // localhost : 2746 / api / v1 / workflows /< NAMESPACE >/< WORKFLOWNAME >/ resume -- header ' content-type: application/json ' \\ -- header \" Authorization: Bearer $ARGO_TOKEN \" \\ -- data ' { \" namespace \" : \" <NAMESPACE> \" , \" name \" : \" <WORKFLOWNAME> \" , \" nodeFieldSelector \" : \" inputs.parameters.uuid.value=<UUID> \" } ' ``` or stop if unsuccessful : curl --request PUT \\ --url http://localhost:2746/api/v1/workflows/ / /stop --header 'content-type: application/json' \\ --header \"Authorization: Bearer $ARGO_TOKEN\" \\ --data '{ \"namespace\": \" \", \"name\": \" \", \"nodeFieldSelector\": \"inputs.parameters.uuid.value= \", \"message\": \" \" }' ```","title":"The pattern"},{"location":"async-pattern/#retrying-failed-jobs","text":"Using argo retry on failed jobs that follow this pattern will cause Argo to re-attempt the Suspend step without re-triggering the job. Instead you need to use the --restart-successful option, eg if using the template from above: argo retry < WORKFLOWNAME > --restart-successful --node-field-selector templateRef.template=run-external-job,phase=Failed See also: access token resuming a workflow via automation submitting a workflow via automation one workflow submitting another","title":"Retrying failed jobs"},{"location":"cli/","text":"CLI \u00b6 The CLI allows to (amongst other things) submit, watch, and list workflows, e.g.: argo submit my-wf.yaml argo list Reference \u00b6 You can find detailed reference here Help \u00b6 Most help topics are provided by built-in help: argo --help Argo Server \u00b6 You'll need to configure your commands to use the Argo Server if you have offloaded node status or are trying to access your workflow archive . To do so, set the ARGO_SERVER environment variable, e.g.: export ARGO_SERVER = localhost : 2746 See TLS .","title":"CLI"},{"location":"cli/#cli","text":"The CLI allows to (amongst other things) submit, watch, and list workflows, e.g.: argo submit my-wf.yaml argo list","title":"CLI"},{"location":"cli/#reference","text":"You can find detailed reference here","title":"Reference"},{"location":"cli/#help","text":"Most help topics are provided by built-in help: argo --help","title":"Help"},{"location":"cli/#argo-server","text":"You'll need to configure your commands to use the Argo Server if you have offloaded node status or are trying to access your workflow archive . To do so, set the ARGO_SERVER environment variable, e.g.: export ARGO_SERVER = localhost : 2746 See TLS .","title":"Argo Server"},{"location":"cluster-workflow-templates/","text":"Cluster Workflow Templates \u00b6 v2.8 and after Introduction \u00b6 ClusterWorkflowTemplates are cluster scoped WorkflowTemplates . ClusterWorkflowTemplate can be created cluster scoped like ClusterRole and can be accessed all namespaces in the cluster. WorkflowTemplates documentation link Defining ClusterWorkflowTemplate \u00b6 apiVersion : argoproj.io/v1alpha1 kind : ClusterWorkflowTemplate metadata : name : cluster-workflow-template-whalesay-template spec : templates : - name : whalesay-template inputs : parameters : - name : message container : image : docker/whalesay command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] Referencing other ClusterWorkflowTemplates \u00b6 You can reference templates from another ClusterWorkflowTemplates using a templateRef field with clusterScope: true . Just as how you reference other templates within the same Workflow , you should do so from a steps or dag template. Here is an example: More examples apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : workflow-template-hello-world- spec : entrypoint : whalesay templates : - name : whalesay steps : # You should only reference external \"templates\" in a \"steps\" or \"dag\" \"template\". - - name : call-whalesay-template templateRef : # You can reference a \"template\" from another \"WorkflowTemplate or ClusterWorkflowTemplate\" using this field name : cluster-workflow-template-whalesay-template # This is the name of the \"WorkflowTemplate or ClusterWorkflowTemplate\" CRD that contains the \"template\" you want template : whalesay-template # This is the name of the \"template\" you want to reference clusterScope : true # This field indicates this templateRef is pointing ClusterWorkflowTemplate arguments : # You can pass in arguments as normal parameters : - name : message value : \"hello world\" 2.9 and after Create Workflow from ClusterWorkflowTemplate Spec \u00b6 You can create Workflow from ClusterWorkflowTemplate spec using workflowTemplateRef with clusterScope: true . If you pass the arguments to created Workflow , it will be merged with ClusterWorkflowTemplate arguments Here is an example for ClusterWorkflowTemplate with entrypoint and arguments apiVersion : argoproj.io/v1alpha1 kind : ClusterWorkflowTemplate metadata : name : cluster-workflow-template-submittable spec : entryPoint : whalesay-template arguments : parameters : - name : message value : hello world templates : - name : whalesay-template inputs : parameters : - name : message container : image : docker/whalesay command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] Here is an example for creating ClusterWorkflowTemplate as Workflow with passing entrypoint and arguments to ClusterWorkflowTemplate apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : cluster-workflow-template-hello-world- spec : entrypoint : whalesay-template arguments : parameters : - name : message value : \"from workflow\" workflowTemplateRef : name : cluster-workflow-template-submittable clusterScope : true Here is an example of a creating WorkflowTemplate as Workflow and using WorkflowTemplates 's entrypoint and Workflow Arguments apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : cluster-workflow-template-hello-world- spec : workflowTemplateRef : name : cluster-workflow-template-submittable clusterScope : true Managing ClusterWorkflowTemplates \u00b6 CLI \u00b6 You can create some example templates as follows: argo cluster - template create https : // raw . githubusercontent . com / argoproj / argo / master / examples / cluster - workflow - template / clustertemplates . yaml The submit a workflow using one of those templates: argo submit https : // raw . githubusercontent . com / argoproj / argo / master / examples / cluster - workflow - template / cluster - wftmpl - dag . yaml 2.7 and after The submit a ClusterWorkflowTemplate as a Workflow : argo submit --from clusterworkflowtemplate/workflow-template-submittable kubectl \u00b6 Using kubectl apply -f and kubectl get cwft UI \u00b6 ClusterWorkflowTemplate resources can also be managed by the UI","title":"Cluster Workflow Templates"},{"location":"cluster-workflow-templates/#cluster-workflow-templates","text":"v2.8 and after","title":"Cluster Workflow Templates"},{"location":"cluster-workflow-templates/#introduction","text":"ClusterWorkflowTemplates are cluster scoped WorkflowTemplates . ClusterWorkflowTemplate can be created cluster scoped like ClusterRole and can be accessed all namespaces in the cluster. WorkflowTemplates documentation link","title":"Introduction"},{"location":"cluster-workflow-templates/#defining-clusterworkflowtemplate","text":"apiVersion : argoproj.io/v1alpha1 kind : ClusterWorkflowTemplate metadata : name : cluster-workflow-template-whalesay-template spec : templates : - name : whalesay-template inputs : parameters : - name : message container : image : docker/whalesay command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ]","title":"Defining ClusterWorkflowTemplate"},{"location":"cluster-workflow-templates/#referencing-other-clusterworkflowtemplates","text":"You can reference templates from another ClusterWorkflowTemplates using a templateRef field with clusterScope: true . Just as how you reference other templates within the same Workflow , you should do so from a steps or dag template. Here is an example: More examples apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : workflow-template-hello-world- spec : entrypoint : whalesay templates : - name : whalesay steps : # You should only reference external \"templates\" in a \"steps\" or \"dag\" \"template\". - - name : call-whalesay-template templateRef : # You can reference a \"template\" from another \"WorkflowTemplate or ClusterWorkflowTemplate\" using this field name : cluster-workflow-template-whalesay-template # This is the name of the \"WorkflowTemplate or ClusterWorkflowTemplate\" CRD that contains the \"template\" you want template : whalesay-template # This is the name of the \"template\" you want to reference clusterScope : true # This field indicates this templateRef is pointing ClusterWorkflowTemplate arguments : # You can pass in arguments as normal parameters : - name : message value : \"hello world\" 2.9 and after","title":"Referencing other ClusterWorkflowTemplates"},{"location":"cluster-workflow-templates/#create-workflow-from-clusterworkflowtemplate-spec","text":"You can create Workflow from ClusterWorkflowTemplate spec using workflowTemplateRef with clusterScope: true . If you pass the arguments to created Workflow , it will be merged with ClusterWorkflowTemplate arguments Here is an example for ClusterWorkflowTemplate with entrypoint and arguments apiVersion : argoproj.io/v1alpha1 kind : ClusterWorkflowTemplate metadata : name : cluster-workflow-template-submittable spec : entryPoint : whalesay-template arguments : parameters : - name : message value : hello world templates : - name : whalesay-template inputs : parameters : - name : message container : image : docker/whalesay command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] Here is an example for creating ClusterWorkflowTemplate as Workflow with passing entrypoint and arguments to ClusterWorkflowTemplate apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : cluster-workflow-template-hello-world- spec : entrypoint : whalesay-template arguments : parameters : - name : message value : \"from workflow\" workflowTemplateRef : name : cluster-workflow-template-submittable clusterScope : true Here is an example of a creating WorkflowTemplate as Workflow and using WorkflowTemplates 's entrypoint and Workflow Arguments apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : cluster-workflow-template-hello-world- spec : workflowTemplateRef : name : cluster-workflow-template-submittable clusterScope : true","title":"Create Workflow from ClusterWorkflowTemplate Spec"},{"location":"cluster-workflow-templates/#managing-clusterworkflowtemplates","text":"","title":"Managing ClusterWorkflowTemplates"},{"location":"cluster-workflow-templates/#cli","text":"You can create some example templates as follows: argo cluster - template create https : // raw . githubusercontent . com / argoproj / argo / master / examples / cluster - workflow - template / clustertemplates . yaml The submit a workflow using one of those templates: argo submit https : // raw . githubusercontent . com / argoproj / argo / master / examples / cluster - workflow - template / cluster - wftmpl - dag . yaml 2.7 and after The submit a ClusterWorkflowTemplate as a Workflow : argo submit --from clusterworkflowtemplate/workflow-template-submittable","title":"CLI"},{"location":"cluster-workflow-templates/#kubectl","text":"Using kubectl apply -f and kubectl get cwft","title":"kubectl"},{"location":"cluster-workflow-templates/#ui","text":"ClusterWorkflowTemplate resources can also be managed by the UI","title":"UI"},{"location":"configure-artifact-repository/","text":"Configuring Your Artifact Repository \u00b6 To run Argo workflows that use artifacts, you must configure and use an artifact repository. Argo supports any S3 compatible artifact repository such as AWS, GCS and Minio. This section shows how to configure the artifact repository. Subsequent sections will show how to use it. Name Inputs Outputs Usage (Feb 2020) Artifactory Yes Yes 11% GCS Yes Yes - Git Yes No - HDFS Yes Yes 3% HTTP Yes No 2% OSS Yes Yes - Raw Yes No 5% S3 Yes Yes 86% Configuring Minio \u00b6 $ brew install helm # mac, helm 3.x $ helm repo add stable https://kubernetes-charts.storage.googleapis.com/ # official Helm stable charts $ helm repo update $ helm install argo-artifacts stable/minio --set service.type = LoadBalancer --set fullnameOverride = argo-artifacts Login to the Minio UI using a web browser (port 9000) after obtaining the external IP using kubectl . $ kubectl get service argo-artifacts On Minikube: $ minikube service --url argo-artifacts NOTE: When minio is installed via Helm, it uses the following hard-wired default credentials, which you will use to login to the UI: AccessKey: AKIAIOSFODNN7EXAMPLE SecretKey: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Create a bucket named my-bucket from the Minio UI. Configuring AWS S3 \u00b6 Create your bucket and access keys for the bucket. AWS access keys have the same permissions as the user they are associated with. In particular, you cannot create access keys with reduced scope. If you want to limit the permissions for an access key, you will need to create a user with just the permissions you want to associate with the access key. Otherwise, you can just create an access key using your existing user account. $ export mybucket = bucket249 $ cat > policy.json <<EOF { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:PutObject\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::$mybucket/*\" } ] } EOF $ aws s3 mb s3:// $mybucket [ --region xxx ] $ aws iam create-user --user-name $mybucket -user $ aws iam put-user-policy --user-name $mybucket -user --policy-name $mybucket -policy --policy-document file://policy.json $ aws iam create-access-key --user-name $mybucket -user > access-key.json NOTE: if you want argo to figure out which region your buckets belong in, you must additionally set the following statement policy. Otherwise, you must specify a bucket region in your workflow configuration. ... { \"Effect\" : \"Allow\" , \"Action\" :[ \"s3:GetBucketLocation\" ], \"Resource\" : \"arn:aws:s3:::*\" } ... Configuring GCS (Google Cloud Storage) \u00b6 Create a bucket from the GCP Console (https://console.cloud.google.com/storage/browser). There are 2 ways to configure a Google Cloud Storage. Through Native GCS APIs \u00b6 Create and download a Google Cloud service account key. Create a kubernetes secret to store the key. Configure gcs artifact as following in the yaml. artifacts : - name : message path : /tmp/message gcs : bucket : my-bucket-name key : path/in/bucket # serviceAccountKeySecret is a secret selector. # It references the k8s secret named 'my-gcs-credentials'. # This secret is expected to have have the key 'serviceAccountKey', # containing the base64 encoded credentials # to the bucket. # # If it's running on GKE and Workload Identity is used, # serviceAccountKeySecret is not needed. serviceAccountKeySecret : name : my-gcs-credentials key : serviceAccountKey If it's a GKE cluster, and Workload Identity is configured, there's no need to create the Service Account key and store it as a K8s secret, serviceAccountKeySecret is also not needed in this case. Please follow the link to configure Workload Identity (https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity). Use S3 APIs \u00b6 Enable S3 compatible access and create an access key. Note that S3 compatible access is on a per project rather than per bucket basis. Navigate to Storage > Settings (https://console.cloud.google.com/storage/settings). Enable interoperability access if needed. Create a new key if needed. Confiture s3 artifact as following exmaple. artifacts : - name : my-output-artifact path : /my-ouput-artifact s3 : endpoint : storage.googleapis.com bucket : my-gcs-bucket-name # NOTE that, by default, all output artifacts are automatically tarred and # gzipped before saving. So as a best practice, .tgz or .tar.gz # should be incorporated into the key name so the resulting file # has an accurate file extension. key : path/in/bucket/my-output-artifact.tgz accessKeySecret : name : my-gcs-s3-credentials key : accessKey secretKeySecret : name : my-gcs-s3-credentials key : secretKey Configure the Default Artifact Repository \u00b6 In order for Argo to use your artifact repository, you can configure it as the default repository. Edit the workflow-controller config map with the correct endpoint and access/secret keys for your repository. S3 compatible artifact repository bucket (such as AWS, GCS and Minio) \u00b6 Use the endpoint corresponding to your S3 provider: AWS: s3.amazonaws.com GCS: storage.googleapis.com Minio: my-minio-endpoint.default:9000 The key is name of the object in the bucket The accessKeySecret and secretKeySecret are secret selectors that reference the specified kubernetes secret. The secret is expected to have the keys 'accessKey' and 'secretKey', containing the base64 encoded credentials to the bucket. For AWS, the accessKeySecret and secretKeySecret correspond to AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY respectively. EC2 provides a metadata API via which applications using the AWS SDK may assume IAM roles associated with the instance. If you are running argo on EC2 and the instance role allows access to your S3 bucket, you can configure the workflow step pods to assume the role. To do so, simply omit the accessKeySecret and secretKeySecret fields. For GCS, the accessKeySecret and secretKeySecret for S3 compatible access can be obtained from the GCP Console. Note that S3 compatible access is on a per project rather than per bucket basis. Navigate to Storage > Settings (https://console.cloud.google.com/storage/settings). Enable interoperability access if needed. Create a new key if needed. For Minio, the accessKeySecret and secretKeySecret naturally correspond the AccessKey and SecretKey. Example: $ kubectl edit configmap workflow - controller - configmap - n argo # assumes argo was installed in the argo namespace ... data : artifactRepository : | s3 : bucket : my - bucket keyFormat : prefix / in / bucket # optional endpoint : my - minio - endpoint . default : 9000 # AWS => s3 . amazonaws . com ; GCS => storage.googleapis.com insecure : true # omit for S3 / GCS . Needed when minio runs without TLS accessKeySecret : # omit if accessing via AWS IAM name : my - minio - cred key : accessKey secretKeySecret : # omit if accessing via AWS IAM name : my - minio - cred key : secretKey useSDKCreds : true # tells argo to use AWS SDK ' s default provider chain, enable for things like IRSA support The secrets are retrieved from the namespace you use to run your workflows. Note that you can specify a keyFormat . Google Cloud Storage (GCS) \u00b6 Argo also can use native GCS APIs to access a Google Cloud Storage bucket. serviceAccountKeySecret refereces to a k8 secret which stores a Google Cloud service account key to access the bucket. Example: $ kubectl edit configmap workflow-controller-configmap -n argo # assumes argo was installed in the argo namespace ... data: artifactRepository: | gcs: bucket: my-bucket keyFormat: prefix/in/bucket #optional, it could reference workflow variables, such as \"{{workflow.name}}/{{pod.name}}\" serviceAccountKeySecret: name: my-gcs-credentials key: serviceAccountKey Accessing Non-Default Artifact Repositories \u00b6 This section shows how to access artifacts from non-default artifact repositories. The endpoint , accessKeySecret and secretKeySecret are the same as for configuring the default artifact repository described previously. templates : - name : artifact - example inputs : artifacts : - name : my - input - artifact path : / my - input - artifact s3 : endpoint : s3 . amazonaws . com bucket : my - aws - bucket - name key : path / in / bucket / my - input - artifact . tgz accessKeySecret : name : my - aws - s3 - credentials key : accessKey secretKeySecret : name : my - aws - s3 - credentials key : secretKey outputs : artifacts : - name : my - output - artifact path : / my - ouput - artifact s3 : endpoint : storage . googleapis . com bucket : my - gcs - bucket - name # NOTE that , by default , all output artifacts are automatically tarred and # gzipped before saving . So as a best practice , . tgz or . tar . gz # should be incorporated into the key name so the resulting file # has an accurate file extension . key : path / in / bucket / my - output - artifact . tgz accessKeySecret : name : my - gcs - s3 - credentials key : accessKey secretKeySecret : name : my - gcs - s3 - credentials key : secretKey region : my - GCS - storage - bucket - region container : image : debian : latest command : [ sh , - c ] args : [ \"cp -r /my-input-artifact /my-output-artifact\" ]","title":"Configuring Your Artifact Repository"},{"location":"configure-artifact-repository/#configuring-your-artifact-repository","text":"To run Argo workflows that use artifacts, you must configure and use an artifact repository. Argo supports any S3 compatible artifact repository such as AWS, GCS and Minio. This section shows how to configure the artifact repository. Subsequent sections will show how to use it. Name Inputs Outputs Usage (Feb 2020) Artifactory Yes Yes 11% GCS Yes Yes - Git Yes No - HDFS Yes Yes 3% HTTP Yes No 2% OSS Yes Yes - Raw Yes No 5% S3 Yes Yes 86%","title":"Configuring Your Artifact Repository"},{"location":"configure-artifact-repository/#configuring-minio","text":"$ brew install helm # mac, helm 3.x $ helm repo add stable https://kubernetes-charts.storage.googleapis.com/ # official Helm stable charts $ helm repo update $ helm install argo-artifacts stable/minio --set service.type = LoadBalancer --set fullnameOverride = argo-artifacts Login to the Minio UI using a web browser (port 9000) after obtaining the external IP using kubectl . $ kubectl get service argo-artifacts On Minikube: $ minikube service --url argo-artifacts NOTE: When minio is installed via Helm, it uses the following hard-wired default credentials, which you will use to login to the UI: AccessKey: AKIAIOSFODNN7EXAMPLE SecretKey: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Create a bucket named my-bucket from the Minio UI.","title":"Configuring Minio"},{"location":"configure-artifact-repository/#configuring-aws-s3","text":"Create your bucket and access keys for the bucket. AWS access keys have the same permissions as the user they are associated with. In particular, you cannot create access keys with reduced scope. If you want to limit the permissions for an access key, you will need to create a user with just the permissions you want to associate with the access key. Otherwise, you can just create an access key using your existing user account. $ export mybucket = bucket249 $ cat > policy.json <<EOF { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Action\":[ \"s3:PutObject\", \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::$mybucket/*\" } ] } EOF $ aws s3 mb s3:// $mybucket [ --region xxx ] $ aws iam create-user --user-name $mybucket -user $ aws iam put-user-policy --user-name $mybucket -user --policy-name $mybucket -policy --policy-document file://policy.json $ aws iam create-access-key --user-name $mybucket -user > access-key.json NOTE: if you want argo to figure out which region your buckets belong in, you must additionally set the following statement policy. Otherwise, you must specify a bucket region in your workflow configuration. ... { \"Effect\" : \"Allow\" , \"Action\" :[ \"s3:GetBucketLocation\" ], \"Resource\" : \"arn:aws:s3:::*\" } ...","title":"Configuring AWS S3"},{"location":"configure-artifact-repository/#configuring-gcs-google-cloud-storage","text":"Create a bucket from the GCP Console (https://console.cloud.google.com/storage/browser). There are 2 ways to configure a Google Cloud Storage.","title":"Configuring GCS (Google Cloud Storage)"},{"location":"configure-artifact-repository/#through-native-gcs-apis","text":"Create and download a Google Cloud service account key. Create a kubernetes secret to store the key. Configure gcs artifact as following in the yaml. artifacts : - name : message path : /tmp/message gcs : bucket : my-bucket-name key : path/in/bucket # serviceAccountKeySecret is a secret selector. # It references the k8s secret named 'my-gcs-credentials'. # This secret is expected to have have the key 'serviceAccountKey', # containing the base64 encoded credentials # to the bucket. # # If it's running on GKE and Workload Identity is used, # serviceAccountKeySecret is not needed. serviceAccountKeySecret : name : my-gcs-credentials key : serviceAccountKey If it's a GKE cluster, and Workload Identity is configured, there's no need to create the Service Account key and store it as a K8s secret, serviceAccountKeySecret is also not needed in this case. Please follow the link to configure Workload Identity (https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity).","title":"Through Native GCS APIs"},{"location":"configure-artifact-repository/#use-s3-apis","text":"Enable S3 compatible access and create an access key. Note that S3 compatible access is on a per project rather than per bucket basis. Navigate to Storage > Settings (https://console.cloud.google.com/storage/settings). Enable interoperability access if needed. Create a new key if needed. Confiture s3 artifact as following exmaple. artifacts : - name : my-output-artifact path : /my-ouput-artifact s3 : endpoint : storage.googleapis.com bucket : my-gcs-bucket-name # NOTE that, by default, all output artifacts are automatically tarred and # gzipped before saving. So as a best practice, .tgz or .tar.gz # should be incorporated into the key name so the resulting file # has an accurate file extension. key : path/in/bucket/my-output-artifact.tgz accessKeySecret : name : my-gcs-s3-credentials key : accessKey secretKeySecret : name : my-gcs-s3-credentials key : secretKey","title":"Use S3 APIs"},{"location":"configure-artifact-repository/#configure-the-default-artifact-repository","text":"In order for Argo to use your artifact repository, you can configure it as the default repository. Edit the workflow-controller config map with the correct endpoint and access/secret keys for your repository.","title":"Configure the Default Artifact Repository"},{"location":"configure-artifact-repository/#s3-compatible-artifact-repository-bucket-such-as-aws-gcs-and-minio","text":"Use the endpoint corresponding to your S3 provider: AWS: s3.amazonaws.com GCS: storage.googleapis.com Minio: my-minio-endpoint.default:9000 The key is name of the object in the bucket The accessKeySecret and secretKeySecret are secret selectors that reference the specified kubernetes secret. The secret is expected to have the keys 'accessKey' and 'secretKey', containing the base64 encoded credentials to the bucket. For AWS, the accessKeySecret and secretKeySecret correspond to AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY respectively. EC2 provides a metadata API via which applications using the AWS SDK may assume IAM roles associated with the instance. If you are running argo on EC2 and the instance role allows access to your S3 bucket, you can configure the workflow step pods to assume the role. To do so, simply omit the accessKeySecret and secretKeySecret fields. For GCS, the accessKeySecret and secretKeySecret for S3 compatible access can be obtained from the GCP Console. Note that S3 compatible access is on a per project rather than per bucket basis. Navigate to Storage > Settings (https://console.cloud.google.com/storage/settings). Enable interoperability access if needed. Create a new key if needed. For Minio, the accessKeySecret and secretKeySecret naturally correspond the AccessKey and SecretKey. Example: $ kubectl edit configmap workflow - controller - configmap - n argo # assumes argo was installed in the argo namespace ... data : artifactRepository : | s3 : bucket : my - bucket keyFormat : prefix / in / bucket # optional endpoint : my - minio - endpoint . default : 9000 # AWS => s3 . amazonaws . com ; GCS => storage.googleapis.com insecure : true # omit for S3 / GCS . Needed when minio runs without TLS accessKeySecret : # omit if accessing via AWS IAM name : my - minio - cred key : accessKey secretKeySecret : # omit if accessing via AWS IAM name : my - minio - cred key : secretKey useSDKCreds : true # tells argo to use AWS SDK ' s default provider chain, enable for things like IRSA support The secrets are retrieved from the namespace you use to run your workflows. Note that you can specify a keyFormat .","title":"S3 compatible artifact repository bucket (such as AWS, GCS and Minio)"},{"location":"configure-artifact-repository/#google-cloud-storage-gcs","text":"Argo also can use native GCS APIs to access a Google Cloud Storage bucket. serviceAccountKeySecret refereces to a k8 secret which stores a Google Cloud service account key to access the bucket. Example: $ kubectl edit configmap workflow-controller-configmap -n argo # assumes argo was installed in the argo namespace ... data: artifactRepository: | gcs: bucket: my-bucket keyFormat: prefix/in/bucket #optional, it could reference workflow variables, such as \"{{workflow.name}}/{{pod.name}}\" serviceAccountKeySecret: name: my-gcs-credentials key: serviceAccountKey","title":"Google Cloud Storage (GCS)"},{"location":"configure-artifact-repository/#accessing-non-default-artifact-repositories","text":"This section shows how to access artifacts from non-default artifact repositories. The endpoint , accessKeySecret and secretKeySecret are the same as for configuring the default artifact repository described previously. templates : - name : artifact - example inputs : artifacts : - name : my - input - artifact path : / my - input - artifact s3 : endpoint : s3 . amazonaws . com bucket : my - aws - bucket - name key : path / in / bucket / my - input - artifact . tgz accessKeySecret : name : my - aws - s3 - credentials key : accessKey secretKeySecret : name : my - aws - s3 - credentials key : secretKey outputs : artifacts : - name : my - output - artifact path : / my - ouput - artifact s3 : endpoint : storage . googleapis . com bucket : my - gcs - bucket - name # NOTE that , by default , all output artifacts are automatically tarred and # gzipped before saving . So as a best practice , . tgz or . tar . gz # should be incorporated into the key name so the resulting file # has an accurate file extension . key : path / in / bucket / my - output - artifact . tgz accessKeySecret : name : my - gcs - s3 - credentials key : accessKey secretKeySecret : name : my - gcs - s3 - credentials key : secretKey region : my - GCS - storage - bucket - region container : image : debian : latest command : [ sh , - c ] args : [ \"cp -r /my-input-artifact /my-output-artifact\" ]","title":"Accessing Non-Default Artifact Repositories"},{"location":"core-concepts/","text":"Core Concepts \u00b6 Note Please read Kubernetes concepts first. Workflow : a Kubernetes resource defining the execution of one or more template . Workflows are named. Template : a step , steps or dag . Step : a single step of a workflow , typically run a container based on inputs and capture the outputs . Steps : a list of steps Entrypoint : the first step to execute when running a workflow Node : a step Directed Acyclic Graph (DAG) : a set of steps (nodes) and the dependencies (edges) between them. Workflow Template : a Kubernetes resource defining a reusable workflow for a namespace Cluster Workflow Template : a Kubernetes resource defining a reusable workflow for a cluster Inputs : parameters and artifacts passed to the step , Outputs : parameters and artifacts outputed by a step Parameters : objects, strings, booleans, arrays Artifacts : files saved by a container Artifact Repository : a place where artifacts are stored Executor : the method to execute a container, e.g. Docker, PNS ( learn more ) Workflow Service Account : the service account that a workflow is executed as ( learn more )","title":"Core Concepts"},{"location":"core-concepts/#core-concepts","text":"Note Please read Kubernetes concepts first. Workflow : a Kubernetes resource defining the execution of one or more template . Workflows are named. Template : a step , steps or dag . Step : a single step of a workflow , typically run a container based on inputs and capture the outputs . Steps : a list of steps Entrypoint : the first step to execute when running a workflow Node : a step Directed Acyclic Graph (DAG) : a set of steps (nodes) and the dependencies (edges) between them. Workflow Template : a Kubernetes resource defining a reusable workflow for a namespace Cluster Workflow Template : a Kubernetes resource defining a reusable workflow for a cluster Inputs : parameters and artifacts passed to the step , Outputs : parameters and artifacts outputed by a step Parameters : objects, strings, booleans, arrays Artifacts : files saved by a container Artifact Repository : a place where artifacts are stored Executor : the method to execute a container, e.g. Docker, PNS ( learn more ) Workflow Service Account : the service account that a workflow is executed as ( learn more )","title":"Core Concepts"},{"location":"cost-optimisation/","text":"Cost Optimisation \u00b6 User Cost Optimisations \u00b6 Suggestions for users running workflows. Set The Workflows Pod Resource Requests \u00b6 Suitable if you are running a workflow with many homogenous pods. Resource duration shows the amount of CPU and memory requested by a pod and is indicative of the cost. You can use this to find costly steps within your workflow. Smaller requests can be set in the pod spec patch's resource requirements . Use A Node Selector To Use Cheaper Instances \u00b6 You can use a node selector for cheaper instances, e.g. spot instances: nodeSelector : \"node-role.kubernetes.io/argo-spot-worker\" : \"true\" Consider trying Volume Claim Templates or Volumes instead of Artifacts \u00b6 Suitable if you have a workflow that passes a lot of artifacts within itself. Copying artifacts to and from storage outside of a cluster can be expensive. The correct choice is dependent on your artifact storage provider is vs. what volume they are using. For example, we believe it may be more expensive to allocate and delete new EBS volumes every workflow using the PVC feature, than it is to upload and download some small files to S3. On the other hand if they are using a NFS volume shared between all their workflows with large artifacts, that might be cheaper than the data transfer and storage costs of S3. Consider: Data transfer costs (upload/download vs. copying) Data storage costs (s3 vs. volume) Requirement for parallel access to data (NFS vs. EBS vs. artifact) Limit The Total Number Of Workflows And Pods \u00b6 Suitable for all. A workflow (and for that matter, any Kubernetes resource) will incur a cost as long as they exist in your cluster. The workflow controller memory and CPU needs increase linearly with the number of pods and workflows you are currently running. Limit the total number of workflows using: Active Deadline Seconds - terminate running workflows that do not complete in a set time. This will make sure workflows do not run forever. Workflow TTL Strategy - delete completed workflows after a time Pod GC - delete completed pods after a time Example spec : # must complete in 8h (28,800 seconds) activeDeadlineSeconds : 28800 # keep workflows for 1d (86,400 seconds) ttlStrategy : secondsAfterCompletion : 86400 # delete all pods as soon as they complete podGC : strategy : OnPodCompletion You can set these configurations globally using Default Workflow Spec . If you need to keep records historically, use the Workflow Archive . Changing these settings will not delete workflows that have already run. To list old workflows: argo list --completed --since 7d v2.9 and after To list/delete workflows completed over 7 days ago: argo list --older 7d argo delete --older 7d Operator Cost Optimisations \u00b6 Suggestions for operators who installed Argo Workflows. Set Resources Requests and Limits \u00b6 Suitable if you have many instances, e.g. on dozens of clusters or namespaces. Set a resource requests and limits for the workflow-controller and argo-server , e.g. requests : cpu : 100m memory : 64Mi limits : cpu : 500m memory : 128Mi This above limit is suitable for the Argo Server, as this is stateless. The Workflow Controller is stateful and will scale to the number of live workflows - so you are likely to need higher values. Configure Executor Resource Requests \u00b6 Suitable for all - unless you have large artifacts. Configure workflow-controller-configmap.yaml to set the executorResources executorResources : requests : cpu : 100m memory : 64Mi limits : cpu : 500m memory : 512Mi The correct values depend on the size of artifacts your workflows download. For artifacts > 10GB, memory usage maybe large - #1322 .","title":"Cost Optimisation"},{"location":"cost-optimisation/#cost-optimisation","text":"","title":"Cost Optimisation"},{"location":"cost-optimisation/#user-cost-optimisations","text":"Suggestions for users running workflows.","title":"User Cost Optimisations"},{"location":"cost-optimisation/#set-the-workflows-pod-resource-requests","text":"Suitable if you are running a workflow with many homogenous pods. Resource duration shows the amount of CPU and memory requested by a pod and is indicative of the cost. You can use this to find costly steps within your workflow. Smaller requests can be set in the pod spec patch's resource requirements .","title":"Set The Workflows Pod Resource Requests"},{"location":"cost-optimisation/#use-a-node-selector-to-use-cheaper-instances","text":"You can use a node selector for cheaper instances, e.g. spot instances: nodeSelector : \"node-role.kubernetes.io/argo-spot-worker\" : \"true\"","title":"Use A Node Selector To Use Cheaper Instances"},{"location":"cost-optimisation/#consider-trying-volume-claim-templates-or-volumes-instead-of-artifacts","text":"Suitable if you have a workflow that passes a lot of artifacts within itself. Copying artifacts to and from storage outside of a cluster can be expensive. The correct choice is dependent on your artifact storage provider is vs. what volume they are using. For example, we believe it may be more expensive to allocate and delete new EBS volumes every workflow using the PVC feature, than it is to upload and download some small files to S3. On the other hand if they are using a NFS volume shared between all their workflows with large artifacts, that might be cheaper than the data transfer and storage costs of S3. Consider: Data transfer costs (upload/download vs. copying) Data storage costs (s3 vs. volume) Requirement for parallel access to data (NFS vs. EBS vs. artifact)","title":"Consider trying Volume Claim Templates or Volumes instead of Artifacts"},{"location":"cost-optimisation/#limit-the-total-number-of-workflows-and-pods","text":"Suitable for all. A workflow (and for that matter, any Kubernetes resource) will incur a cost as long as they exist in your cluster. The workflow controller memory and CPU needs increase linearly with the number of pods and workflows you are currently running. Limit the total number of workflows using: Active Deadline Seconds - terminate running workflows that do not complete in a set time. This will make sure workflows do not run forever. Workflow TTL Strategy - delete completed workflows after a time Pod GC - delete completed pods after a time Example spec : # must complete in 8h (28,800 seconds) activeDeadlineSeconds : 28800 # keep workflows for 1d (86,400 seconds) ttlStrategy : secondsAfterCompletion : 86400 # delete all pods as soon as they complete podGC : strategy : OnPodCompletion You can set these configurations globally using Default Workflow Spec . If you need to keep records historically, use the Workflow Archive . Changing these settings will not delete workflows that have already run. To list old workflows: argo list --completed --since 7d v2.9 and after To list/delete workflows completed over 7 days ago: argo list --older 7d argo delete --older 7d","title":"Limit The Total Number Of Workflows And Pods"},{"location":"cost-optimisation/#operator-cost-optimisations","text":"Suggestions for operators who installed Argo Workflows.","title":"Operator Cost Optimisations"},{"location":"cost-optimisation/#set-resources-requests-and-limits","text":"Suitable if you have many instances, e.g. on dozens of clusters or namespaces. Set a resource requests and limits for the workflow-controller and argo-server , e.g. requests : cpu : 100m memory : 64Mi limits : cpu : 500m memory : 128Mi This above limit is suitable for the Argo Server, as this is stateless. The Workflow Controller is stateful and will scale to the number of live workflows - so you are likely to need higher values.","title":"Set Resources Requests and Limits"},{"location":"cost-optimisation/#configure-executor-resource-requests","text":"Suitable for all - unless you have large artifacts. Configure workflow-controller-configmap.yaml to set the executorResources executorResources : requests : cpu : 100m memory : 64Mi limits : cpu : 500m memory : 512Mi The correct values depend on the size of artifacts your workflows download. For artifacts > 10GB, memory usage maybe large - #1322 .","title":"Configure Executor Resource Requests"},{"location":"cron-backfill/","text":"Cron Backfill \u00b6 Use Case \u00b6 You are using cron workflows to run daily jobs, you may need to re-run for a date, or run some historical days. Solution \u00b6 Create a workflow template for your daily job. Create your cron workflow to run daily and invoke that template. Create a backfill workflow that uses withSequence to run the job for each date. This full example contains: A workflow template named job . A cron workflow named daily-job . A workflow named backfill-v1 that uses a resource template to create one workflow for each backfill date. A alternative workflow named backfill-v2 that uses a steps templates to run one task for each backfill date.","title":"Cron Backfill"},{"location":"cron-backfill/#cron-backfill","text":"","title":"Cron Backfill"},{"location":"cron-backfill/#use-case","text":"You are using cron workflows to run daily jobs, you may need to re-run for a date, or run some historical days.","title":"Use Case"},{"location":"cron-backfill/#solution","text":"Create a workflow template for your daily job. Create your cron workflow to run daily and invoke that template. Create a backfill workflow that uses withSequence to run the job for each date. This full example contains: A workflow template named job . A cron workflow named daily-job . A workflow named backfill-v1 that uses a resource template to create one workflow for each backfill date. A alternative workflow named backfill-v2 that uses a steps templates to run one task for each backfill date.","title":"Solution"},{"location":"cron-workflows/","text":"Cron Workflows \u00b6 v2.5 and after Introduction \u00b6 CronWorkflow are workflows that run on a preset schedule. They are designed to be converted from Workflow easily and to mimick the same options as Kubernetes CronJob . In essence, CronWorkflow = Workflow + some specific cron options. CronWorkflow Spec \u00b6 An example CronWorkflow spec would look like: apiVersion : argoproj.io/v1alpha1 kind : CronWorkflow metadata : name : test-cron-wf spec : schedule : \"* * * * *\" concurrencyPolicy : \"Replace\" startingDeadlineSeconds : 0 workflowSpec : entrypoint : whalesay templates : - name : whalesay container : image : alpine:3.6 command : [ sh , -c ] args : [ \"date; sleep 90\" ] workflowSpec and workflowMetadata \u00b6 CronWorkflow.spec.workflowSpec is the same type as Workflow.spec and serves as a template for Workflow objects that are created from it. Everything under this spec will be converted to a Workflow . The resuling Workflow name will be a generated name based on the CronWorkflow name. In this example it could be something like test-cron-wf-tj6fe . CronWorkflow.spec.workflowMetadata can be used to add labels and annotations . CronWorkflow Options \u00b6 Option Name Default Value Description schedule None, must be provided Schedule at which the Workflow will be run. E.g. 5 4 * * * timezone Machine timezone Timezone during which the Workflow will be run. E.g. America/Los_Angeles suspend false If true Workflow scheduling will not occur. Can be set from the CLI, GitOps, or directly concurrencyPolicy Allow Policy that determines what to do if multiple Workflows are scheduled at the same time. Available options: Allow : allow all, Replace : remove all old before scheduling a new, Forbid : do not allow any new while there are old startingDeadlineSeconds 0 Number of seconds after the last successful run during which a missed Workflow will be run successfulJobsHistoryLimit 3 Number of successful Workflows that will be persisted at a time failedJobsHistoryLimit 1 Number of failed Workflows that will be persisted at a time Managing CronWorkflow \u00b6 CLI \u00b6 CronWorkflow can be created from the CLI by using basic commands: $ argo cron create cron.yaml Name: test-cron-wf Namespace: argo Created: Mon Nov 18 10 :17:06 -0800 ( now ) Schedule: * * * * * Suspended: false StartingDeadlineSeconds: 0 ConcurrencyPolicy: Forbid $ argo cron list NAME AGE LAST RUN SCHEDULE SUSPENDED test-cron-wf 49s N/A * * * * * false # some time passes $ argo cron list NAME AGE LAST RUN SCHEDULE SUSPENDED test-cron-wf 56s 2s * * * * * false $ argo cron get test-cron-wf Name: test-cron-wf Namespace: argo Created: Mon Nov 18 10 :17:06 -0800 ( 4 minutes ago ) Schedule: * * * * * Suspended: false StartingDeadlineSeconds: 0 ConcurrencyPolicy: Replace LastScheduledTime: Mon Nov 18 10 :21:00 -0800 ( 51 seconds ago ) Active Workflows: test-cron-wf-rt4nf kubectl \u00b6 Using kubectl apply -f and kubectl get cwf Backfilling Days \u00b6 See cron backfill . GitOps via Argo CD \u00b6 CronWorkflow resources can be managed with GitOps by using Argo CD UI \u00b6 CronWorkflow resources can also be managed by the UI","title":"Cron Workflows"},{"location":"cron-workflows/#cron-workflows","text":"v2.5 and after","title":"Cron Workflows"},{"location":"cron-workflows/#introduction","text":"CronWorkflow are workflows that run on a preset schedule. They are designed to be converted from Workflow easily and to mimick the same options as Kubernetes CronJob . In essence, CronWorkflow = Workflow + some specific cron options.","title":"Introduction"},{"location":"cron-workflows/#cronworkflow-spec","text":"An example CronWorkflow spec would look like: apiVersion : argoproj.io/v1alpha1 kind : CronWorkflow metadata : name : test-cron-wf spec : schedule : \"* * * * *\" concurrencyPolicy : \"Replace\" startingDeadlineSeconds : 0 workflowSpec : entrypoint : whalesay templates : - name : whalesay container : image : alpine:3.6 command : [ sh , -c ] args : [ \"date; sleep 90\" ]","title":"CronWorkflow Spec"},{"location":"cron-workflows/#workflowspec-and-workflowmetadata","text":"CronWorkflow.spec.workflowSpec is the same type as Workflow.spec and serves as a template for Workflow objects that are created from it. Everything under this spec will be converted to a Workflow . The resuling Workflow name will be a generated name based on the CronWorkflow name. In this example it could be something like test-cron-wf-tj6fe . CronWorkflow.spec.workflowMetadata can be used to add labels and annotations .","title":"workflowSpec and workflowMetadata"},{"location":"cron-workflows/#cronworkflow-options","text":"Option Name Default Value Description schedule None, must be provided Schedule at which the Workflow will be run. E.g. 5 4 * * * timezone Machine timezone Timezone during which the Workflow will be run. E.g. America/Los_Angeles suspend false If true Workflow scheduling will not occur. Can be set from the CLI, GitOps, or directly concurrencyPolicy Allow Policy that determines what to do if multiple Workflows are scheduled at the same time. Available options: Allow : allow all, Replace : remove all old before scheduling a new, Forbid : do not allow any new while there are old startingDeadlineSeconds 0 Number of seconds after the last successful run during which a missed Workflow will be run successfulJobsHistoryLimit 3 Number of successful Workflows that will be persisted at a time failedJobsHistoryLimit 1 Number of failed Workflows that will be persisted at a time","title":"CronWorkflow Options"},{"location":"cron-workflows/#managing-cronworkflow","text":"","title":"Managing CronWorkflow"},{"location":"cron-workflows/#cli","text":"CronWorkflow can be created from the CLI by using basic commands: $ argo cron create cron.yaml Name: test-cron-wf Namespace: argo Created: Mon Nov 18 10 :17:06 -0800 ( now ) Schedule: * * * * * Suspended: false StartingDeadlineSeconds: 0 ConcurrencyPolicy: Forbid $ argo cron list NAME AGE LAST RUN SCHEDULE SUSPENDED test-cron-wf 49s N/A * * * * * false # some time passes $ argo cron list NAME AGE LAST RUN SCHEDULE SUSPENDED test-cron-wf 56s 2s * * * * * false $ argo cron get test-cron-wf Name: test-cron-wf Namespace: argo Created: Mon Nov 18 10 :17:06 -0800 ( 4 minutes ago ) Schedule: * * * * * Suspended: false StartingDeadlineSeconds: 0 ConcurrencyPolicy: Replace LastScheduledTime: Mon Nov 18 10 :21:00 -0800 ( 51 seconds ago ) Active Workflows: test-cron-wf-rt4nf","title":"CLI"},{"location":"cron-workflows/#kubectl","text":"Using kubectl apply -f and kubectl get cwf","title":"kubectl"},{"location":"cron-workflows/#backfilling-days","text":"See cron backfill .","title":"Backfilling Days"},{"location":"cron-workflows/#gitops-via-argo-cd","text":"CronWorkflow resources can be managed with GitOps by using Argo CD","title":"GitOps via Argo CD"},{"location":"cron-workflows/#ui","text":"CronWorkflow resources can also be managed by the UI","title":"UI"},{"location":"default-workflow-specs/","text":"Default Workflow Spec \u00b6 v2.7 and after Introduction \u00b6 Default Workflow spec values can be set at the controller config map that will apply to all Workflows executed from said controller. If a Workflow has a value that also has a default value set in the config map, thw Workflow's value will take precedence. Setting Default Workflow Values \u00b6 Default Workflow values can be specified by adding them under the workflowDefaults key in the workflow-controller-configmap . Values can be added as the would under the Workflow.spec tag. For example, to specify default values that would partially produce the following Workflow : apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : gc-ttl- annotations : argo : workflows labels : foo : bar spec : ttlStrategy : secondsAfterSuccess : 5 # Time to live after workflow is successful parallelism : 3 The following would be specified in the Config Map: # This file describes the config settings available in the workflow controller configmap apiVersion : v1 kind : ConfigMap metadata : name : workflow-controller-configmap data : # Default values that will apply to all Workflows from this controller, unless overridden on the Workflow-level workflowDefaults : | metadata: annotations: argo: workflows labels: foo: bar spec: ttlStrategy: secondsAfterSuccess: 5 parallelism: 3","title":"Default Workflow Spec"},{"location":"default-workflow-specs/#default-workflow-spec","text":"v2.7 and after","title":"Default Workflow Spec"},{"location":"default-workflow-specs/#introduction","text":"Default Workflow spec values can be set at the controller config map that will apply to all Workflows executed from said controller. If a Workflow has a value that also has a default value set in the config map, thw Workflow's value will take precedence.","title":"Introduction"},{"location":"default-workflow-specs/#setting-default-workflow-values","text":"Default Workflow values can be specified by adding them under the workflowDefaults key in the workflow-controller-configmap . Values can be added as the would under the Workflow.spec tag. For example, to specify default values that would partially produce the following Workflow : apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : gc-ttl- annotations : argo : workflows labels : foo : bar spec : ttlStrategy : secondsAfterSuccess : 5 # Time to live after workflow is successful parallelism : 3 The following would be specified in the Config Map: # This file describes the config settings available in the workflow controller configmap apiVersion : v1 kind : ConfigMap metadata : name : workflow-controller-configmap data : # Default values that will apply to all Workflows from this controller, unless overridden on the Workflow-level workflowDefaults : | metadata: annotations: argo: workflows labels: foo: bar spec: ttlStrategy: secondsAfterSuccess: 5 parallelism: 3","title":"Setting Default Workflow Values"},{"location":"enhanced-depends-logic/","text":"Enhanced Depends Logic \u00b6 v2.9 and after Introduction \u00b6 Previous to version 2.8, the only way to specify dependencies in DAG templates was to use the dependencies field and specify a list of other tasks the current task depends on. This syntax was limiting because it does not allow the user to specify which result of the task to depend on. For example, a task may only be relevant to run if the dependent task succeeded (or failed, etc.). Depends \u00b6 To remedy this, there exists a new field called depends , which allows users to specify dependent tasks, their statuses, as well as any complex boolean logic. The field is a string field and the syntax is expression-like with operands having form <task-name>.<task-result> . Examples include task-1.Suceeded , task-2.Failed , task-3.Damenoed . The full list of available task results is as follows: Task Result Description .Succeeded Task Succeeded .Failed Task Failed .Errored Task Errored .Skipped Task Skipped .Daemoned Task is Daemoned and is not Pending For convenience, if an omitted task result is equivalent to (task.Succeeded || task.Skipped || task.Daemoned) . For example: depends : \"task || task-2.Failed\" is equivalent to: depends : ( task . Succeeded || task . Skipped || task . Daemoned ) || task - 2 . Failed Full boolean logic is also available. Operators include: && || ! Example: depends : \"(task-2.Succeeded || task-2.Skipped) && !task-3.Failed\" Compatibility with dependencies and dag.task.continueOn \u00b6 This feature is fully compatible with dependencies and conversion is easy. To convert simply join your dependencies with && : dependencies : [ \"A\" , \"B\" , \"C\" ] is equivalent to: depends : \"A && B && C\" Because of the added control found in depends , the dag.task.continueOn is not available when using it. Furthermore, it is not possible to use both dependencies and depends in the same task group.","title":"Enhanced Depends Logic"},{"location":"enhanced-depends-logic/#enhanced-depends-logic","text":"v2.9 and after","title":"Enhanced Depends Logic"},{"location":"enhanced-depends-logic/#introduction","text":"Previous to version 2.8, the only way to specify dependencies in DAG templates was to use the dependencies field and specify a list of other tasks the current task depends on. This syntax was limiting because it does not allow the user to specify which result of the task to depend on. For example, a task may only be relevant to run if the dependent task succeeded (or failed, etc.).","title":"Introduction"},{"location":"enhanced-depends-logic/#depends","text":"To remedy this, there exists a new field called depends , which allows users to specify dependent tasks, their statuses, as well as any complex boolean logic. The field is a string field and the syntax is expression-like with operands having form <task-name>.<task-result> . Examples include task-1.Suceeded , task-2.Failed , task-3.Damenoed . The full list of available task results is as follows: Task Result Description .Succeeded Task Succeeded .Failed Task Failed .Errored Task Errored .Skipped Task Skipped .Daemoned Task is Daemoned and is not Pending For convenience, if an omitted task result is equivalent to (task.Succeeded || task.Skipped || task.Daemoned) . For example: depends : \"task || task-2.Failed\" is equivalent to: depends : ( task . Succeeded || task . Skipped || task . Daemoned ) || task - 2 . Failed Full boolean logic is also available. Operators include: && || ! Example: depends : \"(task-2.Succeeded || task-2.Skipped) && !task-3.Failed\"","title":"Depends"},{"location":"enhanced-depends-logic/#compatibility-with-dependencies-and-dagtaskcontinueon","text":"This feature is fully compatible with dependencies and conversion is easy. To convert simply join your dependencies with && : dependencies : [ \"A\" , \"B\" , \"C\" ] is equivalent to: depends : \"A && B && C\" Because of the added control found in depends , the dag.task.continueOn is not available when using it. Furthermore, it is not possible to use both dependencies and depends in the same task group.","title":"Compatibility with dependencies and dag.task.continueOn"},{"location":"fields/","text":"Field Reference \u00b6 Workflow \u00b6 Workflow is the definition of a workflow resource Examples (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml) Fields \u00b6 Field Name Field Type Description apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds metadata ObjectMeta No description available spec WorkflowSpec No description available status WorkflowStatus No description available CronWorkflow \u00b6 CronWorkflow is the definition of a scheduled workflow resource Examples (click to open) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) Fields \u00b6 Field Name Field Type Description apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds metadata ObjectMeta No description available spec CronWorkflowSpec No description available status CronWorkflowStatus No description available WorkflowTemplate \u00b6 WorkflowTemplate is the definition of a workflow template resource Examples (click to open) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) Fields \u00b6 Field Name Field Type Description apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds metadata ObjectMeta No description available spec WorkflowTemplateSpec No description available WorkflowSpec \u00b6 WorkflowSpec is the specification of a Workflow. Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml) Fields \u00b6 Field Name Field Type Description activeDeadlineSeconds int64 Optional duration in seconds relative to the workflow start time which the workflow is allowed to run before the controller terminates the io.argoproj.workflow.v1alpha1. A value of zero is used to terminate a Running workflow affinity Affinity Affinity sets the scheduling constraints for all pods in the io.argoproj.workflow.v1alpha1. Can be overridden by an affinity specified in the template arguments Arguments Arguments contain the parameters and artifacts sent to the workflow entrypoint Parameters are referencable globally using the 'workflow' variable prefix. e.g. {{io.argoproj.workflow.v1alpha1.parameters.myparam}} artifactRepositoryRef ArtifactRepositoryRef ArtifactRepositoryRef specifies the configMap name and key containing the artifact repository config. automountServiceAccountToken boolean AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false. dnsConfig PodDNSConfig PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. dnsPolicy string Set DNS policy for the pod. Defaults to \"ClusterFirst\". Valid values are 'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to 'ClusterFirstWithHostNet'. entrypoint string Entrypoint is a template reference to the starting point of the io.argoproj.workflow.v1alpha1. executor ExecutorConfig Executor holds configurations of executor containers of the io.argoproj.workflow.v1alpha1. hostAliases Array< HostAlias > No description available hostNetwork boolean Host networking requested for this workflow pod. Default to false. imagePullSecrets Array< LocalObjectReference > ImagePullSecrets is a list of references to secrets in the same namespace to use for pulling any images in pods that reference this ServiceAccount. ImagePullSecrets are distinct from Secrets because Secrets can be mounted in the pod, but ImagePullSecrets are only accessed by the kubelet. More info: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod metrics Metrics Metrics are a list of metrics emitted from this Workflow nodeSelector Map< string , string > NodeSelector is a selector which will result in all pods of the workflow to be scheduled on the selected node(s). This is able to be overridden by a nodeSelector specified in the template. onExit string OnExit is a template reference which is invoked at the end of the workflow, irrespective of the success, failure, or error of the primary io.argoproj.workflow.v1alpha1. parallelism int64 Parallelism limits the max total parallel pods that can execute at the same time in a workflow podDisruptionBudget PodDisruptionBudgetSpec PodDisruptionBudget holds the number of concurrent disruptions that you allow for Workflow's Pods. Controller will automatically add the selector with workflow name, if selector is empty. Optional: Defaults to empty. podGC PodGC PodGC describes the strategy to use when to deleting completed pods podPriority int32 Priority to apply to workflow pods. podPriorityClassName string PriorityClassName to apply to workflow pods. podSpecPatch string PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits). priority int32 Priority is used if controller is configured to process limited number of workflows in parallel. Workflows with higher priority are processed first. schedulerName string Set scheduler name for all pods. Will be overridden if container/script template's scheduler name is set. Default scheduler will be used if neither specified. securityContext PodSecurityContext SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to empty. See type description for default values of each field. serviceAccountName string ServiceAccountName is the name of the ServiceAccount to run all pods of the workflow as. shutdown string Shutdown will shutdown the workflow according to its ShutdownStrategy suspend boolean Suspend will suspend the workflow and prevent execution of any future steps in the workflow templates Array< Template > Templates is a list of workflow templates used in a workflow tolerations Array< Toleration > Tolerations to apply to workflow pods. ~ ttlSecondsAfterFinished ~ ~ int32 ~ ~TTLSecondsAfterFinished limits the lifetime of a Workflow that has finished execution (Succeeded, Failed, Error). If this field is set, once the Workflow finishes, it will be deleted after ttlSecondsAfterFinished expires. If this field is unset, ttlSecondsAfterFinished will not expire. If this field is set to zero, ttlSecondsAfterFinished expires immediately after the Workflow finishes.~ DEPRECATED: Use TTLStrategy.SecondsAfterCompletion instead. ttlStrategy TTLStrategy TTLStrategy limits the lifetime of a Workflow that has finished execution depending on if it Succeeded or Failed. If this struct is set, once the Workflow finishes, it will be deleted after the time to live expires. If this field is unset, the controller config map will hold the default values. volumeClaimTemplates Array< PersistentVolumeClaim > VolumeClaimTemplates is a list of claims that containers are allowed to reference. The Workflow controller will create the claims at the beginning of the workflow and delete the claims upon completion of the workflow volumes Array< Volume > Volumes is a list of volumes that can be mounted by containers in a io.argoproj.workflow.v1alpha1. workflowTemplateRef WorkflowTemplateRef WorkflowTemplateRef holds a reference to a WorkflowTemplate for execution WorkflowStatus \u00b6 WorkflowStatus contains overall status information about a workflow Fields \u00b6 Field Name Field Type Description compressedNodes string Compressed and base64 decoded Nodes map conditions Array< Condition > Conditions is a list of conditions the Workflow may have finishedAt Time Time at which this workflow completed message string A human readable message indicating details about why the workflow is in this condition. nodes NodeStatus Nodes is a mapping between a node ID and the node's status. offloadNodeStatusVersion string Whether on not node status has been offloaded to a database. If exists, then Nodes and CompressedNodes will be empty. This will actually be populated with a hash of the offloaded data. outputs Outputs Outputs captures output values and artifact locations produced by the workflow via global outputs persistentVolumeClaims Array< Volume > PersistentVolumeClaims tracks all PVCs that were created as part of the io.argoproj.workflow.v1alpha1. The contents of this list are drained at the end of the workflow. phase string Phase a simple, high-level summary of where the workflow is in its lifecycle. resourcesDuration Map< integer , int64 > ResourcesDuration is the total for the workflow startedAt Time Time at which this workflow started storedTemplates Template StoredTemplates is a mapping between a template ref and the node's status. storedWorkflowTemplateSpec WorkflowSpec StoredWorkflowSpec stores the WorkflowTemplate spec for future execution. CronWorkflowSpec \u00b6 CronWorkflowSpec is the specification of a CronWorkflow Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml) Fields \u00b6 Field Name Field Type Description concurrencyPolicy string ConcurrencyPolicy is the K8s-style concurrency policy that will be used failedJobsHistoryLimit int32 FailedJobsHistoryLimit is the number of successful jobs to be kept at a time schedule string Schedule is a schedule to run the Workflow in Cron format startingDeadlineSeconds int64 StartingDeadlineSeconds is the K8s-style deadline that will limit the time a CronWorkflow will be run after its original scheduled time if it is missed. successfulJobsHistoryLimit int32 SuccessfulJobsHistoryLimit is the number of successful jobs to be kept at a time suspend boolean Suspend is a flag that will stop new CronWorkflows from running if set to true timezone string Timezone is the timezone against which the cron schedule will be calculated, e.g. \"Asia/Tokyo\". Default is machine's local time. workflowMetadata ObjectMeta WorkflowMetadata contains some metadata of the workflow to be run workflowSpec WorkflowSpec WorkflowSpec is the spec of the workflow to be run CronWorkflowStatus \u00b6 CronWorkflowStatus is the status of a CronWorkflow Fields \u00b6 Field Name Field Type Description active Array< ObjectReference > Active is a list of active workflows stemming from this CronWorkflow conditions Array< Condition > Conditions is a list of conditions the CronWorkflow may have lastScheduledTime Time LastScheduleTime is the last time the CronWorkflow was scheduled WorkflowTemplateSpec \u00b6 WorkflowTemplateSpec is a spec of WorkflowTemplate. Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml) Fields \u00b6 Field Name Field Type Description activeDeadlineSeconds int64 Optional duration in seconds relative to the workflow start time which the workflow is allowed to run before the controller terminates the io.argoproj.workflow.v1alpha1. A value of zero is used to terminate a Running workflow affinity Affinity Affinity sets the scheduling constraints for all pods in the io.argoproj.workflow.v1alpha1. Can be overridden by an affinity specified in the template arguments Arguments Arguments contain the parameters and artifacts sent to the workflow entrypoint Parameters are referencable globally using the 'workflow' variable prefix. e.g. {{io.argoproj.workflow.v1alpha1.parameters.myparam}} artifactRepositoryRef ArtifactRepositoryRef ArtifactRepositoryRef specifies the configMap name and key containing the artifact repository config. automountServiceAccountToken boolean AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false. dnsConfig PodDNSConfig PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. dnsPolicy string Set DNS policy for the pod. Defaults to \"ClusterFirst\". Valid values are 'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to 'ClusterFirstWithHostNet'. entrypoint string Entrypoint is a template reference to the starting point of the io.argoproj.workflow.v1alpha1. executor ExecutorConfig Executor holds configurations of executor containers of the io.argoproj.workflow.v1alpha1. hostAliases Array< HostAlias > No description available hostNetwork boolean Host networking requested for this workflow pod. Default to false. imagePullSecrets Array< LocalObjectReference > ImagePullSecrets is a list of references to secrets in the same namespace to use for pulling any images in pods that reference this ServiceAccount. ImagePullSecrets are distinct from Secrets because Secrets can be mounted in the pod, but ImagePullSecrets are only accessed by the kubelet. More info: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod metrics Metrics Metrics are a list of metrics emitted from this Workflow nodeSelector Map< string , string > NodeSelector is a selector which will result in all pods of the workflow to be scheduled on the selected node(s). This is able to be overridden by a nodeSelector specified in the template. onExit string OnExit is a template reference which is invoked at the end of the workflow, irrespective of the success, failure, or error of the primary io.argoproj.workflow.v1alpha1. parallelism int64 Parallelism limits the max total parallel pods that can execute at the same time in a workflow podDisruptionBudget PodDisruptionBudgetSpec PodDisruptionBudget holds the number of concurrent disruptions that you allow for Workflow's Pods. Controller will automatically add the selector with workflow name, if selector is empty. Optional: Defaults to empty. podGC PodGC PodGC describes the strategy to use when to deleting completed pods podPriority int32 Priority to apply to workflow pods. podPriorityClassName string PriorityClassName to apply to workflow pods. podSpecPatch string PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits). priority int32 Priority is used if controller is configured to process limited number of workflows in parallel. Workflows with higher priority are processed first. schedulerName string Set scheduler name for all pods. Will be overridden if container/script template's scheduler name is set. Default scheduler will be used if neither specified. securityContext PodSecurityContext SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to empty. See type description for default values of each field. serviceAccountName string ServiceAccountName is the name of the ServiceAccount to run all pods of the workflow as. shutdown string Shutdown will shutdown the workflow according to its ShutdownStrategy suspend boolean Suspend will suspend the workflow and prevent execution of any future steps in the workflow templates Array< Template > Templates is a list of workflow templates used in a workflow tolerations Array< Toleration > Tolerations to apply to workflow pods. ~ ttlSecondsAfterFinished ~ ~ int32 ~ ~TTLSecondsAfterFinished limits the lifetime of a Workflow that has finished execution (Succeeded, Failed, Error). If this field is set, once the Workflow finishes, it will be deleted after ttlSecondsAfterFinished expires. If this field is unset, ttlSecondsAfterFinished will not expire. If this field is set to zero, ttlSecondsAfterFinished expires immediately after the Workflow finishes.~ DEPRECATED: Use TTLStrategy.SecondsAfterCompletion instead. ttlStrategy TTLStrategy TTLStrategy limits the lifetime of a Workflow that has finished execution depending on if it Succeeded or Failed. If this struct is set, once the Workflow finishes, it will be deleted after the time to live expires. If this field is unset, the controller config map will hold the default values. volumeClaimTemplates Array< PersistentVolumeClaim > VolumeClaimTemplates is a list of claims that containers are allowed to reference. The Workflow controller will create the claims at the beginning of the workflow and delete the claims upon completion of the workflow volumes Array< Volume > Volumes is a list of volumes that can be mounted by containers in a io.argoproj.workflow.v1alpha1. workflowTemplateRef WorkflowTemplateRef WorkflowTemplateRef holds a reference to a WorkflowTemplate for execution Arguments \u00b6 Arguments to a template Examples with this field (click to open) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) Fields \u00b6 Field Name Field Type Description artifacts Array< Artifact > Artifacts is the list of artifacts to pass to the template or workflow parameters Array< Parameter > Parameters is the list of parameters to pass to the template or workflow ArtifactRepositoryRef \u00b6 No description available Examples with this field (click to open) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) Fields \u00b6 Field Name Field Type Description configMap string No description available key string No description available ExecutorConfig \u00b6 ExecutorConfig holds configurations of an executor container. Fields \u00b6 Field Name Field Type Description serviceAccountName string ServiceAccountName specifies the service account name of the executor container. Metrics \u00b6 Metrics are a list of metrics emitted from a Workflow/Template Examples with this field (click to open) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) Fields \u00b6 Field Name Field Type Description prometheus Array< Prometheus > Prometheus is a list of prometheus metrics to be emitted PodGC \u00b6 PodGC describes how to delete completed pods as they complete Examples with this field (click to open) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) Fields \u00b6 Field Name Field Type Description strategy string Strategy is the strategy to use. One of \"OnPodCompletion\", \"OnPodSuccess\", \"OnWorkflowCompletion\", \"OnWorkflowSuccess\" Template \u00b6 Template is a reusable and composable unit of execution in a workflow Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) Fields \u00b6 Field Name Field Type Description activeDeadlineSeconds int64 Optional duration in seconds relative to the StartTime that the pod may be active on a node before the system actively tries to terminate the pod; value must be positive integer This field is only applicable to container and script templates. affinity Affinity Affinity sets the pod's scheduling constraints Overrides the affinity set at the workflow level (if any) archiveLocation ArtifactLocation Location in which all files related to the step will be stored (logs, artifacts, etc...). Can be overridden by individual items in Outputs. If omitted, will use the default artifact repository location configured in the controller, appended with the / in the key. ~ arguments ~ ~ Arguments ~ ~Arguments hold arguments to the template.~ DEPRECATED: This field is not used. automountServiceAccountToken boolean AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false. container Container Container is the main container image to run in the pod daemon boolean Deamon will allow a workflow to proceed to the next step so long as the container reaches readiness dag DAGTemplate DAG template subtype which runs a DAG executor ExecutorConfig Executor holds configurations of the executor container. hostAliases Array< HostAlias > HostAliases is an optional list of hosts and IPs that will be injected into the pod spec initContainers Array< UserContainer > InitContainers is a list of containers which run before the main container. inputs Inputs Inputs describe what inputs parameters and artifacts are supplied to this template metadata Metadata Metdata sets the pods's metadata, i.e. annotations and labels metrics Metrics Metrics are a list of metrics emitted from this template name string Name is the name of the template nodeSelector Map< string , string > NodeSelector is a selector to schedule this step of the workflow to be run on the selected node(s). Overrides the selector set at the workflow level. outputs Outputs Outputs describe the parameters and artifacts that this template produces parallelism int64 Parallelism limits the max total parallel pods that can execute at the same time within the boundaries of this template invocation. If additional steps/dag templates are invoked, the pods created by those templates will not be counted towards this total. podSpecPatch string PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits). priority int32 Priority to apply to workflow pods. priorityClassName string PriorityClassName to apply to workflow pods. resource ResourceTemplate Resource template subtype which can run k8s resources resubmitPendingPods boolean ResubmitPendingPods is a flag to enable resubmitting pods that remain Pending after initial submission retryStrategy RetryStrategy RetryStrategy describes how to retry a template when it fails schedulerName string If specified, the pod will be dispatched by specified scheduler. Or it will be dispatched by workflow scope scheduler if specified. If neither specified, the pod will be dispatched by default scheduler. script ScriptTemplate Script runs a portion of code against an interpreter securityContext PodSecurityContext SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to empty. See type description for default values of each field. serviceAccountName string ServiceAccountName to apply to workflow pods sidecars Array< UserContainer > Sidecars is a list of containers which run alongside the main container Sidecars are automatically killed when the main container completes steps Array<Array< WorkflowStep >> Steps define a series of sequential/parallel workflow steps suspend SuspendTemplate Suspend template subtype which can suspend a workflow when reaching the step ~ template ~ ~ string ~ ~Template is the name of the template which is used as the base of this template.~ DEPRECATED: This field is not used. ~ templateRef ~ ~ TemplateRef ~ ~TemplateRef is the reference to the template resource which is used as the base of this template.~ DEPRECATED: This field is not used. tolerations Array< Toleration > Tolerations to apply to workflow pods. volumes Array< Volume > Volumes is a list of volumes that can be mounted by containers in a template. TTLStrategy \u00b6 TTLStrategy is the strategy for the time to live depending on if the workflow succeeded or failed Examples with this field (click to open) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) Fields \u00b6 Field Name Field Type Description secondsAfterCompletion int32 SecondsAfterCompletion is the number of seconds to live after completion secondsAfterFailure int32 SecondsAfterFailure is the number of seconds to live after failure secondsAfterSuccess int32 SecondsAfterSuccess is the number of seconds to live after success WorkflowTemplateRef \u00b6 WorkflowTemplateRef is a reference to a WorkflowTemplate resource. Examples with this field (click to open) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml) Fields \u00b6 Field Name Field Type Description clusterScope boolean ClusterScope indicates the referred template is cluster scoped (i.e. a ClusterWorkflowTemplate). name string Name is the resource name of the workflow template. Condition \u00b6 No description available Fields \u00b6 Field Name Field Type Description message string Message is the condition message status string Status is the status of the condition type string Type is the type of condition NodeStatus \u00b6 NodeStatus contains status information about an individual node in the workflow Fields \u00b6 Field Name Field Type Description boundaryID string BoundaryID indicates the node ID of the associated template root node in which this node belongs to children Array< string > Children is a list of child node IDs daemoned boolean Daemoned tracks whether or not this node was daemoned and need to be terminated displayName string DisplayName is a human readable representation of the node. Unique within a template boundary finishedAt Time Time at which this node completed hostNodeName string HostNodeName name of the Kubernetes node on which the Pod is running, if applicable id string ID is a unique identifier of a node within the worklow It is implemented as a hash of the node name, which makes the ID deterministic inputs Inputs Inputs captures input parameter values and artifact locations supplied to this template invocation message string A human readable message indicating details about why the node is in this condition. name string Name is unique name in the node tree used to generate the node ID outboundNodes Array< string > OutboundNodes tracks the node IDs which are considered \"outbound\" nodes to a template invocation. For every invocation of a template, there are nodes which we considered as \"outbound\". Essentially, these are last nodes in the execution sequence to run, before the template is considered completed. These nodes are then connected as parents to a following step.In the case of single pod steps (i.e. container, script, resource templates), this list will be nil since the pod itself is already considered the \"outbound\" node. In the case of DAGs, outbound nodes are the \"target\" tasks (tasks with no children). In the case of steps, outbound nodes are all the containers involved in the last step group. NOTE: since templates are composable, the list of outbound nodes are carried upwards when a DAG/steps template invokes another DAG/steps template. In other words, the outbound nodes of a template, will be a superset of the outbound nodes of its last children. outputs Outputs Outputs captures output parameter values and artifact locations produced by this template invocation phase string Phase a simple, high-level summary of where the node is in its lifecycle. Can be used as a state machine. podIP string PodIP captures the IP of the pod for daemoned steps resourcesDuration Map< integer , int64 > ResourcesDuration is indicative, but not accurate, resource duration. This is populated when the nodes completes. startedAt Time Time at which this node started ~ storedTemplateID ~ ~ string ~ ~StoredTemplateID is the ID of stored template.~ DEPRECATED: This value is not used anymore. templateName string TemplateName is the template name which this node corresponds to. Not applicable to virtual nodes (e.g. Retry, StepGroup) templateRef TemplateRef TemplateRef is the reference to the template resource which this node corresponds to. Not applicable to virtual nodes (e.g. Retry, StepGroup) templateScope string TemplateScope is the template scope in which the template of this node was retrieved. type string Type indicates type of node ~ workflowTemplateName ~ ~ string ~ ~WorkflowTemplateName is the WorkflowTemplate resource name on which the resolved template of this node is retrieved.~ DEPRECATED: This value is not used anymore. Outputs \u00b6 Outputs hold parameters, artifacts, and results from a step Examples with this field (click to open) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) Fields \u00b6 Field Name Field Type Description artifacts Array< Artifact > Artifacts holds the list of output artifacts produced by a step exitCode string ExitCode holds the exit code of a script template parameters Array< Parameter > Parameters holds the list of output parameters produced by a step result string Result holds the result (stdout) of a script template Artifact \u00b6 Artifact indicates an artifact to place at a specified path Examples with this field (click to open) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) Fields \u00b6 Field Name Field Type Description archive ArchiveStrategy Archive controls how the artifact will be saved to the artifact repository. archiveLogs boolean ArchiveLogs indicates if the container logs should be archived artifactory ArtifactoryArtifact Artifactory contains artifactory artifact location details from string From allows an artifact to reference an artifact from a previous step gcs GCSArtifact GCS contains GCS artifact location details git GitArtifact Git contains git artifact location details globalName string GlobalName exports an output artifact to the global scope, making it available as '{{io.argoproj.workflow.v1alpha1.outputs.artifacts.XXXX}} and in workflow.status.outputs.artifacts hdfs HDFSArtifact HDFS contains HDFS artifact location details http HTTPArtifact HTTP contains HTTP artifact location details mode int32 mode bits to use on this file, must be a value between 0 and 0777 set when loading input artifacts. name string name of the artifact. must be unique within a template's inputs/outputs. optional boolean Make Artifacts optional, if Artifacts doesn't generate or exist oss OSSArtifact OSS contains OSS artifact location details path string Path is the container path to the artifact raw RawArtifact Raw contains raw artifact location details s3 S3Artifact S3 contains S3 artifact location details Parameter \u00b6 Parameter indicate a passed string parameter to a service template with an optional default value Examples with this field (click to open) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) Fields \u00b6 Field Name Field Type Description default IntOrString Default is the default value to use for an input parameter if a value was not supplied globalName string GlobalName exports an output parameter to the global scope, making it available as '{{io.argoproj.workflow.v1alpha1.outputs.parameters.XXXX}} and in workflow.status.outputs.parameters name string Name is the parameter name value IntOrString Value is the literal value to use for the parameter. If specified in the context of an input parameter, the value takes precedence over any passed values valueFrom ValueFrom ValueFrom is the source for the output parameter's value Prometheus \u00b6 Prometheus is a prometheus metric to be emitted Examples with this field (click to open) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) Fields \u00b6 Field Name Field Type Description counter Counter Counter is a counter metric gauge Gauge Gauge is a gauge metric help string Help is a string that describes the metric histogram Histogram Histogram is a histogram metric labels Array< MetricLabel > Labels is a list of metric labels name string Name is the name of the metric when string When is a conditional statement that decides when to emit the metric ArtifactLocation \u00b6 ArtifactLocation describes a location for a single or multiple artifacts. It is used as single artifact in the context of inputs/outputs (e.g. outputs.artifacts.artname). It is also used to describe the location of multiple artifacts such as the archive location of a single workflow step, which the executor will use as a default location to store its files. Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) Fields \u00b6 Field Name Field Type Description archiveLogs boolean ArchiveLogs indicates if the container logs should be archived artifactory ArtifactoryArtifact Artifactory contains artifactory artifact location details gcs GCSArtifact GCS contains GCS artifact location details git GitArtifact Git contains git artifact location details hdfs HDFSArtifact HDFS contains HDFS artifact location details http HTTPArtifact HTTP contains HTTP artifact location details oss OSSArtifact OSS contains OSS artifact location details raw RawArtifact Raw contains raw artifact location details s3 S3Artifact S3 contains S3 artifact location details DAGTemplate \u00b6 DAGTemplate is a template subtype for directed acyclic graph templates Examples with this field (click to open) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) Fields \u00b6 Field Name Field Type Description failFast boolean This flag is for DAG logic. The DAG logic has a built-in \"fail fast\" feature to stop scheduling new steps, as soon as it detects that one of the DAG nodes is failed. Then it waits until all DAG nodes are completed before failing the DAG itself. The FailFast flag default is true, if set to false, it will allow a DAG to run all branches of the DAG to completion (either success or failure), regardless of the failed outcomes of branches in the DAG. More info and example about this feature at https://github.com/argoproj/argo/issues/1442 target string Target are one or more names of targets to execute in a DAG tasks Array< DAGTask > Tasks are a list of DAG tasks UserContainer \u00b6 UserContainer is a container specified by a user. Examples with this field (click to open) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) Fields \u00b6 Field Name Field Type Description args Array< string > Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell command Array< string > Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell env Array< EnvVar > List of environment variables to set in the container. Cannot be updated. envFrom Array< EnvFromSource > List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. image string Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets. imagePullPolicy string Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images lifecycle Lifecycle Actions that the management system should take in response to container lifecycle events. Cannot be updated. livenessProbe Probe Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes mirrorVolumeMounts boolean MirrorVolumeMounts will mount the same volumes specified in the main container to the container (including artifacts), at the same mountPaths. This enables dind daemon to partially see the same filesystem as the main container in order to use features such as docker volume binding name string Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated. ports Array< ContainerPort > List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \"0.0.0.0\" address inside a container will be accessible from the network. Cannot be updated. readinessProbe Probe Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes resources ResourceRequirements Compute Resources required by this container. Cannot be updated. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ securityContext SecurityContext Security options the pod should run with. More info: https://kubernetes.io/docs/concepts/policy/security-context/ More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ startupProbe Probe StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state operation. This cannot be updated. This is an alpha feature enabled by the StartupProbe feature flag. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes stdin boolean Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false. stdinOnce boolean Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false terminationMessagePath string Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated. terminationMessagePolicy string Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated. tty boolean Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false. volumeDevices Array< VolumeDevice > volumeDevices is the list of block devices to be used by the container. This is a beta feature. volumeMounts Array< VolumeMount > Pod volumes to mount into the container's filesystem. Cannot be updated. workingDir string Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated. Inputs \u00b6 Inputs are the mechanism for passing parameters, artifacts, volumes from one template to another Examples with this field (click to open) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) Fields \u00b6 Field Name Field Type Description artifacts Array< Artifact > Artifact are a list of artifacts passed as inputs parameters Array< Parameter > Parameters are a list of parameters passed as inputs Metadata \u00b6 Pod metdata Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml) Fields \u00b6 Field Name Field Type Description annotations Map< string , string > No description available labels Map< string , string > No description available ResourceTemplate \u00b6 ResourceTemplate is a template subtype to manipulate kubernetes resources Examples with this field (click to open) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) Fields \u00b6 Field Name Field Type Description action string Action is the action to perform to the resource. Must be one of: get, create, apply, delete, replace, patch failureCondition string FailureCondition is a label selector expression which describes the conditions of the k8s resource in which the step was considered failed flags Array< string > Flags is a set of additional options passed to kubectl before submitting a resource I.e. to disable resource validation: flags: [ \"--validate=false\" # disable resource validation] manifest string Manifest contains the kubernetes manifest mergeStrategy string MergeStrategy is the strategy used to merge a patch. It defaults to \"strategic\" Must be one of: strategic, merge, json setOwnerReference boolean SetOwnerReference sets the reference to the workflow on the OwnerReference of generated resource. successCondition string SuccessCondition is a label selector expression which describes the conditions of the k8s resource in which it is acceptable to proceed to the following step RetryStrategy \u00b6 RetryStrategy provides controls on how to retry a workflow step Examples with this field (click to open) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) Fields \u00b6 Field Name Field Type Description backoff Backoff Backoff is a backoff strategy limit int32 Limit is the maximum number of attempts when retrying a container retryPolicy string RetryPolicy is a policy of NodePhase statuses that will be retried ScriptTemplate \u00b6 ScriptTemplate is a template subtype to enable scripting through code steps Examples with this field (click to open) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) Fields \u00b6 Field Name Field Type Description args Array< string > Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell command Array< string > Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell env Array< EnvVar > List of environment variables to set in the container. Cannot be updated. envFrom Array< EnvFromSource > List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. image string Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets. imagePullPolicy string Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images lifecycle Lifecycle Actions that the management system should take in response to container lifecycle events. Cannot be updated. livenessProbe Probe Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes name string Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated. ports Array< ContainerPort > List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \"0.0.0.0\" address inside a container will be accessible from the network. Cannot be updated. readinessProbe Probe Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes resources ResourceRequirements Compute Resources required by this container. Cannot be updated. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ securityContext SecurityContext Security options the pod should run with. More info: https://kubernetes.io/docs/concepts/policy/security-context/ More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ source string Source contains the source code of the script to execute startupProbe Probe StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state operation. This cannot be updated. This is an alpha feature enabled by the StartupProbe feature flag. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes stdin boolean Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false. stdinOnce boolean Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false terminationMessagePath string Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated. terminationMessagePolicy string Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated. tty boolean Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false. volumeDevices Array< VolumeDevice > volumeDevices is the list of block devices to be used by the container. This is a beta feature. volumeMounts Array< VolumeMount > Pod volumes to mount into the container's filesystem. Cannot be updated. workingDir string Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated. WorkflowStep \u00b6 WorkflowStep is a reference to a template to execute in a series of step Examples with this field (click to open) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) Fields \u00b6 Field Name Field Type Description arguments Arguments Arguments hold arguments to the template continueOn ContinueOn ContinueOn makes argo to proceed with the following step even if this step fails. Errors and Failed states can be specified name string Name of the step onExit string OnExit is a template reference which is invoked at the end of the template, irrespective of the success, failure, or error of the primary template. template string Template is the name of the template to execute as the step templateRef TemplateRef TemplateRef is the reference to the template resource to execute as the step. when string When is an expression in which the step should conditionally execute withItems Array< Item > WithItems expands a step into multiple parallel steps from the items in the list withParam string WithParam expands a step into multiple parallel steps from the value in the parameter, which is expected to be a JSON list. withSequence Sequence WithSequence expands a step into a numeric sequence SuspendTemplate \u00b6 SuspendTemplate is a template subtype to suspend a workflow at a predetermined point in time Examples with this field (click to open) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) Fields \u00b6 Field Name Field Type Description duration string Duration is the seconds to wait before automatically resuming a template TemplateRef \u00b6 TemplateRef is a reference of template resource. Examples with this field (click to open) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) Fields \u00b6 Field Name Field Type Description clusterScope boolean ClusterScope indicates the referred template is cluster scoped (i.e. a ClusterWorkflowTemplate). name string Name is the resource name of the template. runtimeResolution boolean RuntimeResolution skips validation at creation time. By enabling this option, you can create the referred workflow template before the actual runtime. template string Template is the name of referred template in the resource. ArchiveStrategy \u00b6 ArchiveStrategy describes how to archive files/directory when saving artifacts Examples with this field (click to open) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) Fields \u00b6 Field Name Field Type Description none NoneStrategy No description available tar TarStrategy No description available ArtifactoryArtifact \u00b6 ArtifactoryArtifact is the location of an artifactory artifact Examples with this field (click to open) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) Fields \u00b6 Field Name Field Type Description passwordSecret SecretKeySelector PasswordSecret is the secret selector to the repository password url string URL of the artifact usernameSecret SecretKeySelector UsernameSecret is the secret selector to the repository username GCSArtifact \u00b6 GCSArtifact is the location of a GCS artifact Examples with this field (click to open) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) Fields \u00b6 Field Name Field Type Description bucket string Bucket is the name of the bucket key string Key is the path in the bucket where the artifact resides serviceAccountKeySecret SecretKeySelector ServiceAccountKeySecret is the secret selector to the bucket's service account key GitArtifact \u00b6 GitArtifact is the location of an git artifact Examples with this field (click to open) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) Fields \u00b6 Field Name Field Type Description depth int64 Depth specifies clones/fetches should be shallow and include the given number of commits from the branch tip fetch Array< string > Fetch specifies a number of refs that should be fetched before checkout insecureIgnoreHostKey boolean InsecureIgnoreHostKey disables SSH strict host key checking during git clone passwordSecret SecretKeySelector PasswordSecret is the secret selector to the repository password repo string Repo is the git repository revision string Revision is the git commit, tag, branch to checkout sshPrivateKeySecret SecretKeySelector SSHPrivateKeySecret is the secret selector to the repository ssh private key usernameSecret SecretKeySelector UsernameSecret is the secret selector to the repository username HDFSArtifact \u00b6 HDFSArtifact is the location of an HDFS artifact Examples with this field (click to open) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) Fields \u00b6 Field Name Field Type Description addresses Array< string > Addresses is accessible addresses of HDFS name nodes force boolean Force copies a file forcibly even if it exists (default: false) hdfsUser string HDFSUser is the user to access HDFS file system. It is ignored if either ccache or keytab is used. krbCCacheSecret SecretKeySelector KrbCCacheSecret is the secret selector for Kerberos ccache Either ccache or keytab can be set to use Kerberos. krbConfigConfigMap ConfigMapKeySelector KrbConfig is the configmap selector for Kerberos config as string It must be set if either ccache or keytab is used. krbKeytabSecret SecretKeySelector KrbKeytabSecret is the secret selector for Kerberos keytab Either ccache or keytab can be set to use Kerberos. krbRealm string KrbRealm is the Kerberos realm used with Kerberos keytab It must be set if keytab is used. krbServicePrincipalName string KrbServicePrincipalName is the principal name of Kerberos service It must be set if either ccache or keytab is used. krbUsername string KrbUsername is the Kerberos username used with Kerberos keytab It must be set if keytab is used. path string Path is a file path in HDFS HTTPArtifact \u00b6 HTTPArtifact allows an file served on HTTP to be placed as an input artifact in a container Examples with this field (click to open) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) Fields \u00b6 Field Name Field Type Description url string URL of the artifact OSSArtifact \u00b6 OSSArtifact is the location of an Alibaba Cloud OSS artifact Examples with this field (click to open) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) Fields \u00b6 Field Name Field Type Description accessKeySecret SecretKeySelector AccessKeySecret is the secret selector to the bucket's access key bucket string Bucket is the name of the bucket endpoint string Endpoint is the hostname of the bucket endpoint key string Key is the path in the bucket where the artifact resides secretKeySecret SecretKeySelector SecretKeySecret is the secret selector to the bucket's secret key RawArtifact \u00b6 RawArtifact allows raw string content to be placed as an artifact in a container Examples with this field (click to open) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) Fields \u00b6 Field Name Field Type Description data string Data is the string contents of the artifact S3Artifact \u00b6 S3Artifact is the location of an S3 artifact Fields \u00b6 Field Name Field Type Description accessKeySecret SecretKeySelector AccessKeySecret is the secret selector to the bucket's access key bucket string Bucket is the name of the bucket endpoint string Endpoint is the hostname of the bucket endpoint insecure boolean Insecure will connect to the service with TLS key string Key is the key in the bucket where the artifact resides region string Region contains the optional bucket region roleARN string RoleARN is the Amazon Resource Name (ARN) of the role to assume. secretKeySecret SecretKeySelector SecretKeySecret is the secret selector to the bucket's secret key useSDKCreds boolean UseSDKCreds tells the driver to figure out credentials based on sdk defaults. ValueFrom \u00b6 ValueFrom describes a location in which to obtain the value to a parameter Examples with this field (click to open) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) Fields \u00b6 Field Name Field Type Description default IntOrString Default specifies a value to be used if retrieving the value from the specified source fails jqFilter string JQFilter expression against the resource object in resource templates jsonPath string JSONPath of a resource to retrieve an output parameter value from in resource templates parameter string Parameter reference to a step or dag task in which to retrieve an output parameter value from (e.g. '{{steps.mystep.outputs.myparam}}') path string Path in the container to retrieve an output parameter value from in container templates Counter \u00b6 Counter is a Counter prometheus metric Examples with this field (click to open) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) Fields \u00b6 Field Name Field Type Description value string Value is the value of the metric Gauge \u00b6 Gauge is a Gauge prometheus metric Examples with this field (click to open) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) Fields \u00b6 Field Name Field Type Description realtime boolean Realtime emits this metric in real time if applicable value string Value is the value of the metric Histogram \u00b6 Histogram is a Histogram prometheus metric Examples with this field (click to open) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) Fields \u00b6 Field Name Field Type Description buckets Array< Amount > Buckets is a list of bucket divisors for the histogram value string Value is the value of the metric MetricLabel \u00b6 MetricLabel is a single label for a prometheus metric Examples with this field (click to open) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) Fields \u00b6 Field Name Field Type Description key string No description available value string No description available DAGTask \u00b6 DAGTask represents a node in the graph during DAG execution Examples with this field (click to open) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) Fields \u00b6 Field Name Field Type Description arguments Arguments Arguments are the parameter and artifact arguments to the template continueOn ContinueOn ContinueOn makes argo to proceed with the following step even if this step fails. Errors and Failed states can be specified dependencies Array< string > Dependencies are name of other targets which this depends on depends string Depends are name of other targets which this depends on name string Name is the name of the target onExit string OnExit is a template reference which is invoked at the end of the template, irrespective of the success, failure, or error of the primary template. template string Name of template to execute templateRef TemplateRef TemplateRef is the reference to the template resource to execute. when string When is an expression in which the task should conditionally execute withItems Array< Item > WithItems expands a task into multiple parallel tasks from the items in the list withParam string WithParam expands a task into multiple parallel tasks from the value in the parameter, which is expected to be a JSON list. withSequence Sequence WithSequence expands a task into a numeric sequence Backoff \u00b6 Backoff is a backoff strategy to use within retryStrategy Examples with this field (click to open) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) Fields \u00b6 Field Name Field Type Description duration string Duration is the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\") factor int32 Factor is a factor to multiply the base duration after each failed retry maxDuration string MaxDuration is the maximum amount of time allowed for the backoff strategy ContinueOn \u00b6 ContinueOn defines if a workflow should continue even if a task or step fails/errors. It can be specified if the workflow should continue when the pod errors, fails or both. Examples with this field (click to open) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) Fields \u00b6 Field Name Field Type Description error boolean No description available failed boolean No description available Item \u00b6 Item expands a single workflow step into multiple parallel steps The value of Item can be a map, string, bool, or number Examples with this field (click to open) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) Sequence \u00b6 Sequence expands a workflow step into numeric range Examples with this field (click to open) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) Fields \u00b6 Field Name Field Type Description count string Count is number of elements in the sequence (default: 0). Not to be used with end end string Number at which to end the sequence (default: 0). Not to be used with Count format string Format is a printf format string to format the value in the sequence start string Number at which to start the sequence (default: 0) NoneStrategy \u00b6 NoneStrategy indicates to skip tar process and upload the files or directory tree as independent files. Note that if the artifact is a directory, the artifact driver must support the ability to save/load the directory appropriately. Examples with this field (click to open) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) TarStrategy \u00b6 TarStrategy will tar and gzip the file or directory when saving Examples with this field (click to open) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) Fields \u00b6 Field Name Field Type Description compressionLevel int32 CompressionLevel specifies the gzip compression level to use for the artifact. Defaults to gzip.DefaultCompression. Amount \u00b6 Amount represent a numeric amount. Examples with this field (click to open) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) External Fields \u00b6 ObjectMeta \u00b6 ObjectMeta is metadata that all persisted resources must have, which includes all objects users must create. Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml) Fields \u00b6 Field Name Field Type Description annotations Map< string , string > Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations clusterName string The name of the cluster which the object belongs to. This is used to distinguish resources with same name and namespace in different clusters. This field is not set anywhere right now and apiserver is going to ignore it if set in create or update request. creationTimestamp Time CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC.Populated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata deletionGracePeriodSeconds int64 Number of seconds allowed for this object to gracefully terminate before it will be removed from the system. Only set when deletionTimestamp is also set. May only be shortened. Read-only. deletionTimestamp Time DeletionTimestamp is RFC 3339 date and time at which this resource will be deleted. This field is set by the server when a graceful deletion is requested by the user, and is not directly settable by a client. The resource is expected to be deleted (no longer visible from resource lists, and not reachable by name) after the time in this field, once the finalizers list is empty. As long as the finalizers list contains items, deletion is blocked. Once the deletionTimestamp is set, this value may not be unset or be set further into the future, although it may be shortened or the resource may be deleted prior to this time. For example, a user may request that a pod is deleted in 30 seconds. The Kubelet will react by sending a graceful termination signal to the containers in the pod. After that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL) to the container and after cleanup, remove the pod from the API. In the presence of network partitions, this object may still exist after this timestamp, until an administrator or automated process can determine the resource is fully terminated. If not set, graceful deletion of the object has not been requested.Populated by the system when a graceful deletion is requested. Read-only. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata finalizers Array< string > Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. generateName string GenerateName is an optional prefix, used by the server, to generate a unique name ONLY IF the Name field has not been provided. If this field is used, the name returned to the client will be different than the name passed. This value will also be combined with a unique suffix. The provided value has the same validation rules as the Name field, and may be truncated by the length of the suffix required to make the value unique on the server.If this field is specified and the generated name exists, the server will NOT return a 409 - instead, it will either return 201 Created or 500 with Reason ServerTimeout indicating a unique name could not be found in the time allotted, and the client should retry (optionally after the time indicated in the Retry-After header).Applied only if Name is not specified. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#idempotency generation int64 A sequence number representing a specific generation of the desired state. Populated by the system. Read-only. ~ initializers ~ ~ Initializers ~ ~An initializer is a controller which enforces some system invariant at object creation time. This field is a list of initializers that have not yet acted on this object. If nil or empty, this object has been completely initialized. Otherwise, the object is considered uninitialized and is hidden (in list/watch and get calls) from clients that haven't explicitly asked to observe uninitialized objects.When an object is created, the system will populate this list with the current set of initializers. Only privileged users may set or modify this list. Once it is empty, it may not be modified further by any user~ DEPRECATED - initializers are an alpha field and will be removed in v1.15. labels Map< string , string > Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels managedFields Array< ManagedFieldsEntry > ManagedFields maps workflow-id and version to the set of fields that are managed by that workflow. This is mostly for internal housekeeping, and users typically shouldn't need to set or understand this field. A workflow can be the user's name, a controller's name, or the name of a specific apply path like \"ci-cd\". The set of fields is always in the version that the workflow used when modifying the object.This field is alpha and can be changed or removed without notice. name string Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/identifiers#names namespace string Namespace defines the space within each name must be unique. An empty namespace is equivalent to the \"default\" namespace, but \"default\" is the canonical representation. Not all objects are required to be scoped to a namespace - the value of this field for those objects will be empty.Must be a DNS_LABEL. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/namespaces ownerReferences Array< OwnerReference > List of objects depended by this object. If ALL objects in the list have been deleted, this object will be garbage collected. If this object is managed by a controller, then an entry in this list will point to this controller, with the controller field set to true. There cannot be more than one managing controller. resourceVersion string An opaque value that represents the internal version of this object that can be used by clients to determine when objects have changed. May be used for optimistic concurrency, change detection, and the watch operation on a resource or set of resources. Clients must treat these values as opaque and passed unmodified back to the server. They may only be valid for a particular resource or set of resources.Populated by the system. Read-only. Value must be treated as opaque by clients and . More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency selfLink string SelfLink is a URL representing this object. Populated by the system. Read-only. uid string UID is the unique in time and space value for this object. It is typically generated by the server on successful creation of a resource and is not allowed to change on PUT operations.Populated by the system. Read-only. More info: http://kubernetes.io/docs/user-guide/identifiers#uids Affinity \u00b6 Affinity is a group of affinity scheduling rules. Fields \u00b6 Field Name Field Type Description nodeAffinity NodeAffinity Describes node affinity scheduling rules for the pod. podAffinity PodAffinity Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). podAntiAffinity PodAntiAffinity Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). PodDNSConfig \u00b6 PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. Examples with this field (click to open) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) Fields \u00b6 Field Name Field Type Description nameservers Array< string > A list of DNS name server IP addresses. This will be appended to the base nameservers generated from DNSPolicy. Duplicated nameservers will be removed. options Array< PodDNSConfigOption > A list of DNS resolver options. This will be merged with the base options generated from DNSPolicy. Duplicated entries will be removed. Resolution options given in Options will override those that appear in the base DNSPolicy. searches Array< string > A list of DNS search domains for host-name lookup. This will be appended to the base search paths generated from DNSPolicy. Duplicated search paths will be removed. HostAlias \u00b6 HostAlias holds the mapping between IP and hostnames that will be injected as an entry in the pod's hosts file. Fields \u00b6 Field Name Field Type Description hostnames Array< string > Hostnames for the above IP address. ip string IP address of the host file entry. LocalObjectReference \u00b6 LocalObjectReference contains enough information to let you locate the referenced object inside the same namespace. Examples with this field (click to open) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) Fields \u00b6 Field Name Field Type Description name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names PodDisruptionBudgetSpec \u00b6 PodDisruptionBudgetSpec is a description of a PodDisruptionBudget. Examples with this field (click to open) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) Fields \u00b6 Field Name Field Type Description maxUnavailable IntOrString An eviction is allowed if at most \"maxUnavailable\" pods selected by \"selector\" are unavailable after the eviction, i.e. even in absence of the evicted pod. For example, one can prevent all voluntary evictions by specifying 0. This is a mutually exclusive setting with \"minAvailable\". minAvailable IntOrString An eviction is allowed if at least \"minAvailable\" pods selected by \"selector\" will still be available after the eviction, i.e. even in the absence of the evicted pod. So for example you can prevent all voluntary evictions by specifying \"100%\". selector LabelSelector Label query over pods whose evictions are managed by the disruption budget. PodSecurityContext \u00b6 PodSecurityContext holds pod-level security attributes and common container settings. Some fields are also present in container.securityContext. Field values of container.securityContext take precedence over field values of PodSecurityContext. Examples with this field (click to open) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) Fields \u00b6 Field Name Field Type Description fsGroup int64 A special supplemental group that applies to all containers in a pod. Some volume types allow the Kubelet to change the ownership of that volume to be owned by the pod:1. The owning GID will be the FSGroup 2. The setgid bit is set (new files created in the volume will be owned by FSGroup) 3. The permission bits are OR'd with rw-rw----If unset, the Kubelet will not modify the ownership and permissions of any volume. runAsGroup int64 The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. runAsNonRoot boolean Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. runAsUser int64 The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. seLinuxOptions SELinuxOptions The SELinux context to be applied to all containers. If unspecified, the container runtime will allocate a random SELinux context for each container. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. supplementalGroups Array< integer > A list of groups applied to the first process run in each container, in addition to the container's primary GID. If unspecified, no groups will be added to any container. sysctls Array< Sysctl > Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported sysctls (by the container runtime) might fail to launch. windowsOptions WindowsSecurityContextOptions Windows security options. Toleration \u00b6 The pod this Toleration is attached to tolerates any taint that matches the triple using the matching operator . Fields \u00b6 Field Name Field Type Description effect string Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. key string Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. operator string Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. tolerationSeconds int64 TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. value string Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. PersistentVolumeClaim \u00b6 PersistentVolumeClaim is a user's request for and claim to a persistent volume Examples (click to open) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) Examples with this field (click to open) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) Fields \u00b6 Field Name Field Type Description apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds metadata ObjectMeta Standard object's metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata spec PersistentVolumeClaimSpec Spec defines the desired characteristics of a volume requested by a pod author. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims status PersistentVolumeClaimStatus Status represents the current information/status of a persistent volume claim. Read-only. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims Volume \u00b6 Volume represents a named volume in a pod that may be accessed by any container in the pod. Examples with this field (click to open) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) Fields \u00b6 Field Name Field Type Description awsElasticBlockStore AWSElasticBlockStoreVolumeSource AWSElasticBlockStore represents an AWS Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore azureDisk AzureDiskVolumeSource AzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod. azureFile AzureFileVolumeSource AzureFile represents an Azure File Service mount on the host and bind mount to the pod. cephfs CephFSVolumeSource CephFS represents a Ceph FS mount on the host that shares a pod's lifetime cinder CinderVolumeSource Cinder represents a cinder volume attached and mounted on kubelets host machine More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md configMap ConfigMapVolumeSource ConfigMap represents a configMap that should populate this volume csi CSIVolumeSource CSI (Container Storage Interface) represents storage that is handled by an external CSI driver (Alpha feature). downwardAPI DownwardAPIVolumeSource DownwardAPI represents downward API about the pod that should populate this volume emptyDir EmptyDirVolumeSource EmptyDir represents a temporary directory that shares a pod's lifetime. More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir fc FCVolumeSource FC represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod. flexVolume FlexVolumeSource FlexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin. flocker FlockerVolumeSource Flocker represents a Flocker volume attached to a kubelet's host machine. This depends on the Flocker control service being running gcePersistentDisk GCEPersistentDiskVolumeSource GCEPersistentDisk represents a GCE Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk ~ gitRepo ~ ~ GitRepoVolumeSource ~ ~GitRepo represents a git repository at a particular revision.~ DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir into the Pod's container. glusterfs GlusterfsVolumeSource Glusterfs represents a Glusterfs mount on the host that shares a pod's lifetime. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md hostPath HostPathVolumeSource HostPath represents a pre-existing file or directory on the host machine that is directly exposed to the container. This is generally used for system agents or other privileged things that are allowed to see the host machine. Most containers will NOT need this. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath iscsi ISCSIVolumeSource ISCSI represents an ISCSI Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://releases.k8s.io/HEAD/examples/volumes/iscsi/README.md name string Volume's name. Must be a DNS_LABEL and unique within the pod. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names nfs NFSVolumeSource NFS represents an NFS mount on the host that shares a pod's lifetime More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs persistentVolumeClaim PersistentVolumeClaimVolumeSource PersistentVolumeClaimVolumeSource represents a reference to a PersistentVolumeClaim in the same namespace. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims photonPersistentDisk PhotonPersistentDiskVolumeSource PhotonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine portworxVolume PortworxVolumeSource PortworxVolume represents a portworx volume attached and mounted on kubelets host machine projected ProjectedVolumeSource Items for all in one resources secrets, configmaps, and downward API quobyte QuobyteVolumeSource Quobyte represents a Quobyte mount on the host that shares a pod's lifetime rbd RBDVolumeSource RBD represents a Rados Block Device mount on the host that shares a pod's lifetime. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md scaleIO ScaleIOVolumeSource ScaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes. secret SecretVolumeSource Secret represents a secret that should populate this volume. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret storageos StorageOSVolumeSource StorageOS represents a StorageOS volume attached and mounted on Kubernetes nodes. vsphereVolume VsphereVirtualDiskVolumeSource VsphereVolume represents a vSphere volume attached and mounted on kubelets host machine Time \u00b6 Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON. Wrappers are provided for many of the factory methods that the time package offers. ObjectReference \u00b6 ObjectReference contains enough information to let you inspect or modify the referred object. Fields \u00b6 Field Name Field Type Description apiVersion string API version of the referent. fieldPath string If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names namespace string Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ resourceVersion string Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency uid string UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids Container \u00b6 A single application container that you want to run within a pod. Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) Fields \u00b6 Field Name Field Type Description args Array< string > Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell command Array< string > Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell env Array< EnvVar > List of environment variables to set in the container. Cannot be updated. envFrom Array< EnvFromSource > List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. image string Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets. imagePullPolicy string Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images lifecycle Lifecycle Actions that the management system should take in response to container lifecycle events. Cannot be updated. livenessProbe Probe Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes name string Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated. ports Array< ContainerPort > List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \"0.0.0.0\" address inside a container will be accessible from the network. Cannot be updated. readinessProbe Probe Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes resources ResourceRequirements Compute Resources required by this container. Cannot be updated. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ securityContext SecurityContext Security options the pod should run with. More info: https://kubernetes.io/docs/concepts/policy/security-context/ More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ stdin boolean Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false. stdinOnce boolean Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false terminationMessagePath string Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated. terminationMessagePolicy string Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated. tty boolean Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false. volumeDevices Array< VolumeDevice > volumeDevices is the list of block devices to be used by the container. This is a beta feature. volumeMounts Array< VolumeMount > Pod volumes to mount into the container's filesystem. Cannot be updated. workingDir string Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated. IntOrString \u00b6 IntOrString is a type that can hold an int32 or a string. When used in JSON or YAML marshalling and unmarshalling, it produces or consumes the inner type. This allows you to have, for example, a JSON field that can accept a name or number. Examples with this field (click to open) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) EnvVar \u00b6 EnvVar represents an environment variable present in a Container. Examples with this field (click to open) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) Fields \u00b6 Field Name Field Type Description name string Name of the environment variable. Must be a C_IDENTIFIER. value string Variable references $(VAR_NAME) are expanded using the previous defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to \"\". valueFrom EnvVarSource Source for the environment variable's value. Cannot be used if value is not empty. EnvFromSource \u00b6 EnvFromSource represents the source of a set of ConfigMaps Fields \u00b6 Field Name Field Type Description configMapRef ConfigMapEnvSource The ConfigMap to select from prefix string An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER. secretRef SecretEnvSource The Secret to select from Lifecycle \u00b6 Lifecycle describes actions that the management system should take in response to container lifecycle events. For the PostStart and PreStop lifecycle handlers, management of the container blocks until the action is complete, unless the container process fails, in which case the handler is aborted. Fields \u00b6 Field Name Field Type Description postStart Handler PostStart is called immediately after a container is created. If the handler fails, the container is terminated and restarted according to its restart policy. Other management of the container blocks until the hook completes. More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks preStop Handler PreStop is called immediately before a container is terminated due to an API request or management event such as liveness probe failure, preemption, resource contention, etc. The handler is not called if the container crashes or exits. The reason for termination is passed to the handler. The Pod's termination grace period countdown begins before the PreStop hooked is executed. Regardless of the outcome of the handler, the container will eventually terminate within the Pod's termination grace period. Other management of the container blocks until the hook completes or until the termination grace period is reached. More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks Probe \u00b6 Probe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic. Fields \u00b6 Field Name Field Type Description exec ExecAction One and only one of the following should be specified. Exec specifies the action to take. failureThreshold int32 Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1. httpGet HTTPGetAction HTTPGet specifies the http request to perform. initialDelaySeconds int32 Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes periodSeconds int32 How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. successThreshold int32 Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness. Minimum value is 1. tcpSocket TCPSocketAction TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported timeoutSeconds int32 Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes ContainerPort \u00b6 ContainerPort represents a network port in a single container. Examples with this field (click to open) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) Fields \u00b6 Field Name Field Type Description containerPort int32 Number of port to expose on the pod's IP address. This must be a valid port number, 0 < x < 65536. hostIP string What host IP to bind the external port to. hostPort int32 Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. If HostNetwork is specified, this must match ContainerPort. Most containers do not need this. name string If specified, this must be an IANA_SVC_NAME and unique within the pod. Each named port in a pod must have a unique name. Name for the port that can be referred to by services. protocol string Protocol for port. Must be UDP, TCP, or SCTP. Defaults to \"TCP\". ResourceRequirements \u00b6 ResourceRequirements describes the compute resource requirements. Examples with this field (click to open) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) Fields \u00b6 Field Name Field Type Description limits Quantity Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ requests Quantity Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ SecurityContext \u00b6 SecurityContext holds security configuration that will be applied to a container. Some fields are present in both SecurityContext and PodSecurityContext. When both are set, the values in SecurityContext take precedence. Examples with this field (click to open) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) Fields \u00b6 Field Name Field Type Description allowPrivilegeEscalation boolean AllowPrivilegeEscalation controls whether a process can gain more privileges than its parent process. This bool directly controls if the no_new_privs flag will be set on the container process. AllowPrivilegeEscalation is true always when the container is: 1) run as Privileged 2) has CAP_SYS_ADMIN capabilities Capabilities The capabilities to add/drop when running containers. Defaults to the default set of capabilities granted by the container runtime. privileged boolean Run container in privileged mode. Processes in privileged containers are essentially equivalent to root on the host. Defaults to false. procMount string procMount denotes the type of proc mount to use for the containers. The default is DefaultProcMount which uses the container runtime defaults for readonly paths and masked paths. This requires the ProcMountType feature flag to be enabled. readOnlyRootFilesystem boolean Whether this container has a read-only root filesystem. Default is false. runAsGroup int64 The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. runAsNonRoot boolean Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. runAsUser int64 The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. seLinuxOptions SELinuxOptions The SELinux context to be applied to the container. If unspecified, the container runtime will allocate a random SELinux context for each container. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. windowsOptions WindowsSecurityContextOptions Windows security options. VolumeDevice \u00b6 volumeDevice describes a mapping of a raw block device within a container. Fields \u00b6 Field Name Field Type Description devicePath string devicePath is the path inside of the container that the device will be mapped to. name string name must match the name of a persistentVolumeClaim in the pod VolumeMount \u00b6 VolumeMount describes a mounting of a Volume within a container. Examples with this field (click to open) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) Fields \u00b6 Field Name Field Type Description mountPath string Path within the container at which the volume should be mounted. Must not contain ':'. mountPropagation string mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. This field is beta in 1.10. name string This must match the Name of a Volume. readOnly boolean Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. subPath string Path within the volume from which the container's volume should be mounted. Defaults to \"\" (volume's root). subPathExpr string Expanded path within the volume from which the container's volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment. Defaults to \"\" (volume's root). SubPathExpr and SubPath are mutually exclusive. This field is beta in 1.15. SecretKeySelector \u00b6 SecretKeySelector selects a key of a Secret. Examples with this field (click to open) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) Fields \u00b6 Field Name Field Type Description key string The key of the secret to select from. Must be a valid secret key. name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names optional boolean Specify whether the Secret or its key must be defined ConfigMapKeySelector \u00b6 Selects a key from a ConfigMap. Examples with this field (click to open) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) Fields \u00b6 Field Name Field Type Description key string The key to select. name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names optional boolean Specify whether the ConfigMap or its key must be defined Initializers \u00b6 Initializers tracks the progress of initialization. Fields \u00b6 Field Name Field Type Description pending Array< Initializer > Pending is a list of initializers that must execute in order before this object is visible. When the last pending initializer is removed, and no failing result is set, the initializers struct will be set to nil and the object is considered as initialized and visible to all clients. result Status If result is set with the Failure field, the object will be persisted to storage and then deleted, ensuring that other clients can observe the deletion. ManagedFieldsEntry \u00b6 ManagedFieldsEntry is a workflow-id, a FieldSet and the group version of the resource that the fieldset applies to. Fields \u00b6 Field Name Field Type Description apiVersion string APIVersion defines the version of this resource that this field set applies to. The format is \"group/version\" just like the top-level APIVersion field. It is necessary to track the version of a field set because it cannot be automatically converted. fields Fields Fields identifies a set of fields. manager string Manager is an identifier of the workflow managing these fields. operation string Operation is the type of operation which lead to this ManagedFieldsEntry being created. The only valid values for this field are 'Apply' and 'Update'. time Time Time is timestamp of when these fields were set. It should always be empty if Operation is 'Apply' OwnerReference \u00b6 OwnerReference contains enough information to let you identify an owning object. An owning object must be in the same namespace as the dependent, or be cluster-scoped, so there is no namespace field. Examples with this field (click to open) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) Fields \u00b6 Field Name Field Type Description apiVersion string API version of the referent. blockOwnerDeletion boolean If true, AND if the owner has the \"foregroundDeletion\" finalizer, then the owner cannot be deleted from the key-value store until this reference is removed. Defaults to false. To set this field, a user needs \"delete\" permission of the owner, otherwise 422 (Unprocessable Entity) will be returned. controller boolean If true, this reference points to the managing controller. kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds name string Name of the referent. More info: http://kubernetes.io/docs/user-guide/identifiers#names uid string UID of the referent. More info: http://kubernetes.io/docs/user-guide/identifiers#uids NodeAffinity \u00b6 Node affinity is a group of node affinity scheduling rules. Fields \u00b6 Field Name Field Type Description preferredDuringSchedulingIgnoredDuringExecution Array< PreferredSchedulingTerm > The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node matches the corresponding matchExpressions; the node(s) with the highest sum are the most preferred. requiredDuringSchedulingIgnoredDuringExecution NodeSelector If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to an update), the system may or may not try to eventually evict the pod from its node. PodAffinity \u00b6 Pod affinity is a group of inter pod affinity scheduling rules. Fields \u00b6 Field Name Field Type Description preferredDuringSchedulingIgnoredDuringExecution Array< WeightedPodAffinityTerm > The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred. requiredDuringSchedulingIgnoredDuringExecution Array< PodAffinityTerm > If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied. PodAntiAffinity \u00b6 Pod anti affinity is a group of inter pod anti affinity scheduling rules. Fields \u00b6 Field Name Field Type Description preferredDuringSchedulingIgnoredDuringExecution Array< WeightedPodAffinityTerm > The scheduler will prefer to schedule pods to nodes that satisfy the anti-affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred. requiredDuringSchedulingIgnoredDuringExecution Array< PodAffinityTerm > If the anti-affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the anti-affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied. PodDNSConfigOption \u00b6 PodDNSConfigOption defines DNS resolver options of a pod. Examples with this field (click to open) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) Fields \u00b6 Field Name Field Type Description name string Required. value string No description available LabelSelector \u00b6 A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects. Examples with this field (click to open) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) Fields \u00b6 Field Name Field Type Description matchExpressions Array< LabelSelectorRequirement > matchExpressions is a list of label selector requirements. The requirements are ANDed. matchLabels Map< string , string > matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. SELinuxOptions \u00b6 SELinuxOptions are the labels to be applied to the container Fields \u00b6 Field Name Field Type Description level string Level is SELinux level label that applies to the container. role string Role is a SELinux role label that applies to the container. type string Type is a SELinux type label that applies to the container. user string User is a SELinux user label that applies to the container. Sysctl \u00b6 Sysctl defines a kernel parameter to be set Fields \u00b6 Field Name Field Type Description name string Name of a property to set value string Value of a property to set WindowsSecurityContextOptions \u00b6 WindowsSecurityContextOptions contain Windows-specific options and credentials. Fields \u00b6 Field Name Field Type Description gmsaCredentialSpec string GMSACredentialSpec is where the GMSA admission webhook (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the GMSA credential spec named by the GMSACredentialSpecName field. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag. gmsaCredentialSpecName string GMSACredentialSpecName is the name of the GMSA credential spec to use. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag. PersistentVolumeClaimSpec \u00b6 PersistentVolumeClaimSpec describes the common attributes of storage devices and allows a Source for provider-specific attributes Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml) Fields \u00b6 Field Name Field Type Description accessModes Array< string > AccessModes contains the desired access modes the volume should have. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1 dataSource TypedLocalObjectReference This field requires the VolumeSnapshotDataSource alpha feature gate to be enabled and currently VolumeSnapshot is the only supported data source. If the provisioner can support VolumeSnapshot data source, it will create a new volume and data will be restored to the volume at the same time. If the provisioner does not support VolumeSnapshot data source, volume will not be created and the failure will be reported as an event. In the future, we plan to support more data source types and the behavior of the provisioner may change. resources ResourceRequirements Resources represents the minimum resources the volume should have. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources selector LabelSelector A label query over volumes to consider for binding. storageClassName string Name of the StorageClass required by the claim. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1 volumeMode string volumeMode defines what type of volume is required by the claim. Value of Filesystem is implied when not included in claim spec. This is a beta feature. volumeName string VolumeName is the binding reference to the PersistentVolume backing this claim. PersistentVolumeClaimStatus \u00b6 PersistentVolumeClaimStatus is the current status of a persistent volume claim. Fields \u00b6 Field Name Field Type Description accessModes Array< string > AccessModes contains the actual access modes the volume backing the PVC has. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1 capacity Quantity Represents the actual resources of the underlying volume. conditions Array< PersistentVolumeClaimCondition > Current Condition of persistent volume claim. If underlying persistent volume is being resized then the Condition will be set to 'ResizeStarted'. phase string Phase represents the current phase of PersistentVolumeClaim. AWSElasticBlockStoreVolumeSource \u00b6 Represents a Persistent Disk resource in AWS.An AWS EBS disk must exist before mounting to a container. The disk must also be in the same AWS zone as the kubelet. An AWS EBS disk can only be mounted as read/write once. AWS EBS volumes support ownership management and SELinux relabeling. Fields \u00b6 Field Name Field Type Description fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore partition int32 The partition in the volume that you want to mount. If omitted, the default is to mount by volume name. Examples: For volume /dev/sda1, you specify the partition as \"1\". Similarly, the volume partition for /dev/sda is \"0\" (or you can leave the property empty). readOnly boolean Specify \"true\" to force and set the ReadOnly property in VolumeMounts to \"true\". If omitted, the default is \"false\". More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore volumeID string Unique ID of the persistent disk resource in AWS (Amazon EBS volume). More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore AzureDiskVolumeSource \u00b6 AzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod. Fields \u00b6 Field Name Field Type Description cachingMode string Host Caching mode: None, Read Only, Read Write. diskName string The Name of the data disk in the blob storage diskURI string The URI the data disk in the blob storage fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. kind string Expected values Shared: multiple blob disks per storage account Dedicated: single blob disk per storage account Managed: azure managed data disk (only in managed availability set). defaults to shared readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. AzureFileVolumeSource \u00b6 AzureFile represents an Azure File Service mount on the host and bind mount to the pod. Fields \u00b6 Field Name Field Type Description readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. secretName string the name of secret that contains Azure Storage Account Name and Key shareName string Share Name CephFSVolumeSource \u00b6 Represents a Ceph Filesystem mount that lasts the lifetime of a pod Cephfs volumes do not support ownership management or SELinux relabeling. Fields \u00b6 Field Name Field Type Description monitors Array< string > Required: Monitors is a collection of Ceph monitors More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it path string Optional: Used as the mounted root, rather than the full Ceph tree, default is / readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it secretFile string Optional: SecretFile is the path to key ring for User, default is /etc/ceph/user.secret More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it secretRef LocalObjectReference Optional: SecretRef is reference to the authentication secret for User, default is empty. More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it user string Optional: User is the rados user name, default is admin More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it CinderVolumeSource \u00b6 Represents a cinder volume resource in Openstack. A Cinder volume must exist before mounting to a container. The volume must also be in the same region as the kubelet. Cinder volumes support ownership management and SELinux relabeling. Fields \u00b6 Field Name Field Type Description fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md secretRef LocalObjectReference Optional: points to a secret object containing parameters used to connect to OpenStack. volumeID string volume id used to identify the volume in cinder More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md ConfigMapVolumeSource \u00b6 Adapts a ConfigMap into a volume.The contents of the target ConfigMap's Data field will be presented in a volume as files using the keys in the Data field as the file names, unless the items element is populated with specific mappings of keys to paths. ConfigMap volumes support ownership management and SELinux relabeling. Fields \u00b6 Field Name Field Type Description defaultMode int32 Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. items Array< KeyToPath > If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names optional boolean Specify whether the ConfigMap or its keys must be defined CSIVolumeSource \u00b6 Represents a source location of a volume to mount, managed by an external CSI driver Fields \u00b6 Field Name Field Type Description driver string Driver is the name of the CSI driver that handles this volume. Consult with your admin for the correct name as registered in the cluster. fsType string Filesystem type to mount. Ex. \"ext4\", \"xfs\", \"ntfs\". If not provided, the empty value is passed to the associated CSI driver which will determine the default filesystem to apply. nodePublishSecretRef LocalObjectReference NodePublishSecretRef is a reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI NodePublishVolume and NodeUnpublishVolume calls. This field is optional, and may be empty if no secret is required. If the secret object contains more than one secret, all secret references are passed. readOnly boolean Specifies a read-only configuration for the volume. Defaults to false (read/write). volumeAttributes Map< string , string > VolumeAttributes stores driver-specific properties that are passed to the CSI driver. Consult your driver's documentation for supported values. DownwardAPIVolumeSource \u00b6 DownwardAPIVolumeSource represents a volume containing downward API info. Downward API volumes support ownership management and SELinux relabeling. Fields \u00b6 Field Name Field Type Description defaultMode int32 Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. items Array< DownwardAPIVolumeFile > Items is a list of downward API volume file EmptyDirVolumeSource \u00b6 Represents an empty directory for a pod. Empty directory volumes support ownership management and SELinux relabeling. Examples with this field (click to open) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) Fields \u00b6 Field Name Field Type Description medium string What type of storage medium should back this directory. The default is \"\" which means to use the node's default medium. Must be an empty string (default) or Memory. More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir sizeLimit Quantity Total amount of local storage required for this EmptyDir volume. The size limit is also applicable for memory medium. The maximum usage on memory medium EmptyDir would be the minimum value between the SizeLimit specified here and the sum of memory limits of all containers in a pod. The default is nil which means that the limit is undefined. More info: http://kubernetes.io/docs/user-guide/volumes#emptydir FCVolumeSource \u00b6 Represents a Fibre Channel volume. Fibre Channel volumes can only be mounted as read/write once. Fibre Channel volumes support ownership management and SELinux relabeling. Fields \u00b6 Field Name Field Type Description fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. lun int32 Optional: FC target lun number readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. targetWWNs Array< string > Optional: FC target worldwide names (WWNs) wwids Array< string > Optional: FC volume world wide identifiers (wwids) Either wwids or combination of targetWWNs and lun must be set, but not both simultaneously. FlexVolumeSource \u00b6 FlexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin. Fields \u00b6 Field Name Field Type Description driver string Driver is the name of the driver to use for this volume. fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". The default filesystem depends on FlexVolume script. options Map< string , string > Optional: Extra command options if any. readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. secretRef LocalObjectReference Optional: SecretRef is reference to the secret object containing sensitive information to pass to the plugin scripts. This may be empty if no secret object is specified. If the secret object contains more than one secret, all secrets are passed to the plugin scripts. FlockerVolumeSource \u00b6 Represents a Flocker volume mounted by the Flocker agent. One and only one of datasetName and datasetUUID should be set. Flocker volumes do not support ownership management or SELinux relabeling. Fields \u00b6 Field Name Field Type Description datasetName string Name of the dataset stored as metadata -> name on the dataset for Flocker should be considered as deprecated datasetUUID string UUID of the dataset. This is unique identifier of a Flocker dataset GCEPersistentDiskVolumeSource \u00b6 Represents a Persistent Disk resource in Google Compute Engine.A GCE PD must exist before mounting to a container. The disk must also be in the same GCE project and zone as the kubelet. A GCE PD can only be mounted as read/write once or read-only many times. GCE PDs support ownership management and SELinux relabeling. Fields \u00b6 Field Name Field Type Description fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk partition int32 The partition in the volume that you want to mount. If omitted, the default is to mount by volume name. Examples: For volume /dev/sda1, you specify the partition as \"1\". Similarly, the volume partition for /dev/sda is \"0\" (or you can leave the property empty). More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk pdName string Unique name of the PD resource in GCE. Used to identify the disk in GCE. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk readOnly boolean ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk GitRepoVolumeSource \u00b6 Represents a volume that is populated with the contents of a git repository. Git repo volumes do not support ownership management. Git repo volumes support SELinux relabeling.DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir into the Pod's container. Fields \u00b6 Field Name Field Type Description directory string Target directory name. Must not contain or start with '..'. If '.' is supplied, the volume directory will be the git repository. Otherwise, if specified, the volume will contain the git repository in the subdirectory with the given name. repository string Repository URL revision string Commit hash for the specified revision. GlusterfsVolumeSource \u00b6 Represents a Glusterfs mount that lasts the lifetime of a pod. Glusterfs volumes do not support ownership management or SELinux relabeling. Fields \u00b6 Field Name Field Type Description endpoints string EndpointsName is the endpoint name that details Glusterfs topology. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md#create-a-pod path string Path is the Glusterfs volume path. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md#create-a-pod readOnly boolean ReadOnly here will force the Glusterfs volume to be mounted with read-only permissions. Defaults to false. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md#create-a-pod HostPathVolumeSource \u00b6 Represents a host path mapped into a pod. Host path volumes do not support ownership management or SELinux relabeling. Fields \u00b6 Field Name Field Type Description path string Path of the directory on the host. If the path is a symlink, it will follow the link to the real path. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath type string Type for HostPath Volume Defaults to \"\" More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath ISCSIVolumeSource \u00b6 Represents an ISCSI disk. ISCSI volumes can only be mounted as read/write once. ISCSI volumes support ownership management and SELinux relabeling. Fields \u00b6 Field Name Field Type Description chapAuthDiscovery boolean whether support iSCSI Discovery CHAP authentication chapAuthSession boolean whether support iSCSI Session CHAP authentication fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#iscsi initiatorName string Custom iSCSI Initiator Name. If initiatorName is specified with iscsiInterface simultaneously, new iSCSI interface : will be created for the connection. iqn string Target iSCSI Qualified Name. iscsiInterface string iSCSI Interface Name that uses an iSCSI transport. Defaults to 'default' (tcp). lun int32 iSCSI Target Lun number. portals Array< string > iSCSI Target Portal List. The portal is either an IP or ip_addr:port if the port is other than default (typically TCP ports 860 and 3260). readOnly boolean ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. secretRef LocalObjectReference CHAP Secret for iSCSI target and initiator authentication targetPortal string iSCSI Target Portal. The Portal is either an IP or ip_addr:port if the port is other than default (typically TCP ports 860 and 3260). NFSVolumeSource \u00b6 Represents an NFS mount that lasts the lifetime of a pod. NFS volumes do not support ownership management or SELinux relabeling. Fields \u00b6 Field Name Field Type Description path string Path that is exported by the NFS server. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs readOnly boolean ReadOnly here will force the NFS export to be mounted with read-only permissions. Defaults to false. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs server string Server is the hostname or IP address of the NFS server. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs PersistentVolumeClaimVolumeSource \u00b6 PersistentVolumeClaimVolumeSource references the user's PVC in the same namespace. This volume finds the bound PV and mounts that volume for the pod. A PersistentVolumeClaimVolumeSource is, essentially, a wrapper around another type of volume that is owned by someone else (the system). Examples with this field (click to open) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) Fields \u00b6 Field Name Field Type Description claimName string ClaimName is the name of a PersistentVolumeClaim in the same namespace as the pod using this volume. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims readOnly boolean Will force the ReadOnly setting in VolumeMounts. Default false. PhotonPersistentDiskVolumeSource \u00b6 Represents a Photon Controller persistent disk resource. Fields \u00b6 Field Name Field Type Description fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. pdID string ID that identifies Photon Controller persistent disk PortworxVolumeSource \u00b6 PortworxVolumeSource represents a Portworx volume resource. Fields \u00b6 Field Name Field Type Description fsType string FSType represents the filesystem type to mount Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\". Implicitly inferred to be \"ext4\" if unspecified. readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. volumeID string VolumeID uniquely identifies a Portworx volume ProjectedVolumeSource \u00b6 Represents a projected volume source Fields \u00b6 Field Name Field Type Description defaultMode int32 Mode bits to use on created files by default. Must be a value between 0 and 0777. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. sources Array< VolumeProjection > list of volume projections QuobyteVolumeSource \u00b6 Represents a Quobyte mount that lasts the lifetime of a pod. Quobyte volumes do not support ownership management or SELinux relabeling. Fields \u00b6 Field Name Field Type Description group string Group to map volume access to Default is no group readOnly boolean ReadOnly here will force the Quobyte volume to be mounted with read-only permissions. Defaults to false. registry string Registry represents a single or multiple Quobyte Registry services specified as a string as host:port pair (multiple entries are separated with commas) which acts as the central registry for volumes tenant string Tenant owning the given Quobyte volume in the Backend Used with dynamically provisioned Quobyte volumes, value is set by the plugin user string User to map volume access to Defaults to serivceaccount user volume string Volume is a string that references an already created Quobyte volume by name. RBDVolumeSource \u00b6 Represents a Rados Block Device mount that lasts the lifetime of a pod. RBD volumes support ownership management and SELinux relabeling. Fields \u00b6 Field Name Field Type Description fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#rbd image string The rados image name. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it keyring string Keyring is the path to key ring for RBDUser. Default is /etc/ceph/keyring. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it monitors Array< string > A collection of Ceph monitors. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it pool string The rados pool name. Default is rbd. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it readOnly boolean ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it secretRef LocalObjectReference SecretRef is name of the authentication secret for RBDUser. If provided overrides keyring. Default is nil. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it user string The rados user name. Default is admin. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it ScaleIOVolumeSource \u00b6 ScaleIOVolumeSource represents a persistent ScaleIO volume Fields \u00b6 Field Name Field Type Description fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Default is \"xfs\". gateway string The host address of the ScaleIO API Gateway. protectionDomain string The name of the ScaleIO Protection Domain for the configured storage. readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. secretRef LocalObjectReference SecretRef references to the secret for ScaleIO user and other sensitive information. If this is not provided, Login operation will fail. sslEnabled boolean Flag to enable/disable SSL communication with Gateway, default false storageMode string Indicates whether the storage for a volume should be ThickProvisioned or ThinProvisioned. Default is ThinProvisioned. storagePool string The ScaleIO Storage Pool associated with the protection domain. system string The name of the storage system as configured in ScaleIO. volumeName string The name of a volume already created in the ScaleIO system that is associated with this volume source. SecretVolumeSource \u00b6 Adapts a Secret into a volume.The contents of the target Secret's Data field will be presented in a volume as files using the keys in the Data field as the file names. Secret volumes support ownership management and SELinux relabeling. Examples with this field (click to open) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) Fields \u00b6 Field Name Field Type Description defaultMode int32 Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. items Array< KeyToPath > If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. optional boolean Specify whether the Secret or its keys must be defined secretName string Name of the secret in the pod's namespace to use. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret StorageOSVolumeSource \u00b6 Represents a StorageOS persistent volume resource. Fields \u00b6 Field Name Field Type Description fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. secretRef LocalObjectReference SecretRef specifies the secret to use for obtaining the StorageOS API credentials. If not specified, default values will be attempted. volumeName string VolumeName is the human-readable name of the StorageOS volume. Volume names are only unique within a namespace. volumeNamespace string VolumeNamespace specifies the scope of the volume within StorageOS. If no namespace is specified then the Pod's namespace will be used. This allows the Kubernetes name scoping to be mirrored within StorageOS for tighter integration. Set VolumeName to any name to override the default behaviour. Set to \"default\" if you are not using namespaces within StorageOS. Namespaces that do not pre-exist within StorageOS will be created. VsphereVirtualDiskVolumeSource \u00b6 Represents a vSphere volume resource. Fields \u00b6 Field Name Field Type Description fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. storagePolicyID string Storage Policy Based Management (SPBM) profile ID associated with the StoragePolicyName. storagePolicyName string Storage Policy Based Management (SPBM) profile name. volumePath string Path that identifies vSphere volume vmdk EnvVarSource \u00b6 EnvVarSource represents a source for the value of an EnvVar. Examples with this field (click to open) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) Fields \u00b6 Field Name Field Type Description configMapKeyRef ConfigMapKeySelector Selects a key of a ConfigMap. fieldRef ObjectFieldSelector Selects a field of the pod: supports metadata.name, metadata.namespace, metadata.labels, metadata.annotations, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP. resourceFieldRef ResourceFieldSelector Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. secretKeyRef SecretKeySelector Selects a key of a secret in the pod's namespace ConfigMapEnvSource \u00b6 ConfigMapEnvSource selects a ConfigMap to populate the environment variables with.The contents of the target ConfigMap's Data field will represent the key-value pairs as environment variables. Fields \u00b6 Field Name Field Type Description name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names optional boolean Specify whether the ConfigMap must be defined SecretEnvSource \u00b6 SecretEnvSource selects a Secret to populate the environment variables with.The contents of the target Secret's Data field will represent the key-value pairs as environment variables. Fields \u00b6 Field Name Field Type Description name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names optional boolean Specify whether the Secret must be defined Handler \u00b6 Handler defines a specific action that should be taken Fields \u00b6 Field Name Field Type Description exec ExecAction One and only one of the following should be specified. Exec specifies the action to take. httpGet HTTPGetAction HTTPGet specifies the http request to perform. tcpSocket TCPSocketAction TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported ExecAction \u00b6 ExecAction describes a \"run in container\" action. Fields \u00b6 Field Name Field Type Description command Array< string > Command is the command line to execute inside the container, the working directory for the command is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions (' HTTPGetAction \u00b6 HTTPGetAction describes an action based on HTTP Get requests. Examples with this field (click to open) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) Fields \u00b6 Field Name Field Type Description host string Host name to connect to, defaults to the pod IP. You probably want to set \"Host\" in httpHeaders instead. httpHeaders Array< HTTPHeader > Custom headers to set in the request. HTTP allows repeated headers. path string Path to access on the HTTP server. port IntOrString Name or number of the port to access on the container. Number must be in the range 1 to 65535. Name must be an IANA_SVC_NAME. scheme string Scheme to use for connecting to the host. Defaults to HTTP. TCPSocketAction \u00b6 TCPSocketAction describes an action based on opening a socket Fields \u00b6 Field Name Field Type Description host string Optional: Host name to connect to, defaults to the pod IP. port IntOrString Number or name of the port to access on the container. Number must be in the range 1 to 65535. Name must be an IANA_SVC_NAME. Quantity \u00b6 Quantity is a fixed-point representation of a number. It provides convenient marshaling/unmarshaling in JSON and YAML, in addition to String() and Int64() accessors.The serialization format is: ::= (Note that may be empty, from the \"\" case in .) ::= 0 | 1 | ... | 9 ::= | ::= | . | . | . ::= \"+\" | \"-\" ::= | ::= | | ::= Ki | Mi | Gi | Ti | Pi | Ei (International System of units; See: http://physics.nist.gov/cuu/Units/binary.html) ::= m | \"\" | k | M | G | T | P | E (Note that 1024 = 1Ki but 1000 = 1k; I didn't choose the capitalization.) ::= \"e\" | \"E\" No matter which of the three exponent forms is used, no quantity may represent a number greater than 2^63-1 in magnitude, nor may it have more than 3 decimal places. Numbers larger or more precise will be capped or rounded up. (E.g.: 0.1m will rounded up to 1m.) This may be extended in the future if we require larger or smaller quantities.When a Quantity is parsed from a string, it will remember the type of suffix it had, and will use the same type again when it is serialized.Before serializing, Quantity will be put in \"canonical form\". This means that Exponent/suffix will be adjusted up or down (with a corresponding increase or decrease in Mantissa) such that: a. No precision is lost b. No fractional digits will be emitted c. The exponent (or suffix) is as large as possible.The sign will be omitted unless the number is negative.Examples: 1.5 will be serialized as \"1500m\" 1.5Gi will be serialized as \"1536Mi\"Note that the quantity will NEVER be internally represented by a floating point number. That is the whole point of this exercise.Non-canonical values will still parse as long as they are well formed, but will be re-emitted in their canonical form. (So always use canonical form, or don't diff.)This format is intended to make it difficult to use these numbers without writing some sort of special handling code in the hopes that that will cause implementors to also use a fixed point implementation. Examples with this field (click to open) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) Capabilities \u00b6 Adds and removes POSIX capabilities from running containers. Fields \u00b6 Field Name Field Type Description add Array< string > Added capabilities drop Array< string > Removed capabilities Initializer \u00b6 Initializer is information about an initializer that has not yet completed. Fields \u00b6 Field Name Field Type Description name string name of the process that is responsible for initializing this object. Status \u00b6 Status is a return value for calls that don't return other objects. Fields \u00b6 Field Name Field Type Description apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources code int32 Suggested HTTP return code for this status, 0 if not set. details StatusDetails Extended data associated with the reason. Each reason may define its own extended details. This field is optional and the data returned is not guaranteed to conform to any schema except that defined by the reason type. kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds message string A human-readable description of the status of this operation. metadata ListMeta Standard list metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds reason string A machine-readable description of why this operation is in the \"Failure\" status. If this value is empty there is no information available. A Reason clarifies an HTTP status code but does not override it. status string Status of the operation. One of: \"Success\" or \"Failure\". More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status Fields \u00b6 Fields stores a set of fields in a data structure like a Trie. To understand how this is used, see: https://github.com/kubernetes-sigs/structured-merge-diff PreferredSchedulingTerm \u00b6 An empty preferred scheduling term matches all objects with implicit weight 0 (i.e. it's a no-op). A null preferred scheduling term matches no objects (i.e. is also a no-op). Fields \u00b6 Field Name Field Type Description preference NodeSelectorTerm A node selector term, associated with the corresponding weight. weight int32 Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100. NodeSelector \u00b6 A node selector represents the union of the results of one or more label queries over a set of nodes; that is, it represents the OR of the selectors represented by the node selector terms. Fields \u00b6 Field Name Field Type Description nodeSelectorTerms Array< NodeSelectorTerm > Required. A list of node selector terms. The terms are ORed. WeightedPodAffinityTerm \u00b6 The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s) Fields \u00b6 Field Name Field Type Description podAffinityTerm PodAffinityTerm Required. A pod affinity term, associated with the corresponding weight. weight int32 weight associated with matching the corresponding podAffinityTerm, in the range 1-100. PodAffinityTerm \u00b6 Defines a set of pods (namely those matching the labelSelector relative to the given namespace(s)) that this pod should be co-located (affinity) or not co-located (anti-affinity) with, where co-located is defined as running on a node whose value of the label with key matches that of any node on which a pod of the set of pods is running Fields \u00b6 Field Name Field Type Description labelSelector LabelSelector A label query over a set of resources, in this case pods. namespaces Array< string > namespaces specifies which namespaces the labelSelector applies to (matches against); null or empty list means \"this pod's namespace\" topologyKey string This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching the labelSelector in the specified namespaces, where co-located is defined as running on a node whose value of the label with key topologyKey matches that of any node on which any of the selected pods is running. Empty topologyKey is not allowed. LabelSelectorRequirement \u00b6 A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Fields \u00b6 Field Name Field Type Description key string key is the label key that the selector applies to. operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. values Array< string > values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. TypedLocalObjectReference \u00b6 TypedLocalObjectReference contains enough information to let you locate the typed referenced object inside the same namespace. Fields \u00b6 Field Name Field Type Description apiGroup string APIGroup is the group for the resource being referenced. If APIGroup is not specified, the specified Kind must be in the core API group. For any other third-party types, APIGroup is required. kind string Kind is the type of resource being referenced name string Name is the name of resource being referenced PersistentVolumeClaimCondition \u00b6 PersistentVolumeClaimCondition contails details about state of pvc Fields \u00b6 Field Name Field Type Description lastProbeTime Time Last time we probed the condition. lastTransitionTime Time Last time the condition transitioned from one status to another. message string Human-readable message indicating details about last transition. reason string Unique, this should be a short, machine understandable string that gives the reason for condition's last transition. If it reports \"ResizeStarted\" that means the underlying persistent volume is being resized. status string No description available type string No description available KeyToPath \u00b6 Maps a string key to a path within a volume. Fields \u00b6 Field Name Field Type Description key string The key to project. mode int32 Optional: mode bits to use on this file, must be a value between 0 and 0777. If not specified, the volume defaultMode will be used. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. path string The relative path of the file to map the key to. May not be an absolute path. May not contain the path element '..'. May not start with the string '..'. DownwardAPIVolumeFile \u00b6 DownwardAPIVolumeFile represents information to create the file containing the pod field Fields \u00b6 Field Name Field Type Description fieldRef ObjectFieldSelector Required: Selects a field of the pod: only annotations, labels, name and namespace are supported. mode int32 Optional: mode bits to use on this file, must be a value between 0 and 0777. If not specified, the volume defaultMode will be used. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. path string Required: Path is the relative path name of the file to be created. Must not be absolute or contain the '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..' resourceFieldRef ResourceFieldSelector Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported. VolumeProjection \u00b6 Projection that may be projected along with other supported volume types Fields \u00b6 Field Name Field Type Description configMap ConfigMapProjection information about the configMap data to project downwardAPI DownwardAPIProjection information about the downwardAPI data to project secret SecretProjection information about the secret data to project serviceAccountToken ServiceAccountTokenProjection information about the serviceAccountToken data to project ObjectFieldSelector \u00b6 ObjectFieldSelector selects an APIVersioned field of an object. Fields \u00b6 Field Name Field Type Description apiVersion string Version of the schema the FieldPath is written in terms of, defaults to \"v1\". fieldPath string Path of the field to select in the specified API version. ResourceFieldSelector \u00b6 ResourceFieldSelector represents container resources (cpu, memory) and their output format Fields \u00b6 Field Name Field Type Description containerName string Container name: required for volumes, optional for env vars divisor Quantity Specifies the output format of the exposed resources, defaults to \"1\" resource string Required: resource to select HTTPHeader \u00b6 HTTPHeader describes a custom header to be used in HTTP probes Fields \u00b6 Field Name Field Type Description name string The header field name value string The header field value StatusDetails \u00b6 StatusDetails is a set of additional properties that MAY be set by the server to provide additional information about a response. The Reason field of a Status object defines what attributes will be set. Clients must ignore fields that do not match the defined type of each attribute, and should assume that any attribute may be empty, invalid, or under defined. Fields \u00b6 Field Name Field Type Description causes Array< StatusCause > The Causes array includes more details associated with the StatusReason failure. Not all StatusReasons may provide detailed causes. group string The group attribute of the resource associated with the status StatusReason. kind string The kind attribute of the resource associated with the status StatusReason. On some operations may differ from the requested resource Kind. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds name string The name attribute of the resource associated with the status StatusReason (when there is a single name which can be described). retryAfterSeconds int32 If specified, the time in seconds before the operation should be retried. Some errors may indicate the client must take an alternate action - for those errors this field may indicate how long to wait before taking the alternate action. uid string UID of the resource. (when there is a single resource which can be described). More info: http://kubernetes.io/docs/user-guide/identifiers#uids ListMeta \u00b6 ListMeta describes metadata that synthetic resources must have, including lists and various status objects. A resource may have only one of {ObjectMeta, ListMeta}. Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml) Fields \u00b6 Field Name Field Type Description continue string continue may be set if the user set a limit on the number of items returned, and indicates that the server has more data available. The value is opaque and may be used to issue another request to the endpoint that served this list to retrieve the next set of available objects. Continuing a consistent list may not be possible if the server configuration has changed or more than a few minutes have passed. The resourceVersion field returned when using this continue value will be identical to the value in the first response, unless you have received this token from an error message. remainingItemCount int64 remainingItemCount is the number of subsequent items in the list which are not included in this list response. If the list request contained label or field selectors, then the number of remaining items is unknown and the field will be left unset and omitted during serialization. If the list is complete (either because it is not chunking or because this is the last chunk), then there are no more remaining items and this field will be left unset and omitted during serialization. Servers older than v1.15 do not set this field. The intended use of the remainingItemCount is estimating the size of a collection. Clients should not rely on the remainingItemCount to be set or to be exact.This field is alpha and can be changed or removed without notice. resourceVersion string String that identifies the server's internal version of this object that can be used by clients to determine when objects have changed. Value must be treated as opaque by clients and passed unmodified back to the server. Populated by the system. Read-only. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency selfLink string selfLink is a URL representing this object. Populated by the system. Read-only. NodeSelectorTerm \u00b6 A null or empty node selector term matches no objects. The requirements of them are ANDed. The TopologySelectorTerm type implements a subset of the NodeSelectorTerm. Fields \u00b6 Field Name Field Type Description matchExpressions Array< NodeSelectorRequirement > A list of node selector requirements by node's labels. matchFields Array< NodeSelectorRequirement > A list of node selector requirements by node's fields. ConfigMapProjection \u00b6 Adapts a ConfigMap into a projected volume.The contents of the target ConfigMap's Data field will be presented in a projected volume as files using the keys in the Data field as the file names, unless the items element is populated with specific mappings of keys to paths. Note that this is identical to a configmap volume source without the default mode. Fields \u00b6 Field Name Field Type Description items Array< KeyToPath > If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names optional boolean Specify whether the ConfigMap or its keys must be defined DownwardAPIProjection \u00b6 Represents downward API info for projecting into a projected volume. Note that this is identical to a downwardAPI volume source without the default mode. Fields \u00b6 Field Name Field Type Description items Array< DownwardAPIVolumeFile > Items is a list of DownwardAPIVolume file SecretProjection \u00b6 Adapts a secret into a projected volume.The contents of the target Secret's Data field will be presented in a projected volume as files using the keys in the Data field as the file names. Note that this is identical to a secret volume source without the default mode. Examples with this field (click to open) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) Fields \u00b6 Field Name Field Type Description items Array< KeyToPath > If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names optional boolean Specify whether the Secret or its key must be defined ServiceAccountTokenProjection \u00b6 ServiceAccountTokenProjection represents a projected service account token volume. This projection can be used to insert a service account token into the pods runtime filesystem for use against APIs (Kubernetes API Server or otherwise). Fields \u00b6 Field Name Field Type Description audience string Audience is the intended audience of the token. A recipient of a token must identify itself with an identifier specified in the audience of the token, and otherwise should reject the token. The audience defaults to the identifier of the apiserver. expirationSeconds int64 ExpirationSeconds is the requested duration of validity of the service account token. As the token approaches expiration, the kubelet volume plugin will proactively rotate the service account token. The kubelet will start trying to rotate the token if the token is older than 80 percent of its time to live or if the token is older than 24 hours.Defaults to 1 hour and must be at least 10 minutes. path string Path is the path relative to the mount point of the file to project the token into. StatusCause \u00b6 StatusCause provides more information about an api.Status failure, including cases when multiple errors are encountered. Fields \u00b6 Field Name Field Type Description field string The field of the resource that has caused this error, as named by its JSON serialization. May include dot and postfix notation for nested attributes. Arrays are zero-indexed. Fields may appear more than once in an array of causes due to fields having multiple errors. Optional.Examples: \"name\" - the field \"name\" on the current resource \"items[0].name\" - the field \"name\" on the first array entry in \"items\" message string A human-readable description of the cause of the error. This field may be presented as-is to a reader. reason string A machine-readable description of the cause of the error. If this value is empty there is no information available. NodeSelectorRequirement \u00b6 A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Fields \u00b6 Field Name Field Type Description key string The label key that the selector applies to. operator string Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt. values Array< string > An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the values array must have a single element, which will be interpreted as an integer. This array is replaced during a strategic merge patch.","title":"Field Reference"},{"location":"fields/#field-reference","text":"","title":"Field Reference"},{"location":"fields/#workflow","text":"Workflow is the definition of a workflow resource Examples (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml)","title":"Workflow"},{"location":"fields/#fields","text":"Field Name Field Type Description apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds metadata ObjectMeta No description available spec WorkflowSpec No description available status WorkflowStatus No description available","title":"Fields"},{"location":"fields/#cronworkflow","text":"CronWorkflow is the definition of a scheduled workflow resource Examples (click to open) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml)","title":"CronWorkflow"},{"location":"fields/#fields_1","text":"Field Name Field Type Description apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds metadata ObjectMeta No description available spec CronWorkflowSpec No description available status CronWorkflowStatus No description available","title":"Fields"},{"location":"fields/#workflowtemplate","text":"WorkflowTemplate is the definition of a workflow template resource Examples (click to open) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml)","title":"WorkflowTemplate"},{"location":"fields/#fields_2","text":"Field Name Field Type Description apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds metadata ObjectMeta No description available spec WorkflowTemplateSpec No description available","title":"Fields"},{"location":"fields/#workflowspec","text":"WorkflowSpec is the specification of a Workflow. Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml)","title":"WorkflowSpec"},{"location":"fields/#fields_3","text":"Field Name Field Type Description activeDeadlineSeconds int64 Optional duration in seconds relative to the workflow start time which the workflow is allowed to run before the controller terminates the io.argoproj.workflow.v1alpha1. A value of zero is used to terminate a Running workflow affinity Affinity Affinity sets the scheduling constraints for all pods in the io.argoproj.workflow.v1alpha1. Can be overridden by an affinity specified in the template arguments Arguments Arguments contain the parameters and artifacts sent to the workflow entrypoint Parameters are referencable globally using the 'workflow' variable prefix. e.g. {{io.argoproj.workflow.v1alpha1.parameters.myparam}} artifactRepositoryRef ArtifactRepositoryRef ArtifactRepositoryRef specifies the configMap name and key containing the artifact repository config. automountServiceAccountToken boolean AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false. dnsConfig PodDNSConfig PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. dnsPolicy string Set DNS policy for the pod. Defaults to \"ClusterFirst\". Valid values are 'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to 'ClusterFirstWithHostNet'. entrypoint string Entrypoint is a template reference to the starting point of the io.argoproj.workflow.v1alpha1. executor ExecutorConfig Executor holds configurations of executor containers of the io.argoproj.workflow.v1alpha1. hostAliases Array< HostAlias > No description available hostNetwork boolean Host networking requested for this workflow pod. Default to false. imagePullSecrets Array< LocalObjectReference > ImagePullSecrets is a list of references to secrets in the same namespace to use for pulling any images in pods that reference this ServiceAccount. ImagePullSecrets are distinct from Secrets because Secrets can be mounted in the pod, but ImagePullSecrets are only accessed by the kubelet. More info: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod metrics Metrics Metrics are a list of metrics emitted from this Workflow nodeSelector Map< string , string > NodeSelector is a selector which will result in all pods of the workflow to be scheduled on the selected node(s). This is able to be overridden by a nodeSelector specified in the template. onExit string OnExit is a template reference which is invoked at the end of the workflow, irrespective of the success, failure, or error of the primary io.argoproj.workflow.v1alpha1. parallelism int64 Parallelism limits the max total parallel pods that can execute at the same time in a workflow podDisruptionBudget PodDisruptionBudgetSpec PodDisruptionBudget holds the number of concurrent disruptions that you allow for Workflow's Pods. Controller will automatically add the selector with workflow name, if selector is empty. Optional: Defaults to empty. podGC PodGC PodGC describes the strategy to use when to deleting completed pods podPriority int32 Priority to apply to workflow pods. podPriorityClassName string PriorityClassName to apply to workflow pods. podSpecPatch string PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits). priority int32 Priority is used if controller is configured to process limited number of workflows in parallel. Workflows with higher priority are processed first. schedulerName string Set scheduler name for all pods. Will be overridden if container/script template's scheduler name is set. Default scheduler will be used if neither specified. securityContext PodSecurityContext SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to empty. See type description for default values of each field. serviceAccountName string ServiceAccountName is the name of the ServiceAccount to run all pods of the workflow as. shutdown string Shutdown will shutdown the workflow according to its ShutdownStrategy suspend boolean Suspend will suspend the workflow and prevent execution of any future steps in the workflow templates Array< Template > Templates is a list of workflow templates used in a workflow tolerations Array< Toleration > Tolerations to apply to workflow pods. ~ ttlSecondsAfterFinished ~ ~ int32 ~ ~TTLSecondsAfterFinished limits the lifetime of a Workflow that has finished execution (Succeeded, Failed, Error). If this field is set, once the Workflow finishes, it will be deleted after ttlSecondsAfterFinished expires. If this field is unset, ttlSecondsAfterFinished will not expire. If this field is set to zero, ttlSecondsAfterFinished expires immediately after the Workflow finishes.~ DEPRECATED: Use TTLStrategy.SecondsAfterCompletion instead. ttlStrategy TTLStrategy TTLStrategy limits the lifetime of a Workflow that has finished execution depending on if it Succeeded or Failed. If this struct is set, once the Workflow finishes, it will be deleted after the time to live expires. If this field is unset, the controller config map will hold the default values. volumeClaimTemplates Array< PersistentVolumeClaim > VolumeClaimTemplates is a list of claims that containers are allowed to reference. The Workflow controller will create the claims at the beginning of the workflow and delete the claims upon completion of the workflow volumes Array< Volume > Volumes is a list of volumes that can be mounted by containers in a io.argoproj.workflow.v1alpha1. workflowTemplateRef WorkflowTemplateRef WorkflowTemplateRef holds a reference to a WorkflowTemplate for execution","title":"Fields"},{"location":"fields/#workflowstatus","text":"WorkflowStatus contains overall status information about a workflow","title":"WorkflowStatus"},{"location":"fields/#fields_4","text":"Field Name Field Type Description compressedNodes string Compressed and base64 decoded Nodes map conditions Array< Condition > Conditions is a list of conditions the Workflow may have finishedAt Time Time at which this workflow completed message string A human readable message indicating details about why the workflow is in this condition. nodes NodeStatus Nodes is a mapping between a node ID and the node's status. offloadNodeStatusVersion string Whether on not node status has been offloaded to a database. If exists, then Nodes and CompressedNodes will be empty. This will actually be populated with a hash of the offloaded data. outputs Outputs Outputs captures output values and artifact locations produced by the workflow via global outputs persistentVolumeClaims Array< Volume > PersistentVolumeClaims tracks all PVCs that were created as part of the io.argoproj.workflow.v1alpha1. The contents of this list are drained at the end of the workflow. phase string Phase a simple, high-level summary of where the workflow is in its lifecycle. resourcesDuration Map< integer , int64 > ResourcesDuration is the total for the workflow startedAt Time Time at which this workflow started storedTemplates Template StoredTemplates is a mapping between a template ref and the node's status. storedWorkflowTemplateSpec WorkflowSpec StoredWorkflowSpec stores the WorkflowTemplate spec for future execution.","title":"Fields"},{"location":"fields/#cronworkflowspec","text":"CronWorkflowSpec is the specification of a CronWorkflow Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml)","title":"CronWorkflowSpec"},{"location":"fields/#fields_5","text":"Field Name Field Type Description concurrencyPolicy string ConcurrencyPolicy is the K8s-style concurrency policy that will be used failedJobsHistoryLimit int32 FailedJobsHistoryLimit is the number of successful jobs to be kept at a time schedule string Schedule is a schedule to run the Workflow in Cron format startingDeadlineSeconds int64 StartingDeadlineSeconds is the K8s-style deadline that will limit the time a CronWorkflow will be run after its original scheduled time if it is missed. successfulJobsHistoryLimit int32 SuccessfulJobsHistoryLimit is the number of successful jobs to be kept at a time suspend boolean Suspend is a flag that will stop new CronWorkflows from running if set to true timezone string Timezone is the timezone against which the cron schedule will be calculated, e.g. \"Asia/Tokyo\". Default is machine's local time. workflowMetadata ObjectMeta WorkflowMetadata contains some metadata of the workflow to be run workflowSpec WorkflowSpec WorkflowSpec is the spec of the workflow to be run","title":"Fields"},{"location":"fields/#cronworkflowstatus","text":"CronWorkflowStatus is the status of a CronWorkflow","title":"CronWorkflowStatus"},{"location":"fields/#fields_6","text":"Field Name Field Type Description active Array< ObjectReference > Active is a list of active workflows stemming from this CronWorkflow conditions Array< Condition > Conditions is a list of conditions the CronWorkflow may have lastScheduledTime Time LastScheduleTime is the last time the CronWorkflow was scheduled","title":"Fields"},{"location":"fields/#workflowtemplatespec","text":"WorkflowTemplateSpec is a spec of WorkflowTemplate. Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml)","title":"WorkflowTemplateSpec"},{"location":"fields/#fields_7","text":"Field Name Field Type Description activeDeadlineSeconds int64 Optional duration in seconds relative to the workflow start time which the workflow is allowed to run before the controller terminates the io.argoproj.workflow.v1alpha1. A value of zero is used to terminate a Running workflow affinity Affinity Affinity sets the scheduling constraints for all pods in the io.argoproj.workflow.v1alpha1. Can be overridden by an affinity specified in the template arguments Arguments Arguments contain the parameters and artifacts sent to the workflow entrypoint Parameters are referencable globally using the 'workflow' variable prefix. e.g. {{io.argoproj.workflow.v1alpha1.parameters.myparam}} artifactRepositoryRef ArtifactRepositoryRef ArtifactRepositoryRef specifies the configMap name and key containing the artifact repository config. automountServiceAccountToken boolean AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false. dnsConfig PodDNSConfig PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. dnsPolicy string Set DNS policy for the pod. Defaults to \"ClusterFirst\". Valid values are 'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to 'ClusterFirstWithHostNet'. entrypoint string Entrypoint is a template reference to the starting point of the io.argoproj.workflow.v1alpha1. executor ExecutorConfig Executor holds configurations of executor containers of the io.argoproj.workflow.v1alpha1. hostAliases Array< HostAlias > No description available hostNetwork boolean Host networking requested for this workflow pod. Default to false. imagePullSecrets Array< LocalObjectReference > ImagePullSecrets is a list of references to secrets in the same namespace to use for pulling any images in pods that reference this ServiceAccount. ImagePullSecrets are distinct from Secrets because Secrets can be mounted in the pod, but ImagePullSecrets are only accessed by the kubelet. More info: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod metrics Metrics Metrics are a list of metrics emitted from this Workflow nodeSelector Map< string , string > NodeSelector is a selector which will result in all pods of the workflow to be scheduled on the selected node(s). This is able to be overridden by a nodeSelector specified in the template. onExit string OnExit is a template reference which is invoked at the end of the workflow, irrespective of the success, failure, or error of the primary io.argoproj.workflow.v1alpha1. parallelism int64 Parallelism limits the max total parallel pods that can execute at the same time in a workflow podDisruptionBudget PodDisruptionBudgetSpec PodDisruptionBudget holds the number of concurrent disruptions that you allow for Workflow's Pods. Controller will automatically add the selector with workflow name, if selector is empty. Optional: Defaults to empty. podGC PodGC PodGC describes the strategy to use when to deleting completed pods podPriority int32 Priority to apply to workflow pods. podPriorityClassName string PriorityClassName to apply to workflow pods. podSpecPatch string PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits). priority int32 Priority is used if controller is configured to process limited number of workflows in parallel. Workflows with higher priority are processed first. schedulerName string Set scheduler name for all pods. Will be overridden if container/script template's scheduler name is set. Default scheduler will be used if neither specified. securityContext PodSecurityContext SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to empty. See type description for default values of each field. serviceAccountName string ServiceAccountName is the name of the ServiceAccount to run all pods of the workflow as. shutdown string Shutdown will shutdown the workflow according to its ShutdownStrategy suspend boolean Suspend will suspend the workflow and prevent execution of any future steps in the workflow templates Array< Template > Templates is a list of workflow templates used in a workflow tolerations Array< Toleration > Tolerations to apply to workflow pods. ~ ttlSecondsAfterFinished ~ ~ int32 ~ ~TTLSecondsAfterFinished limits the lifetime of a Workflow that has finished execution (Succeeded, Failed, Error). If this field is set, once the Workflow finishes, it will be deleted after ttlSecondsAfterFinished expires. If this field is unset, ttlSecondsAfterFinished will not expire. If this field is set to zero, ttlSecondsAfterFinished expires immediately after the Workflow finishes.~ DEPRECATED: Use TTLStrategy.SecondsAfterCompletion instead. ttlStrategy TTLStrategy TTLStrategy limits the lifetime of a Workflow that has finished execution depending on if it Succeeded or Failed. If this struct is set, once the Workflow finishes, it will be deleted after the time to live expires. If this field is unset, the controller config map will hold the default values. volumeClaimTemplates Array< PersistentVolumeClaim > VolumeClaimTemplates is a list of claims that containers are allowed to reference. The Workflow controller will create the claims at the beginning of the workflow and delete the claims upon completion of the workflow volumes Array< Volume > Volumes is a list of volumes that can be mounted by containers in a io.argoproj.workflow.v1alpha1. workflowTemplateRef WorkflowTemplateRef WorkflowTemplateRef holds a reference to a WorkflowTemplate for execution","title":"Fields"},{"location":"fields/#arguments","text":"Arguments to a template Examples with this field (click to open) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml)","title":"Arguments"},{"location":"fields/#fields_8","text":"Field Name Field Type Description artifacts Array< Artifact > Artifacts is the list of artifacts to pass to the template or workflow parameters Array< Parameter > Parameters is the list of parameters to pass to the template or workflow","title":"Fields"},{"location":"fields/#artifactrepositoryref","text":"No description available Examples with this field (click to open) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml)","title":"ArtifactRepositoryRef"},{"location":"fields/#fields_9","text":"Field Name Field Type Description configMap string No description available key string No description available","title":"Fields"},{"location":"fields/#executorconfig","text":"ExecutorConfig holds configurations of an executor container.","title":"ExecutorConfig"},{"location":"fields/#fields_10","text":"Field Name Field Type Description serviceAccountName string ServiceAccountName specifies the service account name of the executor container.","title":"Fields"},{"location":"fields/#metrics","text":"Metrics are a list of metrics emitted from a Workflow/Template Examples with this field (click to open) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml)","title":"Metrics"},{"location":"fields/#fields_11","text":"Field Name Field Type Description prometheus Array< Prometheus > Prometheus is a list of prometheus metrics to be emitted","title":"Fields"},{"location":"fields/#podgc","text":"PodGC describes how to delete completed pods as they complete Examples with this field (click to open) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml)","title":"PodGC"},{"location":"fields/#fields_12","text":"Field Name Field Type Description strategy string Strategy is the strategy to use. One of \"OnPodCompletion\", \"OnPodSuccess\", \"OnWorkflowCompletion\", \"OnWorkflowSuccess\"","title":"Fields"},{"location":"fields/#template","text":"Template is a reusable and composable unit of execution in a workflow Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml)","title":"Template"},{"location":"fields/#fields_13","text":"Field Name Field Type Description activeDeadlineSeconds int64 Optional duration in seconds relative to the StartTime that the pod may be active on a node before the system actively tries to terminate the pod; value must be positive integer This field is only applicable to container and script templates. affinity Affinity Affinity sets the pod's scheduling constraints Overrides the affinity set at the workflow level (if any) archiveLocation ArtifactLocation Location in which all files related to the step will be stored (logs, artifacts, etc...). Can be overridden by individual items in Outputs. If omitted, will use the default artifact repository location configured in the controller, appended with the / in the key. ~ arguments ~ ~ Arguments ~ ~Arguments hold arguments to the template.~ DEPRECATED: This field is not used. automountServiceAccountToken boolean AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false. container Container Container is the main container image to run in the pod daemon boolean Deamon will allow a workflow to proceed to the next step so long as the container reaches readiness dag DAGTemplate DAG template subtype which runs a DAG executor ExecutorConfig Executor holds configurations of the executor container. hostAliases Array< HostAlias > HostAliases is an optional list of hosts and IPs that will be injected into the pod spec initContainers Array< UserContainer > InitContainers is a list of containers which run before the main container. inputs Inputs Inputs describe what inputs parameters and artifacts are supplied to this template metadata Metadata Metdata sets the pods's metadata, i.e. annotations and labels metrics Metrics Metrics are a list of metrics emitted from this template name string Name is the name of the template nodeSelector Map< string , string > NodeSelector is a selector to schedule this step of the workflow to be run on the selected node(s). Overrides the selector set at the workflow level. outputs Outputs Outputs describe the parameters and artifacts that this template produces parallelism int64 Parallelism limits the max total parallel pods that can execute at the same time within the boundaries of this template invocation. If additional steps/dag templates are invoked, the pods created by those templates will not be counted towards this total. podSpecPatch string PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits). priority int32 Priority to apply to workflow pods. priorityClassName string PriorityClassName to apply to workflow pods. resource ResourceTemplate Resource template subtype which can run k8s resources resubmitPendingPods boolean ResubmitPendingPods is a flag to enable resubmitting pods that remain Pending after initial submission retryStrategy RetryStrategy RetryStrategy describes how to retry a template when it fails schedulerName string If specified, the pod will be dispatched by specified scheduler. Or it will be dispatched by workflow scope scheduler if specified. If neither specified, the pod will be dispatched by default scheduler. script ScriptTemplate Script runs a portion of code against an interpreter securityContext PodSecurityContext SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to empty. See type description for default values of each field. serviceAccountName string ServiceAccountName to apply to workflow pods sidecars Array< UserContainer > Sidecars is a list of containers which run alongside the main container Sidecars are automatically killed when the main container completes steps Array<Array< WorkflowStep >> Steps define a series of sequential/parallel workflow steps suspend SuspendTemplate Suspend template subtype which can suspend a workflow when reaching the step ~ template ~ ~ string ~ ~Template is the name of the template which is used as the base of this template.~ DEPRECATED: This field is not used. ~ templateRef ~ ~ TemplateRef ~ ~TemplateRef is the reference to the template resource which is used as the base of this template.~ DEPRECATED: This field is not used. tolerations Array< Toleration > Tolerations to apply to workflow pods. volumes Array< Volume > Volumes is a list of volumes that can be mounted by containers in a template.","title":"Fields"},{"location":"fields/#ttlstrategy","text":"TTLStrategy is the strategy for the time to live depending on if the workflow succeeded or failed Examples with this field (click to open) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml)","title":"TTLStrategy"},{"location":"fields/#fields_14","text":"Field Name Field Type Description secondsAfterCompletion int32 SecondsAfterCompletion is the number of seconds to live after completion secondsAfterFailure int32 SecondsAfterFailure is the number of seconds to live after failure secondsAfterSuccess int32 SecondsAfterSuccess is the number of seconds to live after success","title":"Fields"},{"location":"fields/#workflowtemplateref","text":"WorkflowTemplateRef is a reference to a WorkflowTemplate resource. Examples with this field (click to open) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml)","title":"WorkflowTemplateRef"},{"location":"fields/#fields_15","text":"Field Name Field Type Description clusterScope boolean ClusterScope indicates the referred template is cluster scoped (i.e. a ClusterWorkflowTemplate). name string Name is the resource name of the workflow template.","title":"Fields"},{"location":"fields/#condition","text":"No description available","title":"Condition"},{"location":"fields/#fields_16","text":"Field Name Field Type Description message string Message is the condition message status string Status is the status of the condition type string Type is the type of condition","title":"Fields"},{"location":"fields/#nodestatus","text":"NodeStatus contains status information about an individual node in the workflow","title":"NodeStatus"},{"location":"fields/#fields_17","text":"Field Name Field Type Description boundaryID string BoundaryID indicates the node ID of the associated template root node in which this node belongs to children Array< string > Children is a list of child node IDs daemoned boolean Daemoned tracks whether or not this node was daemoned and need to be terminated displayName string DisplayName is a human readable representation of the node. Unique within a template boundary finishedAt Time Time at which this node completed hostNodeName string HostNodeName name of the Kubernetes node on which the Pod is running, if applicable id string ID is a unique identifier of a node within the worklow It is implemented as a hash of the node name, which makes the ID deterministic inputs Inputs Inputs captures input parameter values and artifact locations supplied to this template invocation message string A human readable message indicating details about why the node is in this condition. name string Name is unique name in the node tree used to generate the node ID outboundNodes Array< string > OutboundNodes tracks the node IDs which are considered \"outbound\" nodes to a template invocation. For every invocation of a template, there are nodes which we considered as \"outbound\". Essentially, these are last nodes in the execution sequence to run, before the template is considered completed. These nodes are then connected as parents to a following step.In the case of single pod steps (i.e. container, script, resource templates), this list will be nil since the pod itself is already considered the \"outbound\" node. In the case of DAGs, outbound nodes are the \"target\" tasks (tasks with no children). In the case of steps, outbound nodes are all the containers involved in the last step group. NOTE: since templates are composable, the list of outbound nodes are carried upwards when a DAG/steps template invokes another DAG/steps template. In other words, the outbound nodes of a template, will be a superset of the outbound nodes of its last children. outputs Outputs Outputs captures output parameter values and artifact locations produced by this template invocation phase string Phase a simple, high-level summary of where the node is in its lifecycle. Can be used as a state machine. podIP string PodIP captures the IP of the pod for daemoned steps resourcesDuration Map< integer , int64 > ResourcesDuration is indicative, but not accurate, resource duration. This is populated when the nodes completes. startedAt Time Time at which this node started ~ storedTemplateID ~ ~ string ~ ~StoredTemplateID is the ID of stored template.~ DEPRECATED: This value is not used anymore. templateName string TemplateName is the template name which this node corresponds to. Not applicable to virtual nodes (e.g. Retry, StepGroup) templateRef TemplateRef TemplateRef is the reference to the template resource which this node corresponds to. Not applicable to virtual nodes (e.g. Retry, StepGroup) templateScope string TemplateScope is the template scope in which the template of this node was retrieved. type string Type indicates type of node ~ workflowTemplateName ~ ~ string ~ ~WorkflowTemplateName is the WorkflowTemplate resource name on which the resolved template of this node is retrieved.~ DEPRECATED: This value is not used anymore.","title":"Fields"},{"location":"fields/#outputs","text":"Outputs hold parameters, artifacts, and results from a step Examples with this field (click to open) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml)","title":"Outputs"},{"location":"fields/#fields_18","text":"Field Name Field Type Description artifacts Array< Artifact > Artifacts holds the list of output artifacts produced by a step exitCode string ExitCode holds the exit code of a script template parameters Array< Parameter > Parameters holds the list of output parameters produced by a step result string Result holds the result (stdout) of a script template","title":"Fields"},{"location":"fields/#artifact","text":"Artifact indicates an artifact to place at a specified path Examples with this field (click to open) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml)","title":"Artifact"},{"location":"fields/#fields_19","text":"Field Name Field Type Description archive ArchiveStrategy Archive controls how the artifact will be saved to the artifact repository. archiveLogs boolean ArchiveLogs indicates if the container logs should be archived artifactory ArtifactoryArtifact Artifactory contains artifactory artifact location details from string From allows an artifact to reference an artifact from a previous step gcs GCSArtifact GCS contains GCS artifact location details git GitArtifact Git contains git artifact location details globalName string GlobalName exports an output artifact to the global scope, making it available as '{{io.argoproj.workflow.v1alpha1.outputs.artifacts.XXXX}} and in workflow.status.outputs.artifacts hdfs HDFSArtifact HDFS contains HDFS artifact location details http HTTPArtifact HTTP contains HTTP artifact location details mode int32 mode bits to use on this file, must be a value between 0 and 0777 set when loading input artifacts. name string name of the artifact. must be unique within a template's inputs/outputs. optional boolean Make Artifacts optional, if Artifacts doesn't generate or exist oss OSSArtifact OSS contains OSS artifact location details path string Path is the container path to the artifact raw RawArtifact Raw contains raw artifact location details s3 S3Artifact S3 contains S3 artifact location details","title":"Fields"},{"location":"fields/#parameter","text":"Parameter indicate a passed string parameter to a service template with an optional default value Examples with this field (click to open) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml)","title":"Parameter"},{"location":"fields/#fields_20","text":"Field Name Field Type Description default IntOrString Default is the default value to use for an input parameter if a value was not supplied globalName string GlobalName exports an output parameter to the global scope, making it available as '{{io.argoproj.workflow.v1alpha1.outputs.parameters.XXXX}} and in workflow.status.outputs.parameters name string Name is the parameter name value IntOrString Value is the literal value to use for the parameter. If specified in the context of an input parameter, the value takes precedence over any passed values valueFrom ValueFrom ValueFrom is the source for the output parameter's value","title":"Fields"},{"location":"fields/#prometheus","text":"Prometheus is a prometheus metric to be emitted Examples with this field (click to open) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml)","title":"Prometheus"},{"location":"fields/#fields_21","text":"Field Name Field Type Description counter Counter Counter is a counter metric gauge Gauge Gauge is a gauge metric help string Help is a string that describes the metric histogram Histogram Histogram is a histogram metric labels Array< MetricLabel > Labels is a list of metric labels name string Name is the name of the metric when string When is a conditional statement that decides when to emit the metric","title":"Fields"},{"location":"fields/#artifactlocation","text":"ArtifactLocation describes a location for a single or multiple artifacts. It is used as single artifact in the context of inputs/outputs (e.g. outputs.artifacts.artname). It is also used to describe the location of multiple artifacts such as the archive location of a single workflow step, which the executor will use as a default location to store its files. Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml)","title":"ArtifactLocation"},{"location":"fields/#fields_22","text":"Field Name Field Type Description archiveLogs boolean ArchiveLogs indicates if the container logs should be archived artifactory ArtifactoryArtifact Artifactory contains artifactory artifact location details gcs GCSArtifact GCS contains GCS artifact location details git GitArtifact Git contains git artifact location details hdfs HDFSArtifact HDFS contains HDFS artifact location details http HTTPArtifact HTTP contains HTTP artifact location details oss OSSArtifact OSS contains OSS artifact location details raw RawArtifact Raw contains raw artifact location details s3 S3Artifact S3 contains S3 artifact location details","title":"Fields"},{"location":"fields/#dagtemplate","text":"DAGTemplate is a template subtype for directed acyclic graph templates Examples with this field (click to open) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml)","title":"DAGTemplate"},{"location":"fields/#fields_23","text":"Field Name Field Type Description failFast boolean This flag is for DAG logic. The DAG logic has a built-in \"fail fast\" feature to stop scheduling new steps, as soon as it detects that one of the DAG nodes is failed. Then it waits until all DAG nodes are completed before failing the DAG itself. The FailFast flag default is true, if set to false, it will allow a DAG to run all branches of the DAG to completion (either success or failure), regardless of the failed outcomes of branches in the DAG. More info and example about this feature at https://github.com/argoproj/argo/issues/1442 target string Target are one or more names of targets to execute in a DAG tasks Array< DAGTask > Tasks are a list of DAG tasks","title":"Fields"},{"location":"fields/#usercontainer","text":"UserContainer is a container specified by a user. Examples with this field (click to open) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml)","title":"UserContainer"},{"location":"fields/#fields_24","text":"Field Name Field Type Description args Array< string > Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell command Array< string > Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell env Array< EnvVar > List of environment variables to set in the container. Cannot be updated. envFrom Array< EnvFromSource > List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. image string Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets. imagePullPolicy string Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images lifecycle Lifecycle Actions that the management system should take in response to container lifecycle events. Cannot be updated. livenessProbe Probe Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes mirrorVolumeMounts boolean MirrorVolumeMounts will mount the same volumes specified in the main container to the container (including artifacts), at the same mountPaths. This enables dind daemon to partially see the same filesystem as the main container in order to use features such as docker volume binding name string Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated. ports Array< ContainerPort > List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \"0.0.0.0\" address inside a container will be accessible from the network. Cannot be updated. readinessProbe Probe Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes resources ResourceRequirements Compute Resources required by this container. Cannot be updated. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ securityContext SecurityContext Security options the pod should run with. More info: https://kubernetes.io/docs/concepts/policy/security-context/ More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ startupProbe Probe StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state operation. This cannot be updated. This is an alpha feature enabled by the StartupProbe feature flag. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes stdin boolean Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false. stdinOnce boolean Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false terminationMessagePath string Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated. terminationMessagePolicy string Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated. tty boolean Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false. volumeDevices Array< VolumeDevice > volumeDevices is the list of block devices to be used by the container. This is a beta feature. volumeMounts Array< VolumeMount > Pod volumes to mount into the container's filesystem. Cannot be updated. workingDir string Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated.","title":"Fields"},{"location":"fields/#inputs","text":"Inputs are the mechanism for passing parameters, artifacts, volumes from one template to another Examples with this field (click to open) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml)","title":"Inputs"},{"location":"fields/#fields_25","text":"Field Name Field Type Description artifacts Array< Artifact > Artifact are a list of artifacts passed as inputs parameters Array< Parameter > Parameters are a list of parameters passed as inputs","title":"Fields"},{"location":"fields/#metadata","text":"Pod metdata Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml)","title":"Metadata"},{"location":"fields/#fields_26","text":"Field Name Field Type Description annotations Map< string , string > No description available labels Map< string , string > No description available","title":"Fields"},{"location":"fields/#resourcetemplate","text":"ResourceTemplate is a template subtype to manipulate kubernetes resources Examples with this field (click to open) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml)","title":"ResourceTemplate"},{"location":"fields/#fields_27","text":"Field Name Field Type Description action string Action is the action to perform to the resource. Must be one of: get, create, apply, delete, replace, patch failureCondition string FailureCondition is a label selector expression which describes the conditions of the k8s resource in which the step was considered failed flags Array< string > Flags is a set of additional options passed to kubectl before submitting a resource I.e. to disable resource validation: flags: [ \"--validate=false\" # disable resource validation] manifest string Manifest contains the kubernetes manifest mergeStrategy string MergeStrategy is the strategy used to merge a patch. It defaults to \"strategic\" Must be one of: strategic, merge, json setOwnerReference boolean SetOwnerReference sets the reference to the workflow on the OwnerReference of generated resource. successCondition string SuccessCondition is a label selector expression which describes the conditions of the k8s resource in which it is acceptable to proceed to the following step","title":"Fields"},{"location":"fields/#retrystrategy","text":"RetryStrategy provides controls on how to retry a workflow step Examples with this field (click to open) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml)","title":"RetryStrategy"},{"location":"fields/#fields_28","text":"Field Name Field Type Description backoff Backoff Backoff is a backoff strategy limit int32 Limit is the maximum number of attempts when retrying a container retryPolicy string RetryPolicy is a policy of NodePhase statuses that will be retried","title":"Fields"},{"location":"fields/#scripttemplate","text":"ScriptTemplate is a template subtype to enable scripting through code steps Examples with this field (click to open) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml)","title":"ScriptTemplate"},{"location":"fields/#fields_29","text":"Field Name Field Type Description args Array< string > Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell command Array< string > Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell env Array< EnvVar > List of environment variables to set in the container. Cannot be updated. envFrom Array< EnvFromSource > List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. image string Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets. imagePullPolicy string Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images lifecycle Lifecycle Actions that the management system should take in response to container lifecycle events. Cannot be updated. livenessProbe Probe Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes name string Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated. ports Array< ContainerPort > List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \"0.0.0.0\" address inside a container will be accessible from the network. Cannot be updated. readinessProbe Probe Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes resources ResourceRequirements Compute Resources required by this container. Cannot be updated. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ securityContext SecurityContext Security options the pod should run with. More info: https://kubernetes.io/docs/concepts/policy/security-context/ More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ source string Source contains the source code of the script to execute startupProbe Probe StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state operation. This cannot be updated. This is an alpha feature enabled by the StartupProbe feature flag. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes stdin boolean Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false. stdinOnce boolean Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false terminationMessagePath string Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated. terminationMessagePolicy string Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated. tty boolean Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false. volumeDevices Array< VolumeDevice > volumeDevices is the list of block devices to be used by the container. This is a beta feature. volumeMounts Array< VolumeMount > Pod volumes to mount into the container's filesystem. Cannot be updated. workingDir string Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated.","title":"Fields"},{"location":"fields/#workflowstep","text":"WorkflowStep is a reference to a template to execute in a series of step Examples with this field (click to open) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml)","title":"WorkflowStep"},{"location":"fields/#fields_30","text":"Field Name Field Type Description arguments Arguments Arguments hold arguments to the template continueOn ContinueOn ContinueOn makes argo to proceed with the following step even if this step fails. Errors and Failed states can be specified name string Name of the step onExit string OnExit is a template reference which is invoked at the end of the template, irrespective of the success, failure, or error of the primary template. template string Template is the name of the template to execute as the step templateRef TemplateRef TemplateRef is the reference to the template resource to execute as the step. when string When is an expression in which the step should conditionally execute withItems Array< Item > WithItems expands a step into multiple parallel steps from the items in the list withParam string WithParam expands a step into multiple parallel steps from the value in the parameter, which is expected to be a JSON list. withSequence Sequence WithSequence expands a step into a numeric sequence","title":"Fields"},{"location":"fields/#suspendtemplate","text":"SuspendTemplate is a template subtype to suspend a workflow at a predetermined point in time Examples with this field (click to open) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml)","title":"SuspendTemplate"},{"location":"fields/#fields_31","text":"Field Name Field Type Description duration string Duration is the seconds to wait before automatically resuming a template","title":"Fields"},{"location":"fields/#templateref","text":"TemplateRef is a reference of template resource. Examples with this field (click to open) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml)","title":"TemplateRef"},{"location":"fields/#fields_32","text":"Field Name Field Type Description clusterScope boolean ClusterScope indicates the referred template is cluster scoped (i.e. a ClusterWorkflowTemplate). name string Name is the resource name of the template. runtimeResolution boolean RuntimeResolution skips validation at creation time. By enabling this option, you can create the referred workflow template before the actual runtime. template string Template is the name of referred template in the resource.","title":"Fields"},{"location":"fields/#archivestrategy","text":"ArchiveStrategy describes how to archive files/directory when saving artifacts Examples with this field (click to open) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml)","title":"ArchiveStrategy"},{"location":"fields/#fields_33","text":"Field Name Field Type Description none NoneStrategy No description available tar TarStrategy No description available","title":"Fields"},{"location":"fields/#artifactoryartifact","text":"ArtifactoryArtifact is the location of an artifactory artifact Examples with this field (click to open) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml)","title":"ArtifactoryArtifact"},{"location":"fields/#fields_34","text":"Field Name Field Type Description passwordSecret SecretKeySelector PasswordSecret is the secret selector to the repository password url string URL of the artifact usernameSecret SecretKeySelector UsernameSecret is the secret selector to the repository username","title":"Fields"},{"location":"fields/#gcsartifact","text":"GCSArtifact is the location of a GCS artifact Examples with this field (click to open) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml)","title":"GCSArtifact"},{"location":"fields/#fields_35","text":"Field Name Field Type Description bucket string Bucket is the name of the bucket key string Key is the path in the bucket where the artifact resides serviceAccountKeySecret SecretKeySelector ServiceAccountKeySecret is the secret selector to the bucket's service account key","title":"Fields"},{"location":"fields/#gitartifact","text":"GitArtifact is the location of an git artifact Examples with this field (click to open) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml)","title":"GitArtifact"},{"location":"fields/#fields_36","text":"Field Name Field Type Description depth int64 Depth specifies clones/fetches should be shallow and include the given number of commits from the branch tip fetch Array< string > Fetch specifies a number of refs that should be fetched before checkout insecureIgnoreHostKey boolean InsecureIgnoreHostKey disables SSH strict host key checking during git clone passwordSecret SecretKeySelector PasswordSecret is the secret selector to the repository password repo string Repo is the git repository revision string Revision is the git commit, tag, branch to checkout sshPrivateKeySecret SecretKeySelector SSHPrivateKeySecret is the secret selector to the repository ssh private key usernameSecret SecretKeySelector UsernameSecret is the secret selector to the repository username","title":"Fields"},{"location":"fields/#hdfsartifact","text":"HDFSArtifact is the location of an HDFS artifact Examples with this field (click to open) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml)","title":"HDFSArtifact"},{"location":"fields/#fields_37","text":"Field Name Field Type Description addresses Array< string > Addresses is accessible addresses of HDFS name nodes force boolean Force copies a file forcibly even if it exists (default: false) hdfsUser string HDFSUser is the user to access HDFS file system. It is ignored if either ccache or keytab is used. krbCCacheSecret SecretKeySelector KrbCCacheSecret is the secret selector for Kerberos ccache Either ccache or keytab can be set to use Kerberos. krbConfigConfigMap ConfigMapKeySelector KrbConfig is the configmap selector for Kerberos config as string It must be set if either ccache or keytab is used. krbKeytabSecret SecretKeySelector KrbKeytabSecret is the secret selector for Kerberos keytab Either ccache or keytab can be set to use Kerberos. krbRealm string KrbRealm is the Kerberos realm used with Kerberos keytab It must be set if keytab is used. krbServicePrincipalName string KrbServicePrincipalName is the principal name of Kerberos service It must be set if either ccache or keytab is used. krbUsername string KrbUsername is the Kerberos username used with Kerberos keytab It must be set if keytab is used. path string Path is a file path in HDFS","title":"Fields"},{"location":"fields/#httpartifact","text":"HTTPArtifact allows an file served on HTTP to be placed as an input artifact in a container Examples with this field (click to open) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml)","title":"HTTPArtifact"},{"location":"fields/#fields_38","text":"Field Name Field Type Description url string URL of the artifact","title":"Fields"},{"location":"fields/#ossartifact","text":"OSSArtifact is the location of an Alibaba Cloud OSS artifact Examples with this field (click to open) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml)","title":"OSSArtifact"},{"location":"fields/#fields_39","text":"Field Name Field Type Description accessKeySecret SecretKeySelector AccessKeySecret is the secret selector to the bucket's access key bucket string Bucket is the name of the bucket endpoint string Endpoint is the hostname of the bucket endpoint key string Key is the path in the bucket where the artifact resides secretKeySecret SecretKeySelector SecretKeySecret is the secret selector to the bucket's secret key","title":"Fields"},{"location":"fields/#rawartifact","text":"RawArtifact allows raw string content to be placed as an artifact in a container Examples with this field (click to open) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml)","title":"RawArtifact"},{"location":"fields/#fields_40","text":"Field Name Field Type Description data string Data is the string contents of the artifact","title":"Fields"},{"location":"fields/#s3artifact","text":"S3Artifact is the location of an S3 artifact","title":"S3Artifact"},{"location":"fields/#fields_41","text":"Field Name Field Type Description accessKeySecret SecretKeySelector AccessKeySecret is the secret selector to the bucket's access key bucket string Bucket is the name of the bucket endpoint string Endpoint is the hostname of the bucket endpoint insecure boolean Insecure will connect to the service with TLS key string Key is the key in the bucket where the artifact resides region string Region contains the optional bucket region roleARN string RoleARN is the Amazon Resource Name (ARN) of the role to assume. secretKeySecret SecretKeySelector SecretKeySecret is the secret selector to the bucket's secret key useSDKCreds boolean UseSDKCreds tells the driver to figure out credentials based on sdk defaults.","title":"Fields"},{"location":"fields/#valuefrom","text":"ValueFrom describes a location in which to obtain the value to a parameter Examples with this field (click to open) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml)","title":"ValueFrom"},{"location":"fields/#fields_42","text":"Field Name Field Type Description default IntOrString Default specifies a value to be used if retrieving the value from the specified source fails jqFilter string JQFilter expression against the resource object in resource templates jsonPath string JSONPath of a resource to retrieve an output parameter value from in resource templates parameter string Parameter reference to a step or dag task in which to retrieve an output parameter value from (e.g. '{{steps.mystep.outputs.myparam}}') path string Path in the container to retrieve an output parameter value from in container templates","title":"Fields"},{"location":"fields/#counter","text":"Counter is a Counter prometheus metric Examples with this field (click to open) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml)","title":"Counter"},{"location":"fields/#fields_43","text":"Field Name Field Type Description value string Value is the value of the metric","title":"Fields"},{"location":"fields/#gauge","text":"Gauge is a Gauge prometheus metric Examples with this field (click to open) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml)","title":"Gauge"},{"location":"fields/#fields_44","text":"Field Name Field Type Description realtime boolean Realtime emits this metric in real time if applicable value string Value is the value of the metric","title":"Fields"},{"location":"fields/#histogram","text":"Histogram is a Histogram prometheus metric Examples with this field (click to open) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml)","title":"Histogram"},{"location":"fields/#fields_45","text":"Field Name Field Type Description buckets Array< Amount > Buckets is a list of bucket divisors for the histogram value string Value is the value of the metric","title":"Fields"},{"location":"fields/#metriclabel","text":"MetricLabel is a single label for a prometheus metric Examples with this field (click to open) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml)","title":"MetricLabel"},{"location":"fields/#fields_46","text":"Field Name Field Type Description key string No description available value string No description available","title":"Fields"},{"location":"fields/#dagtask","text":"DAGTask represents a node in the graph during DAG execution Examples with this field (click to open) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml)","title":"DAGTask"},{"location":"fields/#fields_47","text":"Field Name Field Type Description arguments Arguments Arguments are the parameter and artifact arguments to the template continueOn ContinueOn ContinueOn makes argo to proceed with the following step even if this step fails. Errors and Failed states can be specified dependencies Array< string > Dependencies are name of other targets which this depends on depends string Depends are name of other targets which this depends on name string Name is the name of the target onExit string OnExit is a template reference which is invoked at the end of the template, irrespective of the success, failure, or error of the primary template. template string Name of template to execute templateRef TemplateRef TemplateRef is the reference to the template resource to execute. when string When is an expression in which the task should conditionally execute withItems Array< Item > WithItems expands a task into multiple parallel tasks from the items in the list withParam string WithParam expands a task into multiple parallel tasks from the value in the parameter, which is expected to be a JSON list. withSequence Sequence WithSequence expands a task into a numeric sequence","title":"Fields"},{"location":"fields/#backoff","text":"Backoff is a backoff strategy to use within retryStrategy Examples with this field (click to open) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml)","title":"Backoff"},{"location":"fields/#fields_48","text":"Field Name Field Type Description duration string Duration is the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\") factor int32 Factor is a factor to multiply the base duration after each failed retry maxDuration string MaxDuration is the maximum amount of time allowed for the backoff strategy","title":"Fields"},{"location":"fields/#continueon","text":"ContinueOn defines if a workflow should continue even if a task or step fails/errors. It can be specified if the workflow should continue when the pod errors, fails or both. Examples with this field (click to open) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml)","title":"ContinueOn"},{"location":"fields/#fields_49","text":"Field Name Field Type Description error boolean No description available failed boolean No description available","title":"Fields"},{"location":"fields/#item","text":"Item expands a single workflow step into multiple parallel steps The value of Item can be a map, string, bool, or number Examples with this field (click to open) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml)","title":"Item"},{"location":"fields/#sequence","text":"Sequence expands a workflow step into numeric range Examples with this field (click to open) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml)","title":"Sequence"},{"location":"fields/#fields_50","text":"Field Name Field Type Description count string Count is number of elements in the sequence (default: 0). Not to be used with end end string Number at which to end the sequence (default: 0). Not to be used with Count format string Format is a printf format string to format the value in the sequence start string Number at which to start the sequence (default: 0)","title":"Fields"},{"location":"fields/#nonestrategy","text":"NoneStrategy indicates to skip tar process and upload the files or directory tree as independent files. Note that if the artifact is a directory, the artifact driver must support the ability to save/load the directory appropriately. Examples with this field (click to open) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml)","title":"NoneStrategy"},{"location":"fields/#tarstrategy","text":"TarStrategy will tar and gzip the file or directory when saving Examples with this field (click to open) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml)","title":"TarStrategy"},{"location":"fields/#fields_51","text":"Field Name Field Type Description compressionLevel int32 CompressionLevel specifies the gzip compression level to use for the artifact. Defaults to gzip.DefaultCompression.","title":"Fields"},{"location":"fields/#amount","text":"Amount represent a numeric amount. Examples with this field (click to open) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml)","title":"Amount"},{"location":"fields/#external-fields","text":"","title":"External Fields"},{"location":"fields/#objectmeta","text":"ObjectMeta is metadata that all persisted resources must have, which includes all objects users must create. Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml)","title":"ObjectMeta"},{"location":"fields/#fields_52","text":"Field Name Field Type Description annotations Map< string , string > Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations clusterName string The name of the cluster which the object belongs to. This is used to distinguish resources with same name and namespace in different clusters. This field is not set anywhere right now and apiserver is going to ignore it if set in create or update request. creationTimestamp Time CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC.Populated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata deletionGracePeriodSeconds int64 Number of seconds allowed for this object to gracefully terminate before it will be removed from the system. Only set when deletionTimestamp is also set. May only be shortened. Read-only. deletionTimestamp Time DeletionTimestamp is RFC 3339 date and time at which this resource will be deleted. This field is set by the server when a graceful deletion is requested by the user, and is not directly settable by a client. The resource is expected to be deleted (no longer visible from resource lists, and not reachable by name) after the time in this field, once the finalizers list is empty. As long as the finalizers list contains items, deletion is blocked. Once the deletionTimestamp is set, this value may not be unset or be set further into the future, although it may be shortened or the resource may be deleted prior to this time. For example, a user may request that a pod is deleted in 30 seconds. The Kubelet will react by sending a graceful termination signal to the containers in the pod. After that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL) to the container and after cleanup, remove the pod from the API. In the presence of network partitions, this object may still exist after this timestamp, until an administrator or automated process can determine the resource is fully terminated. If not set, graceful deletion of the object has not been requested.Populated by the system when a graceful deletion is requested. Read-only. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata finalizers Array< string > Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. generateName string GenerateName is an optional prefix, used by the server, to generate a unique name ONLY IF the Name field has not been provided. If this field is used, the name returned to the client will be different than the name passed. This value will also be combined with a unique suffix. The provided value has the same validation rules as the Name field, and may be truncated by the length of the suffix required to make the value unique on the server.If this field is specified and the generated name exists, the server will NOT return a 409 - instead, it will either return 201 Created or 500 with Reason ServerTimeout indicating a unique name could not be found in the time allotted, and the client should retry (optionally after the time indicated in the Retry-After header).Applied only if Name is not specified. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#idempotency generation int64 A sequence number representing a specific generation of the desired state. Populated by the system. Read-only. ~ initializers ~ ~ Initializers ~ ~An initializer is a controller which enforces some system invariant at object creation time. This field is a list of initializers that have not yet acted on this object. If nil or empty, this object has been completely initialized. Otherwise, the object is considered uninitialized and is hidden (in list/watch and get calls) from clients that haven't explicitly asked to observe uninitialized objects.When an object is created, the system will populate this list with the current set of initializers. Only privileged users may set or modify this list. Once it is empty, it may not be modified further by any user~ DEPRECATED - initializers are an alpha field and will be removed in v1.15. labels Map< string , string > Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels managedFields Array< ManagedFieldsEntry > ManagedFields maps workflow-id and version to the set of fields that are managed by that workflow. This is mostly for internal housekeeping, and users typically shouldn't need to set or understand this field. A workflow can be the user's name, a controller's name, or the name of a specific apply path like \"ci-cd\". The set of fields is always in the version that the workflow used when modifying the object.This field is alpha and can be changed or removed without notice. name string Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/identifiers#names namespace string Namespace defines the space within each name must be unique. An empty namespace is equivalent to the \"default\" namespace, but \"default\" is the canonical representation. Not all objects are required to be scoped to a namespace - the value of this field for those objects will be empty.Must be a DNS_LABEL. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/namespaces ownerReferences Array< OwnerReference > List of objects depended by this object. If ALL objects in the list have been deleted, this object will be garbage collected. If this object is managed by a controller, then an entry in this list will point to this controller, with the controller field set to true. There cannot be more than one managing controller. resourceVersion string An opaque value that represents the internal version of this object that can be used by clients to determine when objects have changed. May be used for optimistic concurrency, change detection, and the watch operation on a resource or set of resources. Clients must treat these values as opaque and passed unmodified back to the server. They may only be valid for a particular resource or set of resources.Populated by the system. Read-only. Value must be treated as opaque by clients and . More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency selfLink string SelfLink is a URL representing this object. Populated by the system. Read-only. uid string UID is the unique in time and space value for this object. It is typically generated by the server on successful creation of a resource and is not allowed to change on PUT operations.Populated by the system. Read-only. More info: http://kubernetes.io/docs/user-guide/identifiers#uids","title":"Fields"},{"location":"fields/#affinity","text":"Affinity is a group of affinity scheduling rules.","title":"Affinity"},{"location":"fields/#fields_53","text":"Field Name Field Type Description nodeAffinity NodeAffinity Describes node affinity scheduling rules for the pod. podAffinity PodAffinity Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). podAntiAffinity PodAntiAffinity Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)).","title":"Fields"},{"location":"fields/#poddnsconfig","text":"PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. Examples with this field (click to open) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml)","title":"PodDNSConfig"},{"location":"fields/#fields_54","text":"Field Name Field Type Description nameservers Array< string > A list of DNS name server IP addresses. This will be appended to the base nameservers generated from DNSPolicy. Duplicated nameservers will be removed. options Array< PodDNSConfigOption > A list of DNS resolver options. This will be merged with the base options generated from DNSPolicy. Duplicated entries will be removed. Resolution options given in Options will override those that appear in the base DNSPolicy. searches Array< string > A list of DNS search domains for host-name lookup. This will be appended to the base search paths generated from DNSPolicy. Duplicated search paths will be removed.","title":"Fields"},{"location":"fields/#hostalias","text":"HostAlias holds the mapping between IP and hostnames that will be injected as an entry in the pod's hosts file.","title":"HostAlias"},{"location":"fields/#fields_55","text":"Field Name Field Type Description hostnames Array< string > Hostnames for the above IP address. ip string IP address of the host file entry.","title":"Fields"},{"location":"fields/#localobjectreference","text":"LocalObjectReference contains enough information to let you locate the referenced object inside the same namespace. Examples with this field (click to open) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml)","title":"LocalObjectReference"},{"location":"fields/#fields_56","text":"Field Name Field Type Description name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names","title":"Fields"},{"location":"fields/#poddisruptionbudgetspec","text":"PodDisruptionBudgetSpec is a description of a PodDisruptionBudget. Examples with this field (click to open) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml)","title":"PodDisruptionBudgetSpec"},{"location":"fields/#fields_57","text":"Field Name Field Type Description maxUnavailable IntOrString An eviction is allowed if at most \"maxUnavailable\" pods selected by \"selector\" are unavailable after the eviction, i.e. even in absence of the evicted pod. For example, one can prevent all voluntary evictions by specifying 0. This is a mutually exclusive setting with \"minAvailable\". minAvailable IntOrString An eviction is allowed if at least \"minAvailable\" pods selected by \"selector\" will still be available after the eviction, i.e. even in the absence of the evicted pod. So for example you can prevent all voluntary evictions by specifying \"100%\". selector LabelSelector Label query over pods whose evictions are managed by the disruption budget.","title":"Fields"},{"location":"fields/#podsecuritycontext","text":"PodSecurityContext holds pod-level security attributes and common container settings. Some fields are also present in container.securityContext. Field values of container.securityContext take precedence over field values of PodSecurityContext. Examples with this field (click to open) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml)","title":"PodSecurityContext"},{"location":"fields/#fields_58","text":"Field Name Field Type Description fsGroup int64 A special supplemental group that applies to all containers in a pod. Some volume types allow the Kubelet to change the ownership of that volume to be owned by the pod:1. The owning GID will be the FSGroup 2. The setgid bit is set (new files created in the volume will be owned by FSGroup) 3. The permission bits are OR'd with rw-rw----If unset, the Kubelet will not modify the ownership and permissions of any volume. runAsGroup int64 The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. runAsNonRoot boolean Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. runAsUser int64 The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. seLinuxOptions SELinuxOptions The SELinux context to be applied to all containers. If unspecified, the container runtime will allocate a random SELinux context for each container. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. supplementalGroups Array< integer > A list of groups applied to the first process run in each container, in addition to the container's primary GID. If unspecified, no groups will be added to any container. sysctls Array< Sysctl > Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported sysctls (by the container runtime) might fail to launch. windowsOptions WindowsSecurityContextOptions Windows security options.","title":"Fields"},{"location":"fields/#toleration","text":"The pod this Toleration is attached to tolerates any taint that matches the triple using the matching operator .","title":"Toleration"},{"location":"fields/#fields_59","text":"Field Name Field Type Description effect string Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. key string Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. operator string Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. tolerationSeconds int64 TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. value string Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string.","title":"Fields"},{"location":"fields/#persistentvolumeclaim","text":"PersistentVolumeClaim is a user's request for and claim to a persistent volume Examples (click to open) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) Examples with this field (click to open) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml)","title":"PersistentVolumeClaim"},{"location":"fields/#fields_60","text":"Field Name Field Type Description apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds metadata ObjectMeta Standard object's metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata spec PersistentVolumeClaimSpec Spec defines the desired characteristics of a volume requested by a pod author. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims status PersistentVolumeClaimStatus Status represents the current information/status of a persistent volume claim. Read-only. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims","title":"Fields"},{"location":"fields/#volume","text":"Volume represents a named volume in a pod that may be accessed by any container in the pod. Examples with this field (click to open) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml)","title":"Volume"},{"location":"fields/#fields_61","text":"Field Name Field Type Description awsElasticBlockStore AWSElasticBlockStoreVolumeSource AWSElasticBlockStore represents an AWS Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore azureDisk AzureDiskVolumeSource AzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod. azureFile AzureFileVolumeSource AzureFile represents an Azure File Service mount on the host and bind mount to the pod. cephfs CephFSVolumeSource CephFS represents a Ceph FS mount on the host that shares a pod's lifetime cinder CinderVolumeSource Cinder represents a cinder volume attached and mounted on kubelets host machine More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md configMap ConfigMapVolumeSource ConfigMap represents a configMap that should populate this volume csi CSIVolumeSource CSI (Container Storage Interface) represents storage that is handled by an external CSI driver (Alpha feature). downwardAPI DownwardAPIVolumeSource DownwardAPI represents downward API about the pod that should populate this volume emptyDir EmptyDirVolumeSource EmptyDir represents a temporary directory that shares a pod's lifetime. More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir fc FCVolumeSource FC represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod. flexVolume FlexVolumeSource FlexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin. flocker FlockerVolumeSource Flocker represents a Flocker volume attached to a kubelet's host machine. This depends on the Flocker control service being running gcePersistentDisk GCEPersistentDiskVolumeSource GCEPersistentDisk represents a GCE Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk ~ gitRepo ~ ~ GitRepoVolumeSource ~ ~GitRepo represents a git repository at a particular revision.~ DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir into the Pod's container. glusterfs GlusterfsVolumeSource Glusterfs represents a Glusterfs mount on the host that shares a pod's lifetime. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md hostPath HostPathVolumeSource HostPath represents a pre-existing file or directory on the host machine that is directly exposed to the container. This is generally used for system agents or other privileged things that are allowed to see the host machine. Most containers will NOT need this. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath iscsi ISCSIVolumeSource ISCSI represents an ISCSI Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://releases.k8s.io/HEAD/examples/volumes/iscsi/README.md name string Volume's name. Must be a DNS_LABEL and unique within the pod. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names nfs NFSVolumeSource NFS represents an NFS mount on the host that shares a pod's lifetime More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs persistentVolumeClaim PersistentVolumeClaimVolumeSource PersistentVolumeClaimVolumeSource represents a reference to a PersistentVolumeClaim in the same namespace. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims photonPersistentDisk PhotonPersistentDiskVolumeSource PhotonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine portworxVolume PortworxVolumeSource PortworxVolume represents a portworx volume attached and mounted on kubelets host machine projected ProjectedVolumeSource Items for all in one resources secrets, configmaps, and downward API quobyte QuobyteVolumeSource Quobyte represents a Quobyte mount on the host that shares a pod's lifetime rbd RBDVolumeSource RBD represents a Rados Block Device mount on the host that shares a pod's lifetime. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md scaleIO ScaleIOVolumeSource ScaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes. secret SecretVolumeSource Secret represents a secret that should populate this volume. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret storageos StorageOSVolumeSource StorageOS represents a StorageOS volume attached and mounted on Kubernetes nodes. vsphereVolume VsphereVirtualDiskVolumeSource VsphereVolume represents a vSphere volume attached and mounted on kubelets host machine","title":"Fields"},{"location":"fields/#time","text":"Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON. Wrappers are provided for many of the factory methods that the time package offers.","title":"Time"},{"location":"fields/#objectreference","text":"ObjectReference contains enough information to let you inspect or modify the referred object.","title":"ObjectReference"},{"location":"fields/#fields_62","text":"Field Name Field Type Description apiVersion string API version of the referent. fieldPath string If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names namespace string Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ resourceVersion string Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency uid string UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids","title":"Fields"},{"location":"fields/#container","text":"A single application container that you want to run within a pod. Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml)","title":"Container"},{"location":"fields/#fields_63","text":"Field Name Field Type Description args Array< string > Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell command Array< string > Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell env Array< EnvVar > List of environment variables to set in the container. Cannot be updated. envFrom Array< EnvFromSource > List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. image string Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets. imagePullPolicy string Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images lifecycle Lifecycle Actions that the management system should take in response to container lifecycle events. Cannot be updated. livenessProbe Probe Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes name string Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated. ports Array< ContainerPort > List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \"0.0.0.0\" address inside a container will be accessible from the network. Cannot be updated. readinessProbe Probe Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes resources ResourceRequirements Compute Resources required by this container. Cannot be updated. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ securityContext SecurityContext Security options the pod should run with. More info: https://kubernetes.io/docs/concepts/policy/security-context/ More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ stdin boolean Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false. stdinOnce boolean Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false terminationMessagePath string Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated. terminationMessagePolicy string Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated. tty boolean Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false. volumeDevices Array< VolumeDevice > volumeDevices is the list of block devices to be used by the container. This is a beta feature. volumeMounts Array< VolumeMount > Pod volumes to mount into the container's filesystem. Cannot be updated. workingDir string Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated.","title":"Fields"},{"location":"fields/#intorstring","text":"IntOrString is a type that can hold an int32 or a string. When used in JSON or YAML marshalling and unmarshalling, it produces or consumes the inner type. This allows you to have, for example, a JSON field that can accept a name or number. Examples with this field (click to open) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml)","title":"IntOrString"},{"location":"fields/#envvar","text":"EnvVar represents an environment variable present in a Container. Examples with this field (click to open) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml)","title":"EnvVar"},{"location":"fields/#fields_64","text":"Field Name Field Type Description name string Name of the environment variable. Must be a C_IDENTIFIER. value string Variable references $(VAR_NAME) are expanded using the previous defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to \"\". valueFrom EnvVarSource Source for the environment variable's value. Cannot be used if value is not empty.","title":"Fields"},{"location":"fields/#envfromsource","text":"EnvFromSource represents the source of a set of ConfigMaps","title":"EnvFromSource"},{"location":"fields/#fields_65","text":"Field Name Field Type Description configMapRef ConfigMapEnvSource The ConfigMap to select from prefix string An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER. secretRef SecretEnvSource The Secret to select from","title":"Fields"},{"location":"fields/#lifecycle","text":"Lifecycle describes actions that the management system should take in response to container lifecycle events. For the PostStart and PreStop lifecycle handlers, management of the container blocks until the action is complete, unless the container process fails, in which case the handler is aborted.","title":"Lifecycle"},{"location":"fields/#fields_66","text":"Field Name Field Type Description postStart Handler PostStart is called immediately after a container is created. If the handler fails, the container is terminated and restarted according to its restart policy. Other management of the container blocks until the hook completes. More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks preStop Handler PreStop is called immediately before a container is terminated due to an API request or management event such as liveness probe failure, preemption, resource contention, etc. The handler is not called if the container crashes or exits. The reason for termination is passed to the handler. The Pod's termination grace period countdown begins before the PreStop hooked is executed. Regardless of the outcome of the handler, the container will eventually terminate within the Pod's termination grace period. Other management of the container blocks until the hook completes or until the termination grace period is reached. More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks","title":"Fields"},{"location":"fields/#probe","text":"Probe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic.","title":"Probe"},{"location":"fields/#fields_67","text":"Field Name Field Type Description exec ExecAction One and only one of the following should be specified. Exec specifies the action to take. failureThreshold int32 Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1. httpGet HTTPGetAction HTTPGet specifies the http request to perform. initialDelaySeconds int32 Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes periodSeconds int32 How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. successThreshold int32 Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness. Minimum value is 1. tcpSocket TCPSocketAction TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported timeoutSeconds int32 Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes","title":"Fields"},{"location":"fields/#containerport","text":"ContainerPort represents a network port in a single container. Examples with this field (click to open) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml)","title":"ContainerPort"},{"location":"fields/#fields_68","text":"Field Name Field Type Description containerPort int32 Number of port to expose on the pod's IP address. This must be a valid port number, 0 < x < 65536. hostIP string What host IP to bind the external port to. hostPort int32 Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. If HostNetwork is specified, this must match ContainerPort. Most containers do not need this. name string If specified, this must be an IANA_SVC_NAME and unique within the pod. Each named port in a pod must have a unique name. Name for the port that can be referred to by services. protocol string Protocol for port. Must be UDP, TCP, or SCTP. Defaults to \"TCP\".","title":"Fields"},{"location":"fields/#resourcerequirements","text":"ResourceRequirements describes the compute resource requirements. Examples with this field (click to open) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml)","title":"ResourceRequirements"},{"location":"fields/#fields_69","text":"Field Name Field Type Description limits Quantity Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ requests Quantity Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/","title":"Fields"},{"location":"fields/#securitycontext","text":"SecurityContext holds security configuration that will be applied to a container. Some fields are present in both SecurityContext and PodSecurityContext. When both are set, the values in SecurityContext take precedence. Examples with this field (click to open) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml)","title":"SecurityContext"},{"location":"fields/#fields_70","text":"Field Name Field Type Description allowPrivilegeEscalation boolean AllowPrivilegeEscalation controls whether a process can gain more privileges than its parent process. This bool directly controls if the no_new_privs flag will be set on the container process. AllowPrivilegeEscalation is true always when the container is: 1) run as Privileged 2) has CAP_SYS_ADMIN capabilities Capabilities The capabilities to add/drop when running containers. Defaults to the default set of capabilities granted by the container runtime. privileged boolean Run container in privileged mode. Processes in privileged containers are essentially equivalent to root on the host. Defaults to false. procMount string procMount denotes the type of proc mount to use for the containers. The default is DefaultProcMount which uses the container runtime defaults for readonly paths and masked paths. This requires the ProcMountType feature flag to be enabled. readOnlyRootFilesystem boolean Whether this container has a read-only root filesystem. Default is false. runAsGroup int64 The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. runAsNonRoot boolean Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. runAsUser int64 The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. seLinuxOptions SELinuxOptions The SELinux context to be applied to the container. If unspecified, the container runtime will allocate a random SELinux context for each container. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. windowsOptions WindowsSecurityContextOptions Windows security options.","title":"Fields"},{"location":"fields/#volumedevice","text":"volumeDevice describes a mapping of a raw block device within a container.","title":"VolumeDevice"},{"location":"fields/#fields_71","text":"Field Name Field Type Description devicePath string devicePath is the path inside of the container that the device will be mapped to. name string name must match the name of a persistentVolumeClaim in the pod","title":"Fields"},{"location":"fields/#volumemount","text":"VolumeMount describes a mounting of a Volume within a container. Examples with this field (click to open) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml)","title":"VolumeMount"},{"location":"fields/#fields_72","text":"Field Name Field Type Description mountPath string Path within the container at which the volume should be mounted. Must not contain ':'. mountPropagation string mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. This field is beta in 1.10. name string This must match the Name of a Volume. readOnly boolean Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. subPath string Path within the volume from which the container's volume should be mounted. Defaults to \"\" (volume's root). subPathExpr string Expanded path within the volume from which the container's volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment. Defaults to \"\" (volume's root). SubPathExpr and SubPath are mutually exclusive. This field is beta in 1.15.","title":"Fields"},{"location":"fields/#secretkeyselector","text":"SecretKeySelector selects a key of a Secret. Examples with this field (click to open) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml)","title":"SecretKeySelector"},{"location":"fields/#fields_73","text":"Field Name Field Type Description key string The key of the secret to select from. Must be a valid secret key. name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names optional boolean Specify whether the Secret or its key must be defined","title":"Fields"},{"location":"fields/#configmapkeyselector","text":"Selects a key from a ConfigMap. Examples with this field (click to open) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml)","title":"ConfigMapKeySelector"},{"location":"fields/#fields_74","text":"Field Name Field Type Description key string The key to select. name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names optional boolean Specify whether the ConfigMap or its key must be defined","title":"Fields"},{"location":"fields/#initializers","text":"Initializers tracks the progress of initialization.","title":"Initializers"},{"location":"fields/#fields_75","text":"Field Name Field Type Description pending Array< Initializer > Pending is a list of initializers that must execute in order before this object is visible. When the last pending initializer is removed, and no failing result is set, the initializers struct will be set to nil and the object is considered as initialized and visible to all clients. result Status If result is set with the Failure field, the object will be persisted to storage and then deleted, ensuring that other clients can observe the deletion.","title":"Fields"},{"location":"fields/#managedfieldsentry","text":"ManagedFieldsEntry is a workflow-id, a FieldSet and the group version of the resource that the fieldset applies to.","title":"ManagedFieldsEntry"},{"location":"fields/#fields_76","text":"Field Name Field Type Description apiVersion string APIVersion defines the version of this resource that this field set applies to. The format is \"group/version\" just like the top-level APIVersion field. It is necessary to track the version of a field set because it cannot be automatically converted. fields Fields Fields identifies a set of fields. manager string Manager is an identifier of the workflow managing these fields. operation string Operation is the type of operation which lead to this ManagedFieldsEntry being created. The only valid values for this field are 'Apply' and 'Update'. time Time Time is timestamp of when these fields were set. It should always be empty if Operation is 'Apply'","title":"Fields"},{"location":"fields/#ownerreference","text":"OwnerReference contains enough information to let you identify an owning object. An owning object must be in the same namespace as the dependent, or be cluster-scoped, so there is no namespace field. Examples with this field (click to open) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml)","title":"OwnerReference"},{"location":"fields/#fields_77","text":"Field Name Field Type Description apiVersion string API version of the referent. blockOwnerDeletion boolean If true, AND if the owner has the \"foregroundDeletion\" finalizer, then the owner cannot be deleted from the key-value store until this reference is removed. Defaults to false. To set this field, a user needs \"delete\" permission of the owner, otherwise 422 (Unprocessable Entity) will be returned. controller boolean If true, this reference points to the managing controller. kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds name string Name of the referent. More info: http://kubernetes.io/docs/user-guide/identifiers#names uid string UID of the referent. More info: http://kubernetes.io/docs/user-guide/identifiers#uids","title":"Fields"},{"location":"fields/#nodeaffinity","text":"Node affinity is a group of node affinity scheduling rules.","title":"NodeAffinity"},{"location":"fields/#fields_78","text":"Field Name Field Type Description preferredDuringSchedulingIgnoredDuringExecution Array< PreferredSchedulingTerm > The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node matches the corresponding matchExpressions; the node(s) with the highest sum are the most preferred. requiredDuringSchedulingIgnoredDuringExecution NodeSelector If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to an update), the system may or may not try to eventually evict the pod from its node.","title":"Fields"},{"location":"fields/#podaffinity","text":"Pod affinity is a group of inter pod affinity scheduling rules.","title":"PodAffinity"},{"location":"fields/#fields_79","text":"Field Name Field Type Description preferredDuringSchedulingIgnoredDuringExecution Array< WeightedPodAffinityTerm > The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred. requiredDuringSchedulingIgnoredDuringExecution Array< PodAffinityTerm > If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied.","title":"Fields"},{"location":"fields/#podantiaffinity","text":"Pod anti affinity is a group of inter pod anti affinity scheduling rules.","title":"PodAntiAffinity"},{"location":"fields/#fields_80","text":"Field Name Field Type Description preferredDuringSchedulingIgnoredDuringExecution Array< WeightedPodAffinityTerm > The scheduler will prefer to schedule pods to nodes that satisfy the anti-affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred. requiredDuringSchedulingIgnoredDuringExecution Array< PodAffinityTerm > If the anti-affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the anti-affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied.","title":"Fields"},{"location":"fields/#poddnsconfigoption","text":"PodDNSConfigOption defines DNS resolver options of a pod. Examples with this field (click to open) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml)","title":"PodDNSConfigOption"},{"location":"fields/#fields_81","text":"Field Name Field Type Description name string Required. value string No description available","title":"Fields"},{"location":"fields/#labelselector","text":"A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects. Examples with this field (click to open) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml)","title":"LabelSelector"},{"location":"fields/#fields_82","text":"Field Name Field Type Description matchExpressions Array< LabelSelectorRequirement > matchExpressions is a list of label selector requirements. The requirements are ANDed. matchLabels Map< string , string > matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed.","title":"Fields"},{"location":"fields/#selinuxoptions","text":"SELinuxOptions are the labels to be applied to the container","title":"SELinuxOptions"},{"location":"fields/#fields_83","text":"Field Name Field Type Description level string Level is SELinux level label that applies to the container. role string Role is a SELinux role label that applies to the container. type string Type is a SELinux type label that applies to the container. user string User is a SELinux user label that applies to the container.","title":"Fields"},{"location":"fields/#sysctl","text":"Sysctl defines a kernel parameter to be set","title":"Sysctl"},{"location":"fields/#fields_84","text":"Field Name Field Type Description name string Name of a property to set value string Value of a property to set","title":"Fields"},{"location":"fields/#windowssecuritycontextoptions","text":"WindowsSecurityContextOptions contain Windows-specific options and credentials.","title":"WindowsSecurityContextOptions"},{"location":"fields/#fields_85","text":"Field Name Field Type Description gmsaCredentialSpec string GMSACredentialSpec is where the GMSA admission webhook (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the GMSA credential spec named by the GMSACredentialSpecName field. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag. gmsaCredentialSpecName string GMSACredentialSpecName is the name of the GMSA credential spec to use. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag.","title":"Fields"},{"location":"fields/#persistentvolumeclaimspec","text":"PersistentVolumeClaimSpec describes the common attributes of storage devices and allows a Source for provider-specific attributes Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml)","title":"PersistentVolumeClaimSpec"},{"location":"fields/#fields_86","text":"Field Name Field Type Description accessModes Array< string > AccessModes contains the desired access modes the volume should have. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1 dataSource TypedLocalObjectReference This field requires the VolumeSnapshotDataSource alpha feature gate to be enabled and currently VolumeSnapshot is the only supported data source. If the provisioner can support VolumeSnapshot data source, it will create a new volume and data will be restored to the volume at the same time. If the provisioner does not support VolumeSnapshot data source, volume will not be created and the failure will be reported as an event. In the future, we plan to support more data source types and the behavior of the provisioner may change. resources ResourceRequirements Resources represents the minimum resources the volume should have. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources selector LabelSelector A label query over volumes to consider for binding. storageClassName string Name of the StorageClass required by the claim. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1 volumeMode string volumeMode defines what type of volume is required by the claim. Value of Filesystem is implied when not included in claim spec. This is a beta feature. volumeName string VolumeName is the binding reference to the PersistentVolume backing this claim.","title":"Fields"},{"location":"fields/#persistentvolumeclaimstatus","text":"PersistentVolumeClaimStatus is the current status of a persistent volume claim.","title":"PersistentVolumeClaimStatus"},{"location":"fields/#fields_87","text":"Field Name Field Type Description accessModes Array< string > AccessModes contains the actual access modes the volume backing the PVC has. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1 capacity Quantity Represents the actual resources of the underlying volume. conditions Array< PersistentVolumeClaimCondition > Current Condition of persistent volume claim. If underlying persistent volume is being resized then the Condition will be set to 'ResizeStarted'. phase string Phase represents the current phase of PersistentVolumeClaim.","title":"Fields"},{"location":"fields/#awselasticblockstorevolumesource","text":"Represents a Persistent Disk resource in AWS.An AWS EBS disk must exist before mounting to a container. The disk must also be in the same AWS zone as the kubelet. An AWS EBS disk can only be mounted as read/write once. AWS EBS volumes support ownership management and SELinux relabeling.","title":"AWSElasticBlockStoreVolumeSource"},{"location":"fields/#fields_88","text":"Field Name Field Type Description fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore partition int32 The partition in the volume that you want to mount. If omitted, the default is to mount by volume name. Examples: For volume /dev/sda1, you specify the partition as \"1\". Similarly, the volume partition for /dev/sda is \"0\" (or you can leave the property empty). readOnly boolean Specify \"true\" to force and set the ReadOnly property in VolumeMounts to \"true\". If omitted, the default is \"false\". More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore volumeID string Unique ID of the persistent disk resource in AWS (Amazon EBS volume). More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore","title":"Fields"},{"location":"fields/#azurediskvolumesource","text":"AzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.","title":"AzureDiskVolumeSource"},{"location":"fields/#fields_89","text":"Field Name Field Type Description cachingMode string Host Caching mode: None, Read Only, Read Write. diskName string The Name of the data disk in the blob storage diskURI string The URI the data disk in the blob storage fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. kind string Expected values Shared: multiple blob disks per storage account Dedicated: single blob disk per storage account Managed: azure managed data disk (only in managed availability set). defaults to shared readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts.","title":"Fields"},{"location":"fields/#azurefilevolumesource","text":"AzureFile represents an Azure File Service mount on the host and bind mount to the pod.","title":"AzureFileVolumeSource"},{"location":"fields/#fields_90","text":"Field Name Field Type Description readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. secretName string the name of secret that contains Azure Storage Account Name and Key shareName string Share Name","title":"Fields"},{"location":"fields/#cephfsvolumesource","text":"Represents a Ceph Filesystem mount that lasts the lifetime of a pod Cephfs volumes do not support ownership management or SELinux relabeling.","title":"CephFSVolumeSource"},{"location":"fields/#fields_91","text":"Field Name Field Type Description monitors Array< string > Required: Monitors is a collection of Ceph monitors More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it path string Optional: Used as the mounted root, rather than the full Ceph tree, default is / readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it secretFile string Optional: SecretFile is the path to key ring for User, default is /etc/ceph/user.secret More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it secretRef LocalObjectReference Optional: SecretRef is reference to the authentication secret for User, default is empty. More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it user string Optional: User is the rados user name, default is admin More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it","title":"Fields"},{"location":"fields/#cindervolumesource","text":"Represents a cinder volume resource in Openstack. A Cinder volume must exist before mounting to a container. The volume must also be in the same region as the kubelet. Cinder volumes support ownership management and SELinux relabeling.","title":"CinderVolumeSource"},{"location":"fields/#fields_92","text":"Field Name Field Type Description fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md secretRef LocalObjectReference Optional: points to a secret object containing parameters used to connect to OpenStack. volumeID string volume id used to identify the volume in cinder More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md","title":"Fields"},{"location":"fields/#configmapvolumesource","text":"Adapts a ConfigMap into a volume.The contents of the target ConfigMap's Data field will be presented in a volume as files using the keys in the Data field as the file names, unless the items element is populated with specific mappings of keys to paths. ConfigMap volumes support ownership management and SELinux relabeling.","title":"ConfigMapVolumeSource"},{"location":"fields/#fields_93","text":"Field Name Field Type Description defaultMode int32 Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. items Array< KeyToPath > If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names optional boolean Specify whether the ConfigMap or its keys must be defined","title":"Fields"},{"location":"fields/#csivolumesource","text":"Represents a source location of a volume to mount, managed by an external CSI driver","title":"CSIVolumeSource"},{"location":"fields/#fields_94","text":"Field Name Field Type Description driver string Driver is the name of the CSI driver that handles this volume. Consult with your admin for the correct name as registered in the cluster. fsType string Filesystem type to mount. Ex. \"ext4\", \"xfs\", \"ntfs\". If not provided, the empty value is passed to the associated CSI driver which will determine the default filesystem to apply. nodePublishSecretRef LocalObjectReference NodePublishSecretRef is a reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI NodePublishVolume and NodeUnpublishVolume calls. This field is optional, and may be empty if no secret is required. If the secret object contains more than one secret, all secret references are passed. readOnly boolean Specifies a read-only configuration for the volume. Defaults to false (read/write). volumeAttributes Map< string , string > VolumeAttributes stores driver-specific properties that are passed to the CSI driver. Consult your driver's documentation for supported values.","title":"Fields"},{"location":"fields/#downwardapivolumesource","text":"DownwardAPIVolumeSource represents a volume containing downward API info. Downward API volumes support ownership management and SELinux relabeling.","title":"DownwardAPIVolumeSource"},{"location":"fields/#fields_95","text":"Field Name Field Type Description defaultMode int32 Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. items Array< DownwardAPIVolumeFile > Items is a list of downward API volume file","title":"Fields"},{"location":"fields/#emptydirvolumesource","text":"Represents an empty directory for a pod. Empty directory volumes support ownership management and SELinux relabeling. Examples with this field (click to open) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml)","title":"EmptyDirVolumeSource"},{"location":"fields/#fields_96","text":"Field Name Field Type Description medium string What type of storage medium should back this directory. The default is \"\" which means to use the node's default medium. Must be an empty string (default) or Memory. More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir sizeLimit Quantity Total amount of local storage required for this EmptyDir volume. The size limit is also applicable for memory medium. The maximum usage on memory medium EmptyDir would be the minimum value between the SizeLimit specified here and the sum of memory limits of all containers in a pod. The default is nil which means that the limit is undefined. More info: http://kubernetes.io/docs/user-guide/volumes#emptydir","title":"Fields"},{"location":"fields/#fcvolumesource","text":"Represents a Fibre Channel volume. Fibre Channel volumes can only be mounted as read/write once. Fibre Channel volumes support ownership management and SELinux relabeling.","title":"FCVolumeSource"},{"location":"fields/#fields_97","text":"Field Name Field Type Description fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. lun int32 Optional: FC target lun number readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. targetWWNs Array< string > Optional: FC target worldwide names (WWNs) wwids Array< string > Optional: FC volume world wide identifiers (wwids) Either wwids or combination of targetWWNs and lun must be set, but not both simultaneously.","title":"Fields"},{"location":"fields/#flexvolumesource","text":"FlexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin.","title":"FlexVolumeSource"},{"location":"fields/#fields_98","text":"Field Name Field Type Description driver string Driver is the name of the driver to use for this volume. fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". The default filesystem depends on FlexVolume script. options Map< string , string > Optional: Extra command options if any. readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. secretRef LocalObjectReference Optional: SecretRef is reference to the secret object containing sensitive information to pass to the plugin scripts. This may be empty if no secret object is specified. If the secret object contains more than one secret, all secrets are passed to the plugin scripts.","title":"Fields"},{"location":"fields/#flockervolumesource","text":"Represents a Flocker volume mounted by the Flocker agent. One and only one of datasetName and datasetUUID should be set. Flocker volumes do not support ownership management or SELinux relabeling.","title":"FlockerVolumeSource"},{"location":"fields/#fields_99","text":"Field Name Field Type Description datasetName string Name of the dataset stored as metadata -> name on the dataset for Flocker should be considered as deprecated datasetUUID string UUID of the dataset. This is unique identifier of a Flocker dataset","title":"Fields"},{"location":"fields/#gcepersistentdiskvolumesource","text":"Represents a Persistent Disk resource in Google Compute Engine.A GCE PD must exist before mounting to a container. The disk must also be in the same GCE project and zone as the kubelet. A GCE PD can only be mounted as read/write once or read-only many times. GCE PDs support ownership management and SELinux relabeling.","title":"GCEPersistentDiskVolumeSource"},{"location":"fields/#fields_100","text":"Field Name Field Type Description fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk partition int32 The partition in the volume that you want to mount. If omitted, the default is to mount by volume name. Examples: For volume /dev/sda1, you specify the partition as \"1\". Similarly, the volume partition for /dev/sda is \"0\" (or you can leave the property empty). More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk pdName string Unique name of the PD resource in GCE. Used to identify the disk in GCE. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk readOnly boolean ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk","title":"Fields"},{"location":"fields/#gitrepovolumesource","text":"Represents a volume that is populated with the contents of a git repository. Git repo volumes do not support ownership management. Git repo volumes support SELinux relabeling.DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir into the Pod's container.","title":"GitRepoVolumeSource"},{"location":"fields/#fields_101","text":"Field Name Field Type Description directory string Target directory name. Must not contain or start with '..'. If '.' is supplied, the volume directory will be the git repository. Otherwise, if specified, the volume will contain the git repository in the subdirectory with the given name. repository string Repository URL revision string Commit hash for the specified revision.","title":"Fields"},{"location":"fields/#glusterfsvolumesource","text":"Represents a Glusterfs mount that lasts the lifetime of a pod. Glusterfs volumes do not support ownership management or SELinux relabeling.","title":"GlusterfsVolumeSource"},{"location":"fields/#fields_102","text":"Field Name Field Type Description endpoints string EndpointsName is the endpoint name that details Glusterfs topology. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md#create-a-pod path string Path is the Glusterfs volume path. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md#create-a-pod readOnly boolean ReadOnly here will force the Glusterfs volume to be mounted with read-only permissions. Defaults to false. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md#create-a-pod","title":"Fields"},{"location":"fields/#hostpathvolumesource","text":"Represents a host path mapped into a pod. Host path volumes do not support ownership management or SELinux relabeling.","title":"HostPathVolumeSource"},{"location":"fields/#fields_103","text":"Field Name Field Type Description path string Path of the directory on the host. If the path is a symlink, it will follow the link to the real path. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath type string Type for HostPath Volume Defaults to \"\" More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath","title":"Fields"},{"location":"fields/#iscsivolumesource","text":"Represents an ISCSI disk. ISCSI volumes can only be mounted as read/write once. ISCSI volumes support ownership management and SELinux relabeling.","title":"ISCSIVolumeSource"},{"location":"fields/#fields_104","text":"Field Name Field Type Description chapAuthDiscovery boolean whether support iSCSI Discovery CHAP authentication chapAuthSession boolean whether support iSCSI Session CHAP authentication fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#iscsi initiatorName string Custom iSCSI Initiator Name. If initiatorName is specified with iscsiInterface simultaneously, new iSCSI interface : will be created for the connection. iqn string Target iSCSI Qualified Name. iscsiInterface string iSCSI Interface Name that uses an iSCSI transport. Defaults to 'default' (tcp). lun int32 iSCSI Target Lun number. portals Array< string > iSCSI Target Portal List. The portal is either an IP or ip_addr:port if the port is other than default (typically TCP ports 860 and 3260). readOnly boolean ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. secretRef LocalObjectReference CHAP Secret for iSCSI target and initiator authentication targetPortal string iSCSI Target Portal. The Portal is either an IP or ip_addr:port if the port is other than default (typically TCP ports 860 and 3260).","title":"Fields"},{"location":"fields/#nfsvolumesource","text":"Represents an NFS mount that lasts the lifetime of a pod. NFS volumes do not support ownership management or SELinux relabeling.","title":"NFSVolumeSource"},{"location":"fields/#fields_105","text":"Field Name Field Type Description path string Path that is exported by the NFS server. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs readOnly boolean ReadOnly here will force the NFS export to be mounted with read-only permissions. Defaults to false. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs server string Server is the hostname or IP address of the NFS server. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs","title":"Fields"},{"location":"fields/#persistentvolumeclaimvolumesource","text":"PersistentVolumeClaimVolumeSource references the user's PVC in the same namespace. This volume finds the bound PV and mounts that volume for the pod. A PersistentVolumeClaimVolumeSource is, essentially, a wrapper around another type of volume that is owned by someone else (the system). Examples with this field (click to open) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml)","title":"PersistentVolumeClaimVolumeSource"},{"location":"fields/#fields_106","text":"Field Name Field Type Description claimName string ClaimName is the name of a PersistentVolumeClaim in the same namespace as the pod using this volume. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims readOnly boolean Will force the ReadOnly setting in VolumeMounts. Default false.","title":"Fields"},{"location":"fields/#photonpersistentdiskvolumesource","text":"Represents a Photon Controller persistent disk resource.","title":"PhotonPersistentDiskVolumeSource"},{"location":"fields/#fields_107","text":"Field Name Field Type Description fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. pdID string ID that identifies Photon Controller persistent disk","title":"Fields"},{"location":"fields/#portworxvolumesource","text":"PortworxVolumeSource represents a Portworx volume resource.","title":"PortworxVolumeSource"},{"location":"fields/#fields_108","text":"Field Name Field Type Description fsType string FSType represents the filesystem type to mount Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\". Implicitly inferred to be \"ext4\" if unspecified. readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. volumeID string VolumeID uniquely identifies a Portworx volume","title":"Fields"},{"location":"fields/#projectedvolumesource","text":"Represents a projected volume source","title":"ProjectedVolumeSource"},{"location":"fields/#fields_109","text":"Field Name Field Type Description defaultMode int32 Mode bits to use on created files by default. Must be a value between 0 and 0777. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. sources Array< VolumeProjection > list of volume projections","title":"Fields"},{"location":"fields/#quobytevolumesource","text":"Represents a Quobyte mount that lasts the lifetime of a pod. Quobyte volumes do not support ownership management or SELinux relabeling.","title":"QuobyteVolumeSource"},{"location":"fields/#fields_110","text":"Field Name Field Type Description group string Group to map volume access to Default is no group readOnly boolean ReadOnly here will force the Quobyte volume to be mounted with read-only permissions. Defaults to false. registry string Registry represents a single or multiple Quobyte Registry services specified as a string as host:port pair (multiple entries are separated with commas) which acts as the central registry for volumes tenant string Tenant owning the given Quobyte volume in the Backend Used with dynamically provisioned Quobyte volumes, value is set by the plugin user string User to map volume access to Defaults to serivceaccount user volume string Volume is a string that references an already created Quobyte volume by name.","title":"Fields"},{"location":"fields/#rbdvolumesource","text":"Represents a Rados Block Device mount that lasts the lifetime of a pod. RBD volumes support ownership management and SELinux relabeling.","title":"RBDVolumeSource"},{"location":"fields/#fields_111","text":"Field Name Field Type Description fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#rbd image string The rados image name. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it keyring string Keyring is the path to key ring for RBDUser. Default is /etc/ceph/keyring. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it monitors Array< string > A collection of Ceph monitors. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it pool string The rados pool name. Default is rbd. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it readOnly boolean ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it secretRef LocalObjectReference SecretRef is name of the authentication secret for RBDUser. If provided overrides keyring. Default is nil. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it user string The rados user name. Default is admin. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it","title":"Fields"},{"location":"fields/#scaleiovolumesource","text":"ScaleIOVolumeSource represents a persistent ScaleIO volume","title":"ScaleIOVolumeSource"},{"location":"fields/#fields_112","text":"Field Name Field Type Description fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Default is \"xfs\". gateway string The host address of the ScaleIO API Gateway. protectionDomain string The name of the ScaleIO Protection Domain for the configured storage. readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. secretRef LocalObjectReference SecretRef references to the secret for ScaleIO user and other sensitive information. If this is not provided, Login operation will fail. sslEnabled boolean Flag to enable/disable SSL communication with Gateway, default false storageMode string Indicates whether the storage for a volume should be ThickProvisioned or ThinProvisioned. Default is ThinProvisioned. storagePool string The ScaleIO Storage Pool associated with the protection domain. system string The name of the storage system as configured in ScaleIO. volumeName string The name of a volume already created in the ScaleIO system that is associated with this volume source.","title":"Fields"},{"location":"fields/#secretvolumesource","text":"Adapts a Secret into a volume.The contents of the target Secret's Data field will be presented in a volume as files using the keys in the Data field as the file names. Secret volumes support ownership management and SELinux relabeling. Examples with this field (click to open) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml)","title":"SecretVolumeSource"},{"location":"fields/#fields_113","text":"Field Name Field Type Description defaultMode int32 Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. items Array< KeyToPath > If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. optional boolean Specify whether the Secret or its keys must be defined secretName string Name of the secret in the pod's namespace to use. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret","title":"Fields"},{"location":"fields/#storageosvolumesource","text":"Represents a StorageOS persistent volume resource.","title":"StorageOSVolumeSource"},{"location":"fields/#fields_114","text":"Field Name Field Type Description fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. secretRef LocalObjectReference SecretRef specifies the secret to use for obtaining the StorageOS API credentials. If not specified, default values will be attempted. volumeName string VolumeName is the human-readable name of the StorageOS volume. Volume names are only unique within a namespace. volumeNamespace string VolumeNamespace specifies the scope of the volume within StorageOS. If no namespace is specified then the Pod's namespace will be used. This allows the Kubernetes name scoping to be mirrored within StorageOS for tighter integration. Set VolumeName to any name to override the default behaviour. Set to \"default\" if you are not using namespaces within StorageOS. Namespaces that do not pre-exist within StorageOS will be created.","title":"Fields"},{"location":"fields/#vspherevirtualdiskvolumesource","text":"Represents a vSphere volume resource.","title":"VsphereVirtualDiskVolumeSource"},{"location":"fields/#fields_115","text":"Field Name Field Type Description fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. storagePolicyID string Storage Policy Based Management (SPBM) profile ID associated with the StoragePolicyName. storagePolicyName string Storage Policy Based Management (SPBM) profile name. volumePath string Path that identifies vSphere volume vmdk","title":"Fields"},{"location":"fields/#envvarsource","text":"EnvVarSource represents a source for the value of an EnvVar. Examples with this field (click to open) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml)","title":"EnvVarSource"},{"location":"fields/#fields_116","text":"Field Name Field Type Description configMapKeyRef ConfigMapKeySelector Selects a key of a ConfigMap. fieldRef ObjectFieldSelector Selects a field of the pod: supports metadata.name, metadata.namespace, metadata.labels, metadata.annotations, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP. resourceFieldRef ResourceFieldSelector Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. secretKeyRef SecretKeySelector Selects a key of a secret in the pod's namespace","title":"Fields"},{"location":"fields/#configmapenvsource","text":"ConfigMapEnvSource selects a ConfigMap to populate the environment variables with.The contents of the target ConfigMap's Data field will represent the key-value pairs as environment variables.","title":"ConfigMapEnvSource"},{"location":"fields/#fields_117","text":"Field Name Field Type Description name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names optional boolean Specify whether the ConfigMap must be defined","title":"Fields"},{"location":"fields/#secretenvsource","text":"SecretEnvSource selects a Secret to populate the environment variables with.The contents of the target Secret's Data field will represent the key-value pairs as environment variables.","title":"SecretEnvSource"},{"location":"fields/#fields_118","text":"Field Name Field Type Description name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names optional boolean Specify whether the Secret must be defined","title":"Fields"},{"location":"fields/#handler","text":"Handler defines a specific action that should be taken","title":"Handler"},{"location":"fields/#fields_119","text":"Field Name Field Type Description exec ExecAction One and only one of the following should be specified. Exec specifies the action to take. httpGet HTTPGetAction HTTPGet specifies the http request to perform. tcpSocket TCPSocketAction TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported","title":"Fields"},{"location":"fields/#execaction","text":"ExecAction describes a \"run in container\" action.","title":"ExecAction"},{"location":"fields/#fields_120","text":"Field Name Field Type Description command Array< string > Command is the command line to execute inside the container, the working directory for the command is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('","title":"Fields"},{"location":"fields/#httpgetaction","text":"HTTPGetAction describes an action based on HTTP Get requests. Examples with this field (click to open) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml)","title":"HTTPGetAction"},{"location":"fields/#fields_121","text":"Field Name Field Type Description host string Host name to connect to, defaults to the pod IP. You probably want to set \"Host\" in httpHeaders instead. httpHeaders Array< HTTPHeader > Custom headers to set in the request. HTTP allows repeated headers. path string Path to access on the HTTP server. port IntOrString Name or number of the port to access on the container. Number must be in the range 1 to 65535. Name must be an IANA_SVC_NAME. scheme string Scheme to use for connecting to the host. Defaults to HTTP.","title":"Fields"},{"location":"fields/#tcpsocketaction","text":"TCPSocketAction describes an action based on opening a socket","title":"TCPSocketAction"},{"location":"fields/#fields_122","text":"Field Name Field Type Description host string Optional: Host name to connect to, defaults to the pod IP. port IntOrString Number or name of the port to access on the container. Number must be in the range 1 to 65535. Name must be an IANA_SVC_NAME.","title":"Fields"},{"location":"fields/#quantity","text":"Quantity is a fixed-point representation of a number. It provides convenient marshaling/unmarshaling in JSON and YAML, in addition to String() and Int64() accessors.The serialization format is: ::= (Note that may be empty, from the \"\" case in .) ::= 0 | 1 | ... | 9 ::= | ::= | . | . | . ::= \"+\" | \"-\" ::= | ::= | | ::= Ki | Mi | Gi | Ti | Pi | Ei (International System of units; See: http://physics.nist.gov/cuu/Units/binary.html) ::= m | \"\" | k | M | G | T | P | E (Note that 1024 = 1Ki but 1000 = 1k; I didn't choose the capitalization.) ::= \"e\" | \"E\" No matter which of the three exponent forms is used, no quantity may represent a number greater than 2^63-1 in magnitude, nor may it have more than 3 decimal places. Numbers larger or more precise will be capped or rounded up. (E.g.: 0.1m will rounded up to 1m.) This may be extended in the future if we require larger or smaller quantities.When a Quantity is parsed from a string, it will remember the type of suffix it had, and will use the same type again when it is serialized.Before serializing, Quantity will be put in \"canonical form\". This means that Exponent/suffix will be adjusted up or down (with a corresponding increase or decrease in Mantissa) such that: a. No precision is lost b. No fractional digits will be emitted c. The exponent (or suffix) is as large as possible.The sign will be omitted unless the number is negative.Examples: 1.5 will be serialized as \"1500m\" 1.5Gi will be serialized as \"1536Mi\"Note that the quantity will NEVER be internally represented by a floating point number. That is the whole point of this exercise.Non-canonical values will still parse as long as they are well formed, but will be re-emitted in their canonical form. (So always use canonical form, or don't diff.)This format is intended to make it difficult to use these numbers without writing some sort of special handling code in the hopes that that will cause implementors to also use a fixed point implementation. Examples with this field (click to open) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml)","title":"Quantity"},{"location":"fields/#capabilities","text":"Adds and removes POSIX capabilities from running containers.","title":"Capabilities"},{"location":"fields/#fields_123","text":"Field Name Field Type Description add Array< string > Added capabilities drop Array< string > Removed capabilities","title":"Fields"},{"location":"fields/#initializer","text":"Initializer is information about an initializer that has not yet completed.","title":"Initializer"},{"location":"fields/#fields_124","text":"Field Name Field Type Description name string name of the process that is responsible for initializing this object.","title":"Fields"},{"location":"fields/#status","text":"Status is a return value for calls that don't return other objects.","title":"Status"},{"location":"fields/#fields_125","text":"Field Name Field Type Description apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources code int32 Suggested HTTP return code for this status, 0 if not set. details StatusDetails Extended data associated with the reason. Each reason may define its own extended details. This field is optional and the data returned is not guaranteed to conform to any schema except that defined by the reason type. kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds message string A human-readable description of the status of this operation. metadata ListMeta Standard list metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds reason string A machine-readable description of why this operation is in the \"Failure\" status. If this value is empty there is no information available. A Reason clarifies an HTTP status code but does not override it. status string Status of the operation. One of: \"Success\" or \"Failure\". More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status","title":"Fields"},{"location":"fields/#fields_126","text":"Fields stores a set of fields in a data structure like a Trie. To understand how this is used, see: https://github.com/kubernetes-sigs/structured-merge-diff","title":"Fields"},{"location":"fields/#preferredschedulingterm","text":"An empty preferred scheduling term matches all objects with implicit weight 0 (i.e. it's a no-op). A null preferred scheduling term matches no objects (i.e. is also a no-op).","title":"PreferredSchedulingTerm"},{"location":"fields/#fields_127","text":"Field Name Field Type Description preference NodeSelectorTerm A node selector term, associated with the corresponding weight. weight int32 Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100.","title":"Fields"},{"location":"fields/#nodeselector","text":"A node selector represents the union of the results of one or more label queries over a set of nodes; that is, it represents the OR of the selectors represented by the node selector terms.","title":"NodeSelector"},{"location":"fields/#fields_128","text":"Field Name Field Type Description nodeSelectorTerms Array< NodeSelectorTerm > Required. A list of node selector terms. The terms are ORed.","title":"Fields"},{"location":"fields/#weightedpodaffinityterm","text":"The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)","title":"WeightedPodAffinityTerm"},{"location":"fields/#fields_129","text":"Field Name Field Type Description podAffinityTerm PodAffinityTerm Required. A pod affinity term, associated with the corresponding weight. weight int32 weight associated with matching the corresponding podAffinityTerm, in the range 1-100.","title":"Fields"},{"location":"fields/#podaffinityterm","text":"Defines a set of pods (namely those matching the labelSelector relative to the given namespace(s)) that this pod should be co-located (affinity) or not co-located (anti-affinity) with, where co-located is defined as running on a node whose value of the label with key matches that of any node on which a pod of the set of pods is running","title":"PodAffinityTerm"},{"location":"fields/#fields_130","text":"Field Name Field Type Description labelSelector LabelSelector A label query over a set of resources, in this case pods. namespaces Array< string > namespaces specifies which namespaces the labelSelector applies to (matches against); null or empty list means \"this pod's namespace\" topologyKey string This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching the labelSelector in the specified namespaces, where co-located is defined as running on a node whose value of the label with key topologyKey matches that of any node on which any of the selected pods is running. Empty topologyKey is not allowed.","title":"Fields"},{"location":"fields/#labelselectorrequirement","text":"A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.","title":"LabelSelectorRequirement"},{"location":"fields/#fields_131","text":"Field Name Field Type Description key string key is the label key that the selector applies to. operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. values Array< string > values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.","title":"Fields"},{"location":"fields/#typedlocalobjectreference","text":"TypedLocalObjectReference contains enough information to let you locate the typed referenced object inside the same namespace.","title":"TypedLocalObjectReference"},{"location":"fields/#fields_132","text":"Field Name Field Type Description apiGroup string APIGroup is the group for the resource being referenced. If APIGroup is not specified, the specified Kind must be in the core API group. For any other third-party types, APIGroup is required. kind string Kind is the type of resource being referenced name string Name is the name of resource being referenced","title":"Fields"},{"location":"fields/#persistentvolumeclaimcondition","text":"PersistentVolumeClaimCondition contails details about state of pvc","title":"PersistentVolumeClaimCondition"},{"location":"fields/#fields_133","text":"Field Name Field Type Description lastProbeTime Time Last time we probed the condition. lastTransitionTime Time Last time the condition transitioned from one status to another. message string Human-readable message indicating details about last transition. reason string Unique, this should be a short, machine understandable string that gives the reason for condition's last transition. If it reports \"ResizeStarted\" that means the underlying persistent volume is being resized. status string No description available type string No description available","title":"Fields"},{"location":"fields/#keytopath","text":"Maps a string key to a path within a volume.","title":"KeyToPath"},{"location":"fields/#fields_134","text":"Field Name Field Type Description key string The key to project. mode int32 Optional: mode bits to use on this file, must be a value between 0 and 0777. If not specified, the volume defaultMode will be used. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. path string The relative path of the file to map the key to. May not be an absolute path. May not contain the path element '..'. May not start with the string '..'.","title":"Fields"},{"location":"fields/#downwardapivolumefile","text":"DownwardAPIVolumeFile represents information to create the file containing the pod field","title":"DownwardAPIVolumeFile"},{"location":"fields/#fields_135","text":"Field Name Field Type Description fieldRef ObjectFieldSelector Required: Selects a field of the pod: only annotations, labels, name and namespace are supported. mode int32 Optional: mode bits to use on this file, must be a value between 0 and 0777. If not specified, the volume defaultMode will be used. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. path string Required: Path is the relative path name of the file to be created. Must not be absolute or contain the '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..' resourceFieldRef ResourceFieldSelector Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.","title":"Fields"},{"location":"fields/#volumeprojection","text":"Projection that may be projected along with other supported volume types","title":"VolumeProjection"},{"location":"fields/#fields_136","text":"Field Name Field Type Description configMap ConfigMapProjection information about the configMap data to project downwardAPI DownwardAPIProjection information about the downwardAPI data to project secret SecretProjection information about the secret data to project serviceAccountToken ServiceAccountTokenProjection information about the serviceAccountToken data to project","title":"Fields"},{"location":"fields/#objectfieldselector","text":"ObjectFieldSelector selects an APIVersioned field of an object.","title":"ObjectFieldSelector"},{"location":"fields/#fields_137","text":"Field Name Field Type Description apiVersion string Version of the schema the FieldPath is written in terms of, defaults to \"v1\". fieldPath string Path of the field to select in the specified API version.","title":"Fields"},{"location":"fields/#resourcefieldselector","text":"ResourceFieldSelector represents container resources (cpu, memory) and their output format","title":"ResourceFieldSelector"},{"location":"fields/#fields_138","text":"Field Name Field Type Description containerName string Container name: required for volumes, optional for env vars divisor Quantity Specifies the output format of the exposed resources, defaults to \"1\" resource string Required: resource to select","title":"Fields"},{"location":"fields/#httpheader","text":"HTTPHeader describes a custom header to be used in HTTP probes","title":"HTTPHeader"},{"location":"fields/#fields_139","text":"Field Name Field Type Description name string The header field name value string The header field value","title":"Fields"},{"location":"fields/#statusdetails","text":"StatusDetails is a set of additional properties that MAY be set by the server to provide additional information about a response. The Reason field of a Status object defines what attributes will be set. Clients must ignore fields that do not match the defined type of each attribute, and should assume that any attribute may be empty, invalid, or under defined.","title":"StatusDetails"},{"location":"fields/#fields_140","text":"Field Name Field Type Description causes Array< StatusCause > The Causes array includes more details associated with the StatusReason failure. Not all StatusReasons may provide detailed causes. group string The group attribute of the resource associated with the status StatusReason. kind string The kind attribute of the resource associated with the status StatusReason. On some operations may differ from the requested resource Kind. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds name string The name attribute of the resource associated with the status StatusReason (when there is a single name which can be described). retryAfterSeconds int32 If specified, the time in seconds before the operation should be retried. Some errors may indicate the client must take an alternate action - for those errors this field may indicate how long to wait before taking the alternate action. uid string UID of the resource. (when there is a single resource which can be described). More info: http://kubernetes.io/docs/user-guide/identifiers#uids","title":"Fields"},{"location":"fields/#listmeta","text":"ListMeta describes metadata that synthetic resources must have, including lists and various status objects. A resource may have only one of {ObjectMeta, ListMeta}. Examples with this field (click to open) - [`archive-location.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/archive-location.yaml) - [`arguments-artifacts.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-artifacts.yaml) - [`arguments-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/arguments-parameters.yaml) - [`artifact-disable-archive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-disable-archive.yaml) - [`artifact-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-passing.yaml) - [`artifact-path-placeholders.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-path-placeholders.yaml) - [`artifact-repository-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifact-repository-ref.yaml) - [`artifactory-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/artifactory-artifact.yaml) - [`ci-output-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci-output-artifact.yaml) - [`ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/ci.yaml) - [`cluster-wftmpl-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/cluster-wftmpl-dag.yaml) - [`clustertemplates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/clustertemplates.yaml) - [`mixed-cluster-namespaced-wftmpl-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cluster-workflow-template/mixed-cluster-namespaced-wftmpl-steps.yaml) - [`coinflip-recursive.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip-recursive.yaml) - [`coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/coinflip.yaml) - [`colored-logs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/colored-logs.yaml) - [`conditionals.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/conditionals.yaml) - [`continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/continue-on-fail.yaml) - [`cron-backfill.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-backfill.yaml) - [`cron-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/cron-workflow.yaml) - [`custom-metrics.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/custom-metrics.yaml) - [`daemon-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-nginx.yaml) - [`daemon-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemon-step.yaml) - [`daemoned-stateful-set-with-service.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/daemoned-stateful-set-with-service.yaml) - [`dag-coinflip.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-coinflip.yaml) - [`dag-continue-on-fail.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-continue-on-fail.yaml) - [`dag-daemon-task.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-daemon-task.yaml) - [`dag-diamond-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond-steps.yaml) - [`dag-diamond.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-diamond.yaml) - [`dag-disable-failFast.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-disable-failFast.yaml) - [`dag-enhanced-depends.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-enhanced-depends.yaml) - [`dag-multiroot.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-multiroot.yaml) - [`dag-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-nested.yaml) - [`dag-targets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dag-targets.yaml) - [`default-pdb-support.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/default-pdb-support.yaml) - [`dns-config.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/dns-config.yaml) - [`exit-code-output-variable.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-code-output-variable.yaml) - [`exit-handlers.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/exit-handlers.yaml) - [`forever.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/forever.yaml) - [`fun-with-gifs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/fun-with-gifs.yaml) - [`gc-ttl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/gc-ttl.yaml) - [`global-outputs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-outputs.yaml) - [`global-parameters.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/global-parameters.yaml) - [`handle-large-output-results.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/handle-large-output-results.yaml) - [`hdfs-artifact.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hdfs-artifact.yaml) - [`hello-hybrid.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-hybrid.yaml) - [`hello-windows.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-windows.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/hello-world.yaml) - [`image-pull-secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/image-pull-secrets.yaml) - [`influxdb-ci.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/influxdb-ci.yaml) - [`init-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/init-container.yaml) - [`input-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-gcs.yaml) - [`input-artifact-git.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-git.yaml) - [`input-artifact-http.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-http.yaml) - [`input-artifact-oss.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-oss.yaml) - [`input-artifact-raw.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-raw.yaml) - [`input-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/input-artifact-s3.yaml) - [`k8s-jobs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-jobs.yaml) - [`k8s-orchestration.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-orchestration.yaml) - [`k8s-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-owner-reference.yaml) - [`k8s-set-owner-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-set-owner-reference.yaml) - [`k8s-wait-wf.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/k8s-wait-wf.yaml) - [`loops-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-dag.yaml) - [`loops-maps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-maps.yaml) - [`loops-param-argument.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-argument.yaml) - [`loops-param-result.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-param-result.yaml) - [`loops-sequence.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops-sequence.yaml) - [`loops.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/loops.yaml) - [`nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/nested-workflow.yaml) - [`node-selector.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/node-selector.yaml) - [`output-artifact-gcs.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-gcs.yaml) - [`output-artifact-s3.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-artifact-s3.yaml) - [`output-parameter.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/output-parameter.yaml) - [`parallelism-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-limit.yaml) - [`parallelism-nested-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-dag.yaml) - [`parallelism-nested-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested-workflow.yaml) - [`parallelism-nested.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-nested.yaml) - [`parallelism-template-limit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parallelism-template-limit.yaml) - [`parameter-aggregation-dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-dag.yaml) - [`parameter-aggregation-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation-script.yaml) - [`parameter-aggregation.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/parameter-aggregation.yaml) - [`pod-gc-strategy.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-gc-strategy.yaml) - [`pod-metadata.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-metadata.yaml) - [`pod-spec-from-previous-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-from-previous-step.yaml) - [`pod-spec-patch-wf-tmpl.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch-wf-tmpl.yaml) - [`pod-spec-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-patch.yaml) - [`pod-spec-yaml-patch.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/pod-spec-yaml-patch.yaml) - [`recursive-for-loop.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/recursive-for-loop.yaml) - [`resource-delete-with-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-delete-with-flags.yaml) - [`resource-flags.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resource-flags.yaml) - [`resubmit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/resubmit.yaml) - [`retry-backoff.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-backoff.yaml) - [`retry-container-to-completion.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container-to-completion.yaml) - [`retry-container.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-container.yaml) - [`retry-on-error.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-on-error.yaml) - [`retry-script.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-script.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/retry-with-steps.yaml) - [`scripts-bash.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-bash.yaml) - [`scripts-javascript.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-javascript.yaml) - [`scripts-python.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/scripts-python.yaml) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml) - [`sidecar-dind.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-dind.yaml) - [`sidecar-nginx.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar-nginx.yaml) - [`sidecar.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/sidecar.yaml) - [`status-reference.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/status-reference.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/steps.yaml) - [`suspend-template.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/suspend-template.yaml) - [`template-on-exit.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/template-on-exit.yaml) - [`testvolume.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/testvolume.yaml) - [`timeouts-step.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-step.yaml) - [`timeouts-workflow.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/timeouts-workflow.yaml) - [`volumes-emptydir.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-emptydir.yaml) - [`volumes-existing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-existing.yaml) - [`volumes-pvc.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/volumes-pvc.yaml) - [`work-avoidance.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/work-avoidance.yaml) - [`dag.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/dag.yaml) - [`hello-world.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/hello-world.yaml) - [`retry-with-steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/retry-with-steps.yaml) - [`steps.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/steps.yaml) - [`templates.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/templates.yaml) - [`workflow-template-ref-with-entrypoint-arg-passing.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref-with-entrypoint-arg-passing.yaml) - [`workflow-template-ref.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/workflow-template/workflow-template-ref.yaml)","title":"ListMeta"},{"location":"fields/#fields_141","text":"Field Name Field Type Description continue string continue may be set if the user set a limit on the number of items returned, and indicates that the server has more data available. The value is opaque and may be used to issue another request to the endpoint that served this list to retrieve the next set of available objects. Continuing a consistent list may not be possible if the server configuration has changed or more than a few minutes have passed. The resourceVersion field returned when using this continue value will be identical to the value in the first response, unless you have received this token from an error message. remainingItemCount int64 remainingItemCount is the number of subsequent items in the list which are not included in this list response. If the list request contained label or field selectors, then the number of remaining items is unknown and the field will be left unset and omitted during serialization. If the list is complete (either because it is not chunking or because this is the last chunk), then there are no more remaining items and this field will be left unset and omitted during serialization. Servers older than v1.15 do not set this field. The intended use of the remainingItemCount is estimating the size of a collection. Clients should not rely on the remainingItemCount to be set or to be exact.This field is alpha and can be changed or removed without notice. resourceVersion string String that identifies the server's internal version of this object that can be used by clients to determine when objects have changed. Value must be treated as opaque by clients and passed unmodified back to the server. Populated by the system. Read-only. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency selfLink string selfLink is a URL representing this object. Populated by the system. Read-only.","title":"Fields"},{"location":"fields/#nodeselectorterm","text":"A null or empty node selector term matches no objects. The requirements of them are ANDed. The TopologySelectorTerm type implements a subset of the NodeSelectorTerm.","title":"NodeSelectorTerm"},{"location":"fields/#fields_142","text":"Field Name Field Type Description matchExpressions Array< NodeSelectorRequirement > A list of node selector requirements by node's labels. matchFields Array< NodeSelectorRequirement > A list of node selector requirements by node's fields.","title":"Fields"},{"location":"fields/#configmapprojection","text":"Adapts a ConfigMap into a projected volume.The contents of the target ConfigMap's Data field will be presented in a projected volume as files using the keys in the Data field as the file names, unless the items element is populated with specific mappings of keys to paths. Note that this is identical to a configmap volume source without the default mode.","title":"ConfigMapProjection"},{"location":"fields/#fields_143","text":"Field Name Field Type Description items Array< KeyToPath > If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names optional boolean Specify whether the ConfigMap or its keys must be defined","title":"Fields"},{"location":"fields/#downwardapiprojection","text":"Represents downward API info for projecting into a projected volume. Note that this is identical to a downwardAPI volume source without the default mode.","title":"DownwardAPIProjection"},{"location":"fields/#fields_144","text":"Field Name Field Type Description items Array< DownwardAPIVolumeFile > Items is a list of DownwardAPIVolume file","title":"Fields"},{"location":"fields/#secretprojection","text":"Adapts a secret into a projected volume.The contents of the target Secret's Data field will be presented in a projected volume as files using the keys in the Data field as the file names. Note that this is identical to a secret volume source without the default mode. Examples with this field (click to open) - [`secrets.yaml`](https://github.com/argoproj/argo/blob/master/examples/examples/secrets.yaml)","title":"SecretProjection"},{"location":"fields/#fields_145","text":"Field Name Field Type Description items Array< KeyToPath > If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names optional boolean Specify whether the Secret or its key must be defined","title":"Fields"},{"location":"fields/#serviceaccounttokenprojection","text":"ServiceAccountTokenProjection represents a projected service account token volume. This projection can be used to insert a service account token into the pods runtime filesystem for use against APIs (Kubernetes API Server or otherwise).","title":"ServiceAccountTokenProjection"},{"location":"fields/#fields_146","text":"Field Name Field Type Description audience string Audience is the intended audience of the token. A recipient of a token must identify itself with an identifier specified in the audience of the token, and otherwise should reject the token. The audience defaults to the identifier of the apiserver. expirationSeconds int64 ExpirationSeconds is the requested duration of validity of the service account token. As the token approaches expiration, the kubelet volume plugin will proactively rotate the service account token. The kubelet will start trying to rotate the token if the token is older than 80 percent of its time to live or if the token is older than 24 hours.Defaults to 1 hour and must be at least 10 minutes. path string Path is the path relative to the mount point of the file to project the token into.","title":"Fields"},{"location":"fields/#statuscause","text":"StatusCause provides more information about an api.Status failure, including cases when multiple errors are encountered.","title":"StatusCause"},{"location":"fields/#fields_147","text":"Field Name Field Type Description field string The field of the resource that has caused this error, as named by its JSON serialization. May include dot and postfix notation for nested attributes. Arrays are zero-indexed. Fields may appear more than once in an array of causes due to fields having multiple errors. Optional.Examples: \"name\" - the field \"name\" on the current resource \"items[0].name\" - the field \"name\" on the first array entry in \"items\" message string A human-readable description of the cause of the error. This field may be presented as-is to a reader. reason string A machine-readable description of the cause of the error. If this value is empty there is no information available.","title":"Fields"},{"location":"fields/#nodeselectorrequirement","text":"A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values.","title":"NodeSelectorRequirement"},{"location":"fields/#fields_148","text":"Field Name Field Type Description key string The label key that the selector applies to. operator string Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt. values Array< string > An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the values array must have a single element, which will be interpreted as an integer. This array is replaced during a strategic merge patch.","title":"Fields"},{"location":"installation/","text":"Installation \u00b6 You can choose one of three common installations: cluster install Execute workflows in any namespace? namespace install Only execute workflows in the same namespace we install in (typically argo ). managed namespace install : Only execute workflows in a specific namespace ( learn more ). Choose a manifests from the list . E.g. kubectl create ns argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo/stable/manifests/namespace-install.yaml","title":"Installation"},{"location":"installation/#installation","text":"You can choose one of three common installations: cluster install Execute workflows in any namespace? namespace install Only execute workflows in the same namespace we install in (typically argo ). managed namespace install : Only execute workflows in a specific namespace ( learn more ). Choose a manifests from the list . E.g. kubectl create ns argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo/stable/manifests/namespace-install.yaml","title":"Installation"},{"location":"kubectl/","text":"Kubectl \u00b6 You can also create Workflows directly with kubectl . However, the Argo CLI offers extra features that kubectl does not, such as YAML validation, workflow visualization, parameter passing, retries and resubmits, suspend and resume, and more. kubectl create -n argo -f https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-world.yaml kubectl get wf -n argo kubectl get wf hello-world-xxx -n argo kubectl get po -n argo --selector = workflows.argoproj.io/workflow = hello-world-xxx kubectl logs hello-world-yyy -c main -n argo","title":"Kubectl"},{"location":"kubectl/#kubectl","text":"You can also create Workflows directly with kubectl . However, the Argo CLI offers extra features that kubectl does not, such as YAML validation, workflow visualization, parameter passing, retries and resubmits, suspend and resume, and more. kubectl create -n argo -f https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-world.yaml kubectl get wf -n argo kubectl get wf hello-world-xxx -n argo kubectl get po -n argo --selector = workflows.argoproj.io/workflow = hello-world-xxx kubectl logs hello-world-yyy -c main -n argo","title":"Kubectl"},{"location":"links/","text":"Links \u00b6 v2.7 and after You can configure Argo Server to show deep-links to your facilities (e.g. logging facility) in the user interface. See workflow-controller-configmap.yaml","title":"Links"},{"location":"links/#links","text":"v2.7 and after You can configure Argo Server to show deep-links to your facilities (e.g. logging facility) in the user interface. See workflow-controller-configmap.yaml","title":"Links"},{"location":"managed-namespace/","text":"Managed Namespace \u00b6 v2.5 and after You can install Argo in either cluster scoped or namespace scope configurations. This dictates if you must set-up cluster roles or normal roles. In namespace scope configuration, you must run both the Workflow Controller and Argo Server using --namespaced . If you would like to have the workflows running in a separated namespace, add --managed-namespace as well. For example: - args : - --configmap - workflow - controller - configmap - --executor-image - argoproj / workflow - controller : v2 . 5 . 1 - --namespaced - --managed-namespace - default","title":"Managed Namespace"},{"location":"managed-namespace/#managed-namespace","text":"v2.5 and after You can install Argo in either cluster scoped or namespace scope configurations. This dictates if you must set-up cluster roles or normal roles. In namespace scope configuration, you must run both the Workflow Controller and Argo Server using --namespaced . If you would like to have the workflows running in a separated namespace, add --managed-namespace as well. For example: - args : - --configmap - workflow - controller - configmap - --executor-image - argoproj / workflow - controller : v2 . 5 . 1 - --namespaced - --managed-namespace - default","title":"Managed Namespace"},{"location":"metrics/","text":"Prometheus Metrics \u00b6 v2.7 and after Introduction \u00b6 Custom prometheus metrics can be defined to be emitted on a Workflow - and Template -level basis. These can be useful for many cases; some examples: Keeping track of the duration of a Workflow or Template over time, and setting an alert if it goes beyond a threshold Keeping track of the number of times a Workflow or Template fails over time Reporting an important internal metric, such as a model training score or an internal error rate Emitting custom metrics with Argo is easy, but it's important to understand what makes a good Prometheus metric and the best way to define metrics in Argo to avoid problems such as cardinality explosion . Metrics and metrics in Argo \u00b6 Emitting metrics is the responsibility of the emitter owner. Since the user defines Workflows in Argo, the user is responsible for emitting metrics correctly. What is and isn't a Prometheus metric \u00b6 Prometheus metrics should be thought of as ephemeral data points of running processes; i.e., they are the answer to the question \"What is the state of my system right now ?\". Metrics should report things such as: a counter of the number of times a workflow or steps has failed, or a gauge of workflow duration, or an average of an internal metric such as a model training score or error rate. Metrics are then routinely scraped and stored and -- when they are correctly designed -- they can represent time series. Aggregating the examples above over time could answer useful questions such as: How has the error rate of this workflow or step changed over time? How has the duration of this workflow changed over time? Is the current workflow running for too long? Is our model improving over time? Prometheus metrics should not be thought of as a store of data. Since metrics should only report the state of the system at the current time, they should not be used to report historical data such as: the status of an individual instance of a workflow, or how long a particular instance of a step took to run. Metrics are also ephemeral, meaning there is no guarantee that they will be persisted for any amount of time. If you need a way to view and analyze historical data, consider the workflow archive or reporting to logs. Metric types \u00b6 Please see the Prometheus docs on metric types . How metrics work in Argo \u00b6 In order to analyze the behavior of a workflow over time, we need to be able to link different instances (i.e. individual executions) of a workflow together into a \"series\" for the purposes of emitting metrics. We do so by linking them together with the same metric descriptor. In prometheus, a metric descriptor is defined as a metric's name and its key-value labels. For example, for a metric tracking the duration of model execution over time, a metric descriptor could be: argo_workflows_model_exec_time{model_name=\"model_a\",phase=\"validation\"} This metric then represents the amount of time that \"Model A\" took to train in the phase \"Validation\". It is important to understand that the metric name and its labels form the descriptor: argo_workflows_model_exec_time{model_name=\"model_b\",phase=\"validation\"} is a different metric (and will track a different \"series\" altogether). Now, whenever we run our first workflow that validates \"Model A\" a metric with the amount of time it took it to do so will be created and emitted. For each subsequent time that this happens, no new metrics will be emitted and the same metric will be updated with the new value. Since, in effect, we are interested on the execution time of \"validation\" of \"Model A\" over time, we are no longer interested in the previous metric and can assume it has already been scraped. In summary, whenever you want to track a particular metric over time, you should use the same metric name and metric labels wherever it is emitted. This is how these metrics are \"linked\" as belonging to the same series. Defining metrics \u00b6 Metrics are defined in-place on the Workflow/Step/Task where they are emitted from. Metrics are always processed after the Workflow/Step/Task completes, with the exception of realtime metrics . Metric definitions must include a name and a help doc string. They can also include any number of labels (when defining labels avoid cardinality explosion). All metrics can also be conditionally emitted by defining a when clause. This when clause works the same as elsewhere in a workflow. A metric must also have a type, it can be one of gauge , histogram , and counter ( see below ). Within the metric type a value must be specified. This value can be either a literal value of be an Argo variable . When defining a histogram , buckets must also be provided (see below). Argo variables can be included anywhere in the metric spec, such as in labels , name , help , when , etc. Metric names can only contain alphanumeric characters, _ , and : . Metric Spec \u00b6 In Argo you can define a metric on the Workflow level or on the Template level. Here is an example of a Workflow level Gauge metric that will report the Workflow duration time: ```yaml apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: model-training- spec: entrypoint: steps metrics: prometheus: - name: exec_duration_gauge # Metric name (will be prepended with \"argo_workflows_\") labels: # Labels are optional. Avoid cardinality explosion. - key: name value: model_a help: \"Duration gauge by name\" # A help doc describing your metric. This is required. gauge: # The metric type. Available are \"gauge\", \"histogram\", and \"counter\". value: \"{{workflow.duration}}\" # The value of your metric. It could be an Argo variable (see variables doc) or a literal value ... An example of a `Template` - level Counter metric that will increase a counter every time the step fails : `` ` yaml ... templates : - name : flakey metrics : prometheus : - name : result_counter help : \"Count of step execution by result status\" labels : - key : name value : flakey when : \"{{status}} == Failed\" # Emit the metric conditionally. Works the same as normal \"when\" counter : value : \"1\" # This increments the counter by 1 container : image : python : alpine3 . 6 command : [ \"python\" , - c ] # fail with a 66% probability args : [ \"import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)\" ] ... A similar example of such a Counter metric that will increase for every step status ... templates : - name : flakey metrics : prometheus : - name : result_counter help : \"Count of step execution by result status\" labels : - key : name value : flakey - key : status value : \"{{status}}\" # Argo variable in `labels` counter : value : \"1\" container : image : python:alpine3.6 command : [ \"python\" , -c ] # fail with a 66% probability args : [ \"import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)\" ] Finally, an example of a Template -level Histogram metric that tracks an internal value: ... templates : - name : random-int metrics : prometheus : - name : random_int_step_histogram help : \"Value of the int emitted by random-int at step level\" when : \"{{status}} == Succeeded\" # Only emit metric when step succeeds histogram : buckets : # Bins must be defined for histogram metrics - 2.01 # and are part of the metric descriptor. - 4.01 # All metrics in this series MUST have the - 6.01 # same buckets. - 8.01 - 10.01 value : \"{{outputs.parameters.rand-int-value}}\" # References itself for its output (see variables doc) outputs : parameters : - name : rand-int-value globalName : rand-int-value valueFrom : path : /tmp/rand_int.txt container : image : alpine:latest command : [ sh , -c ] args : [ \"RAND_INT=$((1 + RANDOM % 10)); echo $RAND_INT; echo $RAND_INT > /tmp/rand_int.txt\" ] ... Realtime metrics \u00b6 Argo supports a limited number of real-time metrics. These metrics are emitted in realtime, beginning when the step execution starts and ending when it completes. Realtime metrics are only available on Gauge type metrics and with a limited number of variables . To define a realtime metric simply add realtime: true to a gauge metric with a valid realtime variable. For example: gauge : realtime : true value : \"{{duration}}\"","title":"Prometheus Metrics"},{"location":"metrics/#prometheus-metrics","text":"v2.7 and after","title":"Prometheus Metrics"},{"location":"metrics/#introduction","text":"Custom prometheus metrics can be defined to be emitted on a Workflow - and Template -level basis. These can be useful for many cases; some examples: Keeping track of the duration of a Workflow or Template over time, and setting an alert if it goes beyond a threshold Keeping track of the number of times a Workflow or Template fails over time Reporting an important internal metric, such as a model training score or an internal error rate Emitting custom metrics with Argo is easy, but it's important to understand what makes a good Prometheus metric and the best way to define metrics in Argo to avoid problems such as cardinality explosion .","title":"Introduction"},{"location":"metrics/#metrics-and-metrics-in-argo","text":"Emitting metrics is the responsibility of the emitter owner. Since the user defines Workflows in Argo, the user is responsible for emitting metrics correctly.","title":"Metrics and metrics in Argo"},{"location":"metrics/#what-is-and-isnt-a-prometheus-metric","text":"Prometheus metrics should be thought of as ephemeral data points of running processes; i.e., they are the answer to the question \"What is the state of my system right now ?\". Metrics should report things such as: a counter of the number of times a workflow or steps has failed, or a gauge of workflow duration, or an average of an internal metric such as a model training score or error rate. Metrics are then routinely scraped and stored and -- when they are correctly designed -- they can represent time series. Aggregating the examples above over time could answer useful questions such as: How has the error rate of this workflow or step changed over time? How has the duration of this workflow changed over time? Is the current workflow running for too long? Is our model improving over time? Prometheus metrics should not be thought of as a store of data. Since metrics should only report the state of the system at the current time, they should not be used to report historical data such as: the status of an individual instance of a workflow, or how long a particular instance of a step took to run. Metrics are also ephemeral, meaning there is no guarantee that they will be persisted for any amount of time. If you need a way to view and analyze historical data, consider the workflow archive or reporting to logs.","title":"What is and isn't a Prometheus metric"},{"location":"metrics/#metric-types","text":"Please see the Prometheus docs on metric types .","title":"Metric types"},{"location":"metrics/#how-metrics-work-in-argo","text":"In order to analyze the behavior of a workflow over time, we need to be able to link different instances (i.e. individual executions) of a workflow together into a \"series\" for the purposes of emitting metrics. We do so by linking them together with the same metric descriptor. In prometheus, a metric descriptor is defined as a metric's name and its key-value labels. For example, for a metric tracking the duration of model execution over time, a metric descriptor could be: argo_workflows_model_exec_time{model_name=\"model_a\",phase=\"validation\"} This metric then represents the amount of time that \"Model A\" took to train in the phase \"Validation\". It is important to understand that the metric name and its labels form the descriptor: argo_workflows_model_exec_time{model_name=\"model_b\",phase=\"validation\"} is a different metric (and will track a different \"series\" altogether). Now, whenever we run our first workflow that validates \"Model A\" a metric with the amount of time it took it to do so will be created and emitted. For each subsequent time that this happens, no new metrics will be emitted and the same metric will be updated with the new value. Since, in effect, we are interested on the execution time of \"validation\" of \"Model A\" over time, we are no longer interested in the previous metric and can assume it has already been scraped. In summary, whenever you want to track a particular metric over time, you should use the same metric name and metric labels wherever it is emitted. This is how these metrics are \"linked\" as belonging to the same series.","title":"How metrics work in Argo"},{"location":"metrics/#defining-metrics","text":"Metrics are defined in-place on the Workflow/Step/Task where they are emitted from. Metrics are always processed after the Workflow/Step/Task completes, with the exception of realtime metrics . Metric definitions must include a name and a help doc string. They can also include any number of labels (when defining labels avoid cardinality explosion). All metrics can also be conditionally emitted by defining a when clause. This when clause works the same as elsewhere in a workflow. A metric must also have a type, it can be one of gauge , histogram , and counter ( see below ). Within the metric type a value must be specified. This value can be either a literal value of be an Argo variable . When defining a histogram , buckets must also be provided (see below). Argo variables can be included anywhere in the metric spec, such as in labels , name , help , when , etc. Metric names can only contain alphanumeric characters, _ , and : .","title":"Defining metrics"},{"location":"metrics/#metric-spec","text":"In Argo you can define a metric on the Workflow level or on the Template level. Here is an example of a Workflow level Gauge metric that will report the Workflow duration time: ```yaml apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: model-training- spec: entrypoint: steps metrics: prometheus: - name: exec_duration_gauge # Metric name (will be prepended with \"argo_workflows_\") labels: # Labels are optional. Avoid cardinality explosion. - key: name value: model_a help: \"Duration gauge by name\" # A help doc describing your metric. This is required. gauge: # The metric type. Available are \"gauge\", \"histogram\", and \"counter\". value: \"{{workflow.duration}}\" # The value of your metric. It could be an Argo variable (see variables doc) or a literal value ... An example of a `Template` - level Counter metric that will increase a counter every time the step fails : `` ` yaml ... templates : - name : flakey metrics : prometheus : - name : result_counter help : \"Count of step execution by result status\" labels : - key : name value : flakey when : \"{{status}} == Failed\" # Emit the metric conditionally. Works the same as normal \"when\" counter : value : \"1\" # This increments the counter by 1 container : image : python : alpine3 . 6 command : [ \"python\" , - c ] # fail with a 66% probability args : [ \"import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)\" ] ... A similar example of such a Counter metric that will increase for every step status ... templates : - name : flakey metrics : prometheus : - name : result_counter help : \"Count of step execution by result status\" labels : - key : name value : flakey - key : status value : \"{{status}}\" # Argo variable in `labels` counter : value : \"1\" container : image : python:alpine3.6 command : [ \"python\" , -c ] # fail with a 66% probability args : [ \"import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)\" ] Finally, an example of a Template -level Histogram metric that tracks an internal value: ... templates : - name : random-int metrics : prometheus : - name : random_int_step_histogram help : \"Value of the int emitted by random-int at step level\" when : \"{{status}} == Succeeded\" # Only emit metric when step succeeds histogram : buckets : # Bins must be defined for histogram metrics - 2.01 # and are part of the metric descriptor. - 4.01 # All metrics in this series MUST have the - 6.01 # same buckets. - 8.01 - 10.01 value : \"{{outputs.parameters.rand-int-value}}\" # References itself for its output (see variables doc) outputs : parameters : - name : rand-int-value globalName : rand-int-value valueFrom : path : /tmp/rand_int.txt container : image : alpine:latest command : [ sh , -c ] args : [ \"RAND_INT=$((1 + RANDOM % 10)); echo $RAND_INT; echo $RAND_INT > /tmp/rand_int.txt\" ] ...","title":"Metric Spec"},{"location":"metrics/#realtime-metrics","text":"Argo supports a limited number of real-time metrics. These metrics are emitted in realtime, beginning when the step execution starts and ending when it completes. Realtime metrics are only available on Gauge type metrics and with a limited number of variables . To define a realtime metric simply add realtime: true to a gauge metric with a valid realtime variable. For example: gauge : realtime : true value : \"{{duration}}\"","title":"Realtime metrics"},{"location":"node-field-selector/","text":"Node Field Selectors \u00b6 v2.8 and after Introduction \u00b6 The resume, stop and retry Argo CLI and API commands support a --node-field-selector parameter to allow the user to select a subset of nodes for the command to apply to. In the case of the resume and stop commands these are the nodes that should be resumed or stopped. In the case of the retry command it allows specifying nodes that should be restarted even if they were previously successful (and must be used in combination with --restart-successful ) The format of this when used with the CLI is: --node-field-selector=FIELD=VALUE Possible options \u00b6 The field can be any of: Field Description displayName Display name of the node templateName Template name of the node phase Phase status of the node - eg Running templateRef.name The name of the WorkflowTemplate the node is referring to templateRef.template The template within the WorkflowTemplate the node is referring to inputs.parameters. .value The value of input parameter NAME The operator can be '=' or '!='. Multiple selectors can be combined with a comma, in which case they are ANDed together. Examples \u00b6 To filter for nodes where the input parameter 'foo' is equal to 'bar': --node-field-selector=inputs.parameters.foo.value=bar To filter for nodes where the input parameter 'foo' is equal to 'bar' and phase is not running: --node-field-selector=foo1=bar1,phase!=Running","title":"Node Field Selectors"},{"location":"node-field-selector/#node-field-selectors","text":"v2.8 and after","title":"Node Field Selectors"},{"location":"node-field-selector/#introduction","text":"The resume, stop and retry Argo CLI and API commands support a --node-field-selector parameter to allow the user to select a subset of nodes for the command to apply to. In the case of the resume and stop commands these are the nodes that should be resumed or stopped. In the case of the retry command it allows specifying nodes that should be restarted even if they were previously successful (and must be used in combination with --restart-successful ) The format of this when used with the CLI is: --node-field-selector=FIELD=VALUE","title":"Introduction"},{"location":"node-field-selector/#possible-options","text":"The field can be any of: Field Description displayName Display name of the node templateName Template name of the node phase Phase status of the node - eg Running templateRef.name The name of the WorkflowTemplate the node is referring to templateRef.template The template within the WorkflowTemplate the node is referring to inputs.parameters. .value The value of input parameter NAME The operator can be '=' or '!='. Multiple selectors can be combined with a comma, in which case they are ANDed together.","title":"Possible options"},{"location":"node-field-selector/#examples","text":"To filter for nodes where the input parameter 'foo' is equal to 'bar': --node-field-selector=inputs.parameters.foo.value=bar To filter for nodes where the input parameter 'foo' is equal to 'bar' and phase is not running: --node-field-selector=foo1=bar1,phase!=Running","title":"Examples"},{"location":"offloading-large-workflows/","text":"Offloading Large Workflows \u00b6 v2.4 and after Argo stores workflows as Kubernetes resources (i.e. within EtcD). This creates a limit to their size as resources must be under 1MB. Each resource includes the status of each node, which is stored in the /status/nodes field for the resource. This can be over 1MB. If this happens, we try and compress the node status and store it in /status/compressedNodes . If the status is still too large, we then try and store it in an SQL database. To enable this feature, configure a Postgres or MySQL database under persistence in your configuration and set nodeStatusOffLoad: true . FAQ \u00b6 Why aren't my workflows appearing in the database? \u00b6 Offloading is expensive and often unneccessary, so we only offload when we need to. Your workflows aren't probably large enough. Error \"Failed to submit workflow: etcdserver: request is too large.\" \u00b6 You must use the Argo CLI having exported export ARGO_SERVER=... .","title":"Offloading Large Workflows"},{"location":"offloading-large-workflows/#offloading-large-workflows","text":"v2.4 and after Argo stores workflows as Kubernetes resources (i.e. within EtcD). This creates a limit to their size as resources must be under 1MB. Each resource includes the status of each node, which is stored in the /status/nodes field for the resource. This can be over 1MB. If this happens, we try and compress the node status and store it in /status/compressedNodes . If the status is still too large, we then try and store it in an SQL database. To enable this feature, configure a Postgres or MySQL database under persistence in your configuration and set nodeStatusOffLoad: true .","title":"Offloading Large Workflows"},{"location":"offloading-large-workflows/#faq","text":"","title":"FAQ"},{"location":"offloading-large-workflows/#why-arent-my-workflows-appearing-in-the-database","text":"Offloading is expensive and often unneccessary, so we only offload when we need to. Your workflows aren't probably large enough.","title":"Why aren't my workflows appearing in the database?"},{"location":"offloading-large-workflows/#error-failed-to-submit-workflow-etcdserver-request-is-too-large","text":"You must use the Argo CLI having exported export ARGO_SERVER=... .","title":"Error \"Failed to submit workflow: etcdserver: request is too large.\""},{"location":"public-api/","text":"Public API \u00b6 Argo Workflows public API is defined by the following: The file api/openapi-spec/swagger.json The schema of the table argo_archived_workflows . The installation options listed in manifests/README.md . See: Versioning","title":"Public API"},{"location":"public-api/#public-api","text":"Argo Workflows public API is defined by the following: The file api/openapi-spec/swagger.json The schema of the table argo_archived_workflows . The installation options listed in manifests/README.md . See: Versioning","title":"Public API"},{"location":"quick-start/","text":"Quick Start \u00b6 To see how Argo works, you can install it and run examples of simple workflows and workflows that use artifacts. Firstly, you'll need a Kubernetes cluster and kubectl set-up Install Argo Workflows \u00b6 To get started quickly, you can use the quick start manifest which will install Argo Workflow as well as some commonly used components: kubectl create ns argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo/stable/manifests/quick-start-postgres.yaml Note On GKE, you may need to grant your account the ability to create new clusterrole s kubectl create clusterrolebinding YOURNAME-cluster-admin-binding --clusterrole = cluster-admin --user = YOUREMAIL@gmail.com If you are running Argo Workflows locally (e.g. using Minikube or Docker for Desktop), the open a port forward so you can access the namespace: kubectl -n argo port-forward deployment/argo-server 2746 :2746 This well server the user interface on https://localhost:2746 If you're using running Argo Workflows on a remote cluster (e.g. on EKS or GKE) then follow these instructions . Next, Download the latest Argo CLI from our releases page . Finally, submit an example workflow: argo submit -n argo --watch https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-world.yaml argo list -n argo argo get -n argo @latest argo logs -n argo @latest","title":"Quick Start"},{"location":"quick-start/#quick-start","text":"To see how Argo works, you can install it and run examples of simple workflows and workflows that use artifacts. Firstly, you'll need a Kubernetes cluster and kubectl set-up","title":"Quick Start"},{"location":"quick-start/#install-argo-workflows","text":"To get started quickly, you can use the quick start manifest which will install Argo Workflow as well as some commonly used components: kubectl create ns argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo/stable/manifests/quick-start-postgres.yaml Note On GKE, you may need to grant your account the ability to create new clusterrole s kubectl create clusterrolebinding YOURNAME-cluster-admin-binding --clusterrole = cluster-admin --user = YOUREMAIL@gmail.com If you are running Argo Workflows locally (e.g. using Minikube or Docker for Desktop), the open a port forward so you can access the namespace: kubectl -n argo port-forward deployment/argo-server 2746 :2746 This well server the user interface on https://localhost:2746 If you're using running Argo Workflows on a remote cluster (e.g. on EKS or GKE) then follow these instructions . Next, Download the latest Argo CLI from our releases page . Finally, submit an example workflow: argo submit -n argo --watch https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-world.yaml argo list -n argo argo get -n argo @latest argo logs -n argo @latest","title":"Install Argo Workflows"},{"location":"releasing/","text":"Release Instructions \u00b6 Allow 1h to do a release. Preparation \u00b6 Cherry-pick your changes from master onto the release branch. The release branch should be green in CI before you start. Release \u00b6 To generate new manifests and perform basic checks: make prepare - release VERSION = v2 . 7 . 2 Publish the images and local Git changes (disabling K3D as this is faster and more reliable for releases): make publish - release K3D = false VERSION = v2 . 7 . 2 [ ] Check the images were pushed successfully. docker pull argoproj / workflow - controller : v2 . 7 . 2 docker pull argoproj / argoexec : v2 . 7 . 2 docker pull argoproj / argocli : v2 . 7 . 2 [ ] Check the correct versions are printed: docker run argoproj / workflow - controller : v2 . 7 . 2 version docker run argoproj / argoexec : v2 . 7 . 2 version docker run argoproj / argocli : v2 . 7 . 2 version [ ] Check the manifests contain the correct tags: https://raw.githubusercontent.com/argoproj/argo/v2.7.2/manifests/install.yaml [ ] Check the manifests apply: kubectl -n argo apply -f https://raw.githubusercontent.com/argoproj/argo/v2.7.2/manifests/install.yaml Release Notes \u00b6 Create the release in Github. You can get some text for this using Github Toolkit : ght relnote v2 . 7 . 1 .. v2 . 7 . 2 Release notes checklist: [ ] All breaking changes are listed with migration steps [ ] The release notes identify every publicly known vulnerability with a CVE assignment Update Stable Tag \u00b6 If this is GA: [ ] Update the stable tag git tag - f stable git push - f origin stable [ ] Check the manifests contain the correct tags: https://raw.githubusercontent.com/argoproj/argo/stable/manifests/install.yaml Update Homebrew \u00b6 If this is GA: [ ] Update the Homebrew tap . [ ] Check Homebrew was successfully updated: brew upgrade argoproj/tap/argo argo version","title":"Release Instructions"},{"location":"releasing/#release-instructions","text":"Allow 1h to do a release.","title":"Release Instructions"},{"location":"releasing/#preparation","text":"Cherry-pick your changes from master onto the release branch. The release branch should be green in CI before you start.","title":"Preparation"},{"location":"releasing/#release","text":"To generate new manifests and perform basic checks: make prepare - release VERSION = v2 . 7 . 2 Publish the images and local Git changes (disabling K3D as this is faster and more reliable for releases): make publish - release K3D = false VERSION = v2 . 7 . 2 [ ] Check the images were pushed successfully. docker pull argoproj / workflow - controller : v2 . 7 . 2 docker pull argoproj / argoexec : v2 . 7 . 2 docker pull argoproj / argocli : v2 . 7 . 2 [ ] Check the correct versions are printed: docker run argoproj / workflow - controller : v2 . 7 . 2 version docker run argoproj / argoexec : v2 . 7 . 2 version docker run argoproj / argocli : v2 . 7 . 2 version [ ] Check the manifests contain the correct tags: https://raw.githubusercontent.com/argoproj/argo/v2.7.2/manifests/install.yaml [ ] Check the manifests apply: kubectl -n argo apply -f https://raw.githubusercontent.com/argoproj/argo/v2.7.2/manifests/install.yaml","title":"Release"},{"location":"releasing/#release-notes","text":"Create the release in Github. You can get some text for this using Github Toolkit : ght relnote v2 . 7 . 1 .. v2 . 7 . 2 Release notes checklist: [ ] All breaking changes are listed with migration steps [ ] The release notes identify every publicly known vulnerability with a CVE assignment","title":"Release Notes"},{"location":"releasing/#update-stable-tag","text":"If this is GA: [ ] Update the stable tag git tag - f stable git push - f origin stable [ ] Check the manifests contain the correct tags: https://raw.githubusercontent.com/argoproj/argo/stable/manifests/install.yaml","title":"Update Stable Tag"},{"location":"releasing/#update-homebrew","text":"If this is GA: [ ] Update the Homebrew tap . [ ] Check Homebrew was successfully updated: brew upgrade argoproj/tap/argo argo version","title":"Update Homebrew"},{"location":"resource-duration/","text":"Resource Duration \u00b6 v2.7 and after Argo Workflows provides an indication of how much resource your workflow has used and saves this information. This is intended to be an indicative but not accurate value. Calculation \u00b6 The calculation is always an estimate, and is calculated by duration.go based on container duration, specified pod resource requests, limits, or (for memory and CPU) defaults. Each indicator is divided by a common denominator depending on resource type. Base Amounts \u00b6 Each resource type has a denominator used to make large values smaller. CPU: 1 Memory: 100Mi Storage: 10Gi Ephemeral Storage: 10Gi All others: 1 The requested fraction of the base amount will be multiplied by the container's run time to get the container's Resource Duration. For example, if you've requested 100Mi of memory (one tenth of the base amount), and the container runs 120sec, then the reported Resource Duration will be 12sec * (1Gi memory) . Request Defaults \u00b6 If requests are not set for a container, Kubernetes defaults to limits . If limits are not set, Argo falls back to 100m for CPU and 100Mi for memory. Note: these are Argo's defaults, not Kubernetes' defaults. For the most meaningful results, set requests and/or limits for all containers. Example \u00b6 A pod that runs for 3min, with a CPU limit of 2000m , no memory request and an nvidia.com/gpu resource limit of 1 : CPU : 3 min * 2000 m / 1000 m = 6 min * ( 1 cpu ) Memory : 3 min * 100 Mi / 1 Gi = 18 sec * ( 100 Mi memory ) GPU : 3 min * 1 / 1 = 2min * (1 nvidia.com/g pu ) Web/CLI reporting \u00b6 Both the web and CLI give abbreviated usage, like 9m10s*cpu,6s*memory,2m31s*nvidia.com/gpu . In this context, resources like memory refer to the \"base amounts\". For example, memory means \"amount of time a resource requested 1Gi of memory.\" If a container only uses 100Mi, each second it runs will only count as a tenth-second of memory . Rounding Down \u00b6 For short running pods (<10s), the memory value may be 0s. This is because the default is 100Mi , but the denominator is 1Gi .","title":"Resource Duration"},{"location":"resource-duration/#resource-duration","text":"v2.7 and after Argo Workflows provides an indication of how much resource your workflow has used and saves this information. This is intended to be an indicative but not accurate value.","title":"Resource Duration"},{"location":"resource-duration/#calculation","text":"The calculation is always an estimate, and is calculated by duration.go based on container duration, specified pod resource requests, limits, or (for memory and CPU) defaults. Each indicator is divided by a common denominator depending on resource type.","title":"Calculation"},{"location":"resource-duration/#base-amounts","text":"Each resource type has a denominator used to make large values smaller. CPU: 1 Memory: 100Mi Storage: 10Gi Ephemeral Storage: 10Gi All others: 1 The requested fraction of the base amount will be multiplied by the container's run time to get the container's Resource Duration. For example, if you've requested 100Mi of memory (one tenth of the base amount), and the container runs 120sec, then the reported Resource Duration will be 12sec * (1Gi memory) .","title":"Base Amounts"},{"location":"resource-duration/#request-defaults","text":"If requests are not set for a container, Kubernetes defaults to limits . If limits are not set, Argo falls back to 100m for CPU and 100Mi for memory. Note: these are Argo's defaults, not Kubernetes' defaults. For the most meaningful results, set requests and/or limits for all containers.","title":"Request Defaults"},{"location":"resource-duration/#example","text":"A pod that runs for 3min, with a CPU limit of 2000m , no memory request and an nvidia.com/gpu resource limit of 1 : CPU : 3 min * 2000 m / 1000 m = 6 min * ( 1 cpu ) Memory : 3 min * 100 Mi / 1 Gi = 18 sec * ( 100 Mi memory ) GPU : 3 min * 1 / 1 = 2min * (1 nvidia.com/g pu )","title":"Example"},{"location":"resource-duration/#webcli-reporting","text":"Both the web and CLI give abbreviated usage, like 9m10s*cpu,6s*memory,2m31s*nvidia.com/gpu . In this context, resources like memory refer to the \"base amounts\". For example, memory means \"amount of time a resource requested 1Gi of memory.\" If a container only uses 100Mi, each second it runs will only count as a tenth-second of memory .","title":"Web/CLI reporting"},{"location":"resource-duration/#rounding-down","text":"For short running pods (<10s), the memory value may be 0s. This is because the default is 100Mi , but the denominator is 1Gi .","title":"Rounding Down"},{"location":"rest-api/","text":"REST API \u00b6 Argo Server API \u00b6 v2.5 and after Argo Workflows ships with a server that provide more features and security than before. The server can be configured with or without client auth ( server --auth-mode client ). When it is disabled, then clients must pass their Kubeconfig base 64 encoded in the HTTP Authorization header: ARGO_TOKEN = $ ( argo auth token ) curl - H \"Authorization: $ARGO_TOKEN\" http : // localhost : 2746 / api / v1 / workflows / argo Learn more on how to generate an access token . API reference docs : Latest docs (maybe incorrect) Interactively in the Argo Server UI .","title":"REST API"},{"location":"rest-api/#rest-api","text":"","title":"REST API"},{"location":"rest-api/#argo-server-api","text":"v2.5 and after Argo Workflows ships with a server that provide more features and security than before. The server can be configured with or without client auth ( server --auth-mode client ). When it is disabled, then clients must pass their Kubeconfig base 64 encoded in the HTTP Authorization header: ARGO_TOKEN = $ ( argo auth token ) curl - H \"Authorization: $ARGO_TOKEN\" http : // localhost : 2746 / api / v1 / workflows / argo Learn more on how to generate an access token . API reference docs : Latest docs (maybe incorrect) Interactively in the Argo Server UI .","title":"Argo Server API"},{"location":"rest-examples/","text":"API Examples \u00b6 Document contains couple of examples of workflow JSON's to submit via argo-server REST API. v2.5 and after Assuming the namespace of argo-server is argo authentication is turned off (otherwise provide Authentication header) argo-server is available on localhost:2746 Submitting workflow \u00b6 curl --request POST \\ --url http://localhost:2746/api/v1/workflows/argo \\ --header 'content-type: application/json' \\ --data '{ \"namespace\" : \"argo\" , \"serverDryRun\" : false , \"workflow\" : { \"metadata\" : { \"generateName\" : \"hello-world-\" , \"namespace\" : \"argo\" , \"labels\" : { \"workflows.argoproj.io/completed\" : \"false\" } } , \"spec\" : { \"templates\" : [ { \"name\" : \"whalesay\" , \"arguments\" : {} , \"inputs\" : {} , \"outputs\" : {} , \"metadata\" : {} , \"container\" : { \"name\" : \"\" , \"image\" : \"docker/whalesay:latest\" , \"command\" : [ \"cowsay\" ], \"args\" : [ \"hello world\" ], \"resources\" : {} } } ], \"entrypoint\" : \"whalesay\" , \"arguments\" : {} } } }' Getting workflows for namespace argo \u00b6 curl --request GET \\ --url http://localhost:2746/api/v1/workflows/argo Getting single workflow for namespace argo \u00b6 curl --request GET \\ --url http://localhost:2746/api/v1/workflows/argo/abc-dthgt Deleting single workflow for namespace argo \u00b6 curl --request DELETE \\ --url http://localhost:2746/api/v1/workflows/argo/abc-dthgt","title":"API Examples"},{"location":"rest-examples/#api-examples","text":"Document contains couple of examples of workflow JSON's to submit via argo-server REST API. v2.5 and after Assuming the namespace of argo-server is argo authentication is turned off (otherwise provide Authentication header) argo-server is available on localhost:2746","title":"API Examples"},{"location":"rest-examples/#submitting-workflow","text":"curl --request POST \\ --url http://localhost:2746/api/v1/workflows/argo \\ --header 'content-type: application/json' \\ --data '{ \"namespace\" : \"argo\" , \"serverDryRun\" : false , \"workflow\" : { \"metadata\" : { \"generateName\" : \"hello-world-\" , \"namespace\" : \"argo\" , \"labels\" : { \"workflows.argoproj.io/completed\" : \"false\" } } , \"spec\" : { \"templates\" : [ { \"name\" : \"whalesay\" , \"arguments\" : {} , \"inputs\" : {} , \"outputs\" : {} , \"metadata\" : {} , \"container\" : { \"name\" : \"\" , \"image\" : \"docker/whalesay:latest\" , \"command\" : [ \"cowsay\" ], \"args\" : [ \"hello world\" ], \"resources\" : {} } } ], \"entrypoint\" : \"whalesay\" , \"arguments\" : {} } } }'","title":"Submitting workflow"},{"location":"rest-examples/#getting-workflows-for-namespace-argo","text":"curl --request GET \\ --url http://localhost:2746/api/v1/workflows/argo","title":"Getting workflows for namespace argo"},{"location":"rest-examples/#getting-single-workflow-for-namespace-argo","text":"curl --request GET \\ --url http://localhost:2746/api/v1/workflows/argo/abc-dthgt","title":"Getting single workflow for namespace argo"},{"location":"rest-examples/#deleting-single-workflow-for-namespace-argo","text":"curl --request DELETE \\ --url http://localhost:2746/api/v1/workflows/argo/abc-dthgt","title":"Deleting single workflow for namespace argo"},{"location":"resuming-workflow-via-automation/","text":"Resume The Template \u00b6 For automation, we want just the name of the workflow, we can use labels to get just this our suspended workflow: WF = $( argo list -l workflows.argoproj.io/workflow-template = wait --running -o name ) WF = $( curl $ARGO_SERVER /api/v1/workflows/argo?listOptions.labelSelector = workflows.argoproj.io/workflow-template = wait, \\! workflows.argoproj.io/completed \\ -fs \\ -H \"Authorization: $ARGO_TOKEN \" | jq -r '.items[0].metadata.name' ) You can resume the workflow via the CLI or API too. If you have more than one node waiting, you must target it using a node field selector . argo resume $WF --node-field-selector displayName = a curl $ARGO_SERVER /api/v1/workflows/argo/ $WF /resume \\ -fs \\ -X 'PUT' \\ -H \"Authorization: $ARGO_TOKEN \" \\ -d '{\"nodeFieldSelector\": \"displayName=a\"}' Now the workflow will have resumed and completed. See also: access token resuming a workflow via automation submitting a workflow via automation one workflow submitting another async pattern","title":"Resume The Template"},{"location":"resuming-workflow-via-automation/#resume-the-template","text":"For automation, we want just the name of the workflow, we can use labels to get just this our suspended workflow: WF = $( argo list -l workflows.argoproj.io/workflow-template = wait --running -o name ) WF = $( curl $ARGO_SERVER /api/v1/workflows/argo?listOptions.labelSelector = workflows.argoproj.io/workflow-template = wait, \\! workflows.argoproj.io/completed \\ -fs \\ -H \"Authorization: $ARGO_TOKEN \" | jq -r '.items[0].metadata.name' ) You can resume the workflow via the CLI or API too. If you have more than one node waiting, you must target it using a node field selector . argo resume $WF --node-field-selector displayName = a curl $ARGO_SERVER /api/v1/workflows/argo/ $WF /resume \\ -fs \\ -X 'PUT' \\ -H \"Authorization: $ARGO_TOKEN \" \\ -d '{\"nodeFieldSelector\": \"displayName=a\"}' Now the workflow will have resumed and completed. See also: access token resuming a workflow via automation submitting a workflow via automation one workflow submitting another async pattern","title":"Resume The Template"},{"location":"running-locally/","text":"Running Locally \u00b6 Pre-requisites \u00b6 Go (The project currently uses version 1.13) Yarn Docker Kustomize protoc brew install protobuf jq A local Kubernetes cluster We recommend using K3D to set up the local Kubernetes cluster since this will allow you to test RBAC set-up and is fast. You can set-up K3D to be part of your default kube config as follows: cp ~/ . kube / config ~/ . kube / config . bak cat $ ( k3d get - kubeconfig --name='k3s-default') >> ~/.kube/config Alternatively, you can use Minikube to set up the local Kubernetes cluster. Once a local Kubernetes cluster has started via minikube start , your kube config will use Minikube's context automatically. Add to /etc/hosts: 127 . 0 . 0 . 1 dex 127 . 0 . 0 . 1 minio 127 . 0 . 0 . 1 postgres 127 . 0 . 0 . 1 mysql To install into the \u201cargo\u201d namespace of your cluster: Argo, MinIO (for saving artifacts and logs) and Postgres (for offloading or archiving): make start If you prefer MySQL: make start DB = mysql You\u2019ll now have Argo on https://localhost:2746 MinIO http://localhost:9000 (use admin/password) Either: Postgres on http://localhost:5432, run make postgres-cli to access. MySQL on http://localhost:3306, run make mysql-cli to access. You need the token to access the CLI or UI: eval $ ( make env ) . / dist / argo auth token At this point you\u2019ll have everything you need to use the CLI and UI. User Interface \u00b6 Tip: If you want to make UI changes without a time-consuming build: cd ui yarn install yarn start The UI will start up on http://localhost:8080. Debugging \u00b6 If you want to run controller or argo-server in your IDE (e.g. so you can debug it): Start with only components you don't want to debug; make start COMPONENTS = controller Or make start COMPONENTS = argo - server To find the command arguments you need to use, you\u2019ll have to look at the start target in the Makefile .` Running Sonar Locally \u00b6 This can only be done if you have already created a pull request. Install the scanner: brew install sonar - scanner Run the tests: make test CI = true make test - reports / test - report . out Perform a scan: # the key is PR number ( e . g . \"2666\" ), the branch is the CI branch , e . g . \"pull/2666\" SONAR_TOKEN = ... sonar - scanner - Dsonar . pullrequest . key = ... - Dsonar . pullrequest . branch = ... Clean \u00b6 To clean-up everything: make clean kubectl delete ns argo docker system prune - af","title":"Running Locally"},{"location":"running-locally/#running-locally","text":"","title":"Running Locally"},{"location":"running-locally/#pre-requisites","text":"Go (The project currently uses version 1.13) Yarn Docker Kustomize protoc brew install protobuf jq A local Kubernetes cluster We recommend using K3D to set up the local Kubernetes cluster since this will allow you to test RBAC set-up and is fast. You can set-up K3D to be part of your default kube config as follows: cp ~/ . kube / config ~/ . kube / config . bak cat $ ( k3d get - kubeconfig --name='k3s-default') >> ~/.kube/config Alternatively, you can use Minikube to set up the local Kubernetes cluster. Once a local Kubernetes cluster has started via minikube start , your kube config will use Minikube's context automatically. Add to /etc/hosts: 127 . 0 . 0 . 1 dex 127 . 0 . 0 . 1 minio 127 . 0 . 0 . 1 postgres 127 . 0 . 0 . 1 mysql To install into the \u201cargo\u201d namespace of your cluster: Argo, MinIO (for saving artifacts and logs) and Postgres (for offloading or archiving): make start If you prefer MySQL: make start DB = mysql You\u2019ll now have Argo on https://localhost:2746 MinIO http://localhost:9000 (use admin/password) Either: Postgres on http://localhost:5432, run make postgres-cli to access. MySQL on http://localhost:3306, run make mysql-cli to access. You need the token to access the CLI or UI: eval $ ( make env ) . / dist / argo auth token At this point you\u2019ll have everything you need to use the CLI and UI.","title":"Pre-requisites"},{"location":"running-locally/#user-interface","text":"Tip: If you want to make UI changes without a time-consuming build: cd ui yarn install yarn start The UI will start up on http://localhost:8080.","title":"User Interface"},{"location":"running-locally/#debugging","text":"If you want to run controller or argo-server in your IDE (e.g. so you can debug it): Start with only components you don't want to debug; make start COMPONENTS = controller Or make start COMPONENTS = argo - server To find the command arguments you need to use, you\u2019ll have to look at the start target in the Makefile .`","title":"Debugging"},{"location":"running-locally/#running-sonar-locally","text":"This can only be done if you have already created a pull request. Install the scanner: brew install sonar - scanner Run the tests: make test CI = true make test - reports / test - report . out Perform a scan: # the key is PR number ( e . g . \"2666\" ), the branch is the CI branch , e . g . \"pull/2666\" SONAR_TOKEN = ... sonar - scanner - Dsonar . pullrequest . key = ... - Dsonar . pullrequest . branch = ...","title":"Running Sonar Locally"},{"location":"running-locally/#clean","text":"To clean-up everything: make clean kubectl delete ns argo docker system prune - af","title":"Clean"},{"location":"scaling/","text":"Scaling \u00b6 For running large workflows, you'll typically need to scale the controller to match. Horizontally Scaling \u00b6 You cannot horizontally scale the controller. Vertically Scaling \u00b6 You can scale the controller vertically: If you have workflows with many steps, increase --pod-workers . If you have many workflows, increase --workflow-workers . Increase both --qps and --burst . You will need to increase the controller's memory and CPU. Sharding \u00b6 One Install Per Namespace \u00b6 Rather than running a single installation in your cluster, run one per namespace using the --namespaced flag. Instance ID \u00b6 Within a cluster can use instance ID to run N Argo instances within a cluster. Create one namespace for each Argo, e.g. argo-i1 , argo-i2 :. Edit workflow-controller-configmap.yaml for each namespace to set an instance ID. apiVersion : v1 kind : ConfigMap metadata : name : workflow - controller - configmap data : instanceID : i1 v2.9 and after You'll may need to pass the instance ID to the CLI: argo --instance-id i1 submit my-wf.yaml You do not need to have one instance ID per namespace, you could have many or few.","title":"Scaling"},{"location":"scaling/#scaling","text":"For running large workflows, you'll typically need to scale the controller to match.","title":"Scaling"},{"location":"scaling/#horizontally-scaling","text":"You cannot horizontally scale the controller.","title":"Horizontally Scaling"},{"location":"scaling/#vertically-scaling","text":"You can scale the controller vertically: If you have workflows with many steps, increase --pod-workers . If you have many workflows, increase --workflow-workers . Increase both --qps and --burst . You will need to increase the controller's memory and CPU.","title":"Vertically Scaling"},{"location":"scaling/#sharding","text":"","title":"Sharding"},{"location":"scaling/#one-install-per-namespace","text":"Rather than running a single installation in your cluster, run one per namespace using the --namespaced flag.","title":"One Install Per Namespace"},{"location":"scaling/#instance-id","text":"Within a cluster can use instance ID to run N Argo instances within a cluster. Create one namespace for each Argo, e.g. argo-i1 , argo-i2 :. Edit workflow-controller-configmap.yaml for each namespace to set an instance ID. apiVersion : v1 kind : ConfigMap metadata : name : workflow - controller - configmap data : instanceID : i1 v2.9 and after You'll may need to pass the instance ID to the CLI: argo --instance-id i1 submit my-wf.yaml You do not need to have one instance ID per namespace, you could have many or few.","title":"Instance ID"},{"location":"security/","text":"Security \u00b6 Argo Server Security \u00b6 Argo Server implements security in three layers. Firstly, you should enable transport layer security to ensure your data cannot be read in transit. Secondly, you should enable an authentication mode to ensure that you do not run workflows from unknown users. Finally, you should configure the argo-server role and role binding with the correct permissions. Read-Only \u00b6 You can achieve this by configuring the argo-server role ( example with only read access (i.e. only get / list / watch verbs).","title":"Security"},{"location":"security/#security","text":"","title":"Security"},{"location":"security/#argo-server-security","text":"Argo Server implements security in three layers. Firstly, you should enable transport layer security to ensure your data cannot be read in transit. Secondly, you should enable an authentication mode to ensure that you do not run workflows from unknown users. Finally, you should configure the argo-server role and role binding with the correct permissions.","title":"Argo Server Security"},{"location":"security/#read-only","text":"You can achieve this by configuring the argo-server role ( example with only read access (i.e. only get / list / watch verbs).","title":"Read-Only"},{"location":"service-accounts/","text":"Service Accounts \u00b6 Configure the service account to run Workflows \u00b6 Roles, RoleBindings, and ServiceAccounts \u00b6 In order for Argo to support features such as artifacts, outputs, access to secrets, etc. it needs to communicate with Kubernetes resources using the Kubernetes API. To communicate with the Kubernetes API, Argo uses a ServiceAccount to authenticate itself to the Kubernetes API. You can specify which Role (i.e. which permissions) the ServiceAccount that Argo uses by binding a Role to a ServiceAccount using a RoleBinding Then, when submitting Workflows you can specify which ServiceAccount Argo uses using: argo submit --serviceaccount <name> When no ServiceAccount is provided, Argo will use the default ServiceAccount from the namespace from which it is run, which will almost always have insufficient privileges by default. For more information about granting Argo the necessary permissions for your use case see Workflow RBAC . Granting admin privileges \u00b6 For the purposes of this demo, we will grant the default ServiceAccount admin privileges (i.e., we will bind the admin Role to the default ServiceAccount of the current namespace): kubectl create rolebinding default-admin --clusterrole = admin --serviceaccount = argo:default -n argo Note that this will grant admin privileges to the default ServiceAccount in the namespace that the command is run from, so you will only be able to run Workflows in the namespace where the RoleBinding was made.","title":"Service Accounts"},{"location":"service-accounts/#service-accounts","text":"","title":"Service Accounts"},{"location":"service-accounts/#configure-the-service-account-to-run-workflows","text":"","title":"Configure the service account to run Workflows"},{"location":"service-accounts/#roles-rolebindings-and-serviceaccounts","text":"In order for Argo to support features such as artifacts, outputs, access to secrets, etc. it needs to communicate with Kubernetes resources using the Kubernetes API. To communicate with the Kubernetes API, Argo uses a ServiceAccount to authenticate itself to the Kubernetes API. You can specify which Role (i.e. which permissions) the ServiceAccount that Argo uses by binding a Role to a ServiceAccount using a RoleBinding Then, when submitting Workflows you can specify which ServiceAccount Argo uses using: argo submit --serviceaccount <name> When no ServiceAccount is provided, Argo will use the default ServiceAccount from the namespace from which it is run, which will almost always have insufficient privileges by default. For more information about granting Argo the necessary permissions for your use case see Workflow RBAC .","title":"Roles, RoleBindings, and ServiceAccounts"},{"location":"service-accounts/#granting-admin-privileges","text":"For the purposes of this demo, we will grant the default ServiceAccount admin privileges (i.e., we will bind the admin Role to the default ServiceAccount of the current namespace): kubectl create rolebinding default-admin --clusterrole = admin --serviceaccount = argo:default -n argo Note that this will grant admin privileges to the default ServiceAccount in the namespace that the command is run from, so you will only be able to run Workflows in the namespace where the RoleBinding was made.","title":"Granting admin privileges"},{"location":"static-code-analysis/","text":"Static Code Analysis \u00b6 We use the following static code analysis tools: golangci-lint and tslint for compile time linting snyk.io - for image scanning sonarcloud.io - for code scans and security alerts These are at least run daily or on each pull request.","title":"Static Code Analysis"},{"location":"static-code-analysis/#static-code-analysis","text":"We use the following static code analysis tools: golangci-lint and tslint for compile time linting snyk.io - for image scanning sonarcloud.io - for code scans and security alerts These are at least run daily or on each pull request.","title":"Static Code Analysis"},{"location":"submit-workflow-via-automation/","text":"Submitting A Workflow Via Automation \u00b6 v2.8 and after Firstly, to do any automation, you'll need an ( access token ). For this example, our role needs extra permissions: kubectl patch role jenkins -p '{\"rules\": [{\"apiGroups\": [\"argoproj.io\"], \"resources\": [\"workflowtemplates\"], \"verbs\": [\"get\"]}, {\"apiGroups\": [\"argoproj.io\"], \"resources\": [\"workflows\"], \"verbs\": [\"create\", \"list\", \"get\", \"update\"]}]}' Next, create a workflow template apiVersion : argoproj.io/v1alpha1 kind : WorkflowTemplate metadata : name : hello-argo spec : entrypoint : main templates : - name : main steps : - - name : a template : whalesay - name : whalesay container : image : docker/whalesay:latest You can submit this workflow via an CLI or the Argo Server API . Submit via CLI (note how I add a label to help identify it later on): argo submit --from wftmpl/hello-argo -l workflows.argoproj.io/workflow-template = hello-argo Or submit via API: curl $ARGO_SERVER /api/v1/workflows/argo/submit \\ -fs \\ -H \"Authorization: $ARGO_TOKEN \" \\ -d '{\"resourceKind\": \"WorkflowTemplate\", \"resourceName\": \"hello-argo\", \"submitOptions\": {\"labels\": \"workflows.argoproj.io/workflow-template=hello-argo\"}}' You'll see that the workflow has been created: argo list NAME STATUS AGE DURATION PRIORITY hello-argo-77m4l Running 33s 33s 0 See also: See also: access token resuming a workflow via automation one workflow submitting another async pattern","title":"Submitting A Workflow Via Automation"},{"location":"submit-workflow-via-automation/#submitting-a-workflow-via-automation","text":"v2.8 and after Firstly, to do any automation, you'll need an ( access token ). For this example, our role needs extra permissions: kubectl patch role jenkins -p '{\"rules\": [{\"apiGroups\": [\"argoproj.io\"], \"resources\": [\"workflowtemplates\"], \"verbs\": [\"get\"]}, {\"apiGroups\": [\"argoproj.io\"], \"resources\": [\"workflows\"], \"verbs\": [\"create\", \"list\", \"get\", \"update\"]}]}' Next, create a workflow template apiVersion : argoproj.io/v1alpha1 kind : WorkflowTemplate metadata : name : hello-argo spec : entrypoint : main templates : - name : main steps : - - name : a template : whalesay - name : whalesay container : image : docker/whalesay:latest You can submit this workflow via an CLI or the Argo Server API . Submit via CLI (note how I add a label to help identify it later on): argo submit --from wftmpl/hello-argo -l workflows.argoproj.io/workflow-template = hello-argo Or submit via API: curl $ARGO_SERVER /api/v1/workflows/argo/submit \\ -fs \\ -H \"Authorization: $ARGO_TOKEN \" \\ -d '{\"resourceKind\": \"WorkflowTemplate\", \"resourceName\": \"hello-argo\", \"submitOptions\": {\"labels\": \"workflows.argoproj.io/workflow-template=hello-argo\"}}' You'll see that the workflow has been created: argo list NAME STATUS AGE DURATION PRIORITY hello-argo-77m4l Running 33s 33s 0 See also: See also: access token resuming a workflow via automation one workflow submitting another async pattern","title":"Submitting A Workflow Via Automation"},{"location":"swagger/","text":"Argo \u00b6 Argo Version: latest \u00b6 Security \u00b6 BearerToken apiKey API Key Description Bearer Token authentication Name authorization In header HTTPBasic basic Basic Description HTTP Basic authentication /api/v1/archived-workflows \u00b6 GET \u00b6 Parameters \u00b6 Name Located in Description Required Schema listOptions.labelSelector query A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. No string listOptions.fieldSelector query A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. No string listOptions.watch query Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. No boolean (boolean) listOptions.allowWatchBookmarks query allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. If the feature gate WatchBookmarks is not enabled in apiserver, this field is ignored. +optional. No boolean (boolean) listOptions.resourceVersion query When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. +optional. No string listOptions.timeoutSeconds query Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. No string (int64) listOptions.limit query limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. No string (int64) listOptions.continue query The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\". This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. No string Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowList /api/v1/archived-workflows/{uid} \u00b6 GET \u00b6 Parameters \u00b6 Name Located in Description Required Schema uid path Yes string Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow DELETE \u00b6 Parameters \u00b6 Name Located in Description Required Schema uid path Yes string Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.ArchivedWorkflowDeletedResponse /api/v1/cluster-workflow-templates \u00b6 GET \u00b6 Parameters \u00b6 Name Located in Description Required Schema listOptions.labelSelector query A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. No string listOptions.fieldSelector query A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. No string listOptions.watch query Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. No boolean (boolean) listOptions.allowWatchBookmarks query allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. If the feature gate WatchBookmarks is not enabled in apiserver, this field is ignored. +optional. No boolean (boolean) listOptions.resourceVersion query When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. +optional. No string listOptions.timeoutSeconds query Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. No string (int64) listOptions.limit query limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. No string (int64) listOptions.continue query The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\". This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. No string Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateList POST \u00b6 Parameters \u00b6 Name Located in Description Required Schema body body Yes io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateCreateRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate /api/v1/cluster-workflow-templates/lint \u00b6 POST \u00b6 Parameters \u00b6 Name Located in Description Required Schema body body Yes io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateLintRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate /api/v1/cluster-workflow-templates/{name} \u00b6 GET \u00b6 Parameters \u00b6 Name Located in Description Required Schema name path Yes string getOptions.resourceVersion query When specified: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. No string Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate PUT \u00b6 Parameters \u00b6 Name Located in Description Required Schema name path DEPRECATED: This field is ignored. Yes string body body Yes io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateUpdateRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate DELETE \u00b6 Parameters \u00b6 Name Located in Description Required Schema name path Yes string deleteOptions.gracePeriodSeconds query The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. No string (int64) deleteOptions.preconditions.uid query Specifies the target UID. +optional. No string deleteOptions.preconditions.resourceVersion query Specifies the target ResourceVersion +optional. No string deleteOptions.orphanDependents query Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. No boolean (boolean) deleteOptions.propagationPolicy query Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. No string deleteOptions.dryRun query When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. No [ string ] Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateDeleteResponse /api/v1/cron-workflows/{namespace} \u00b6 GET \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string listOptions.labelSelector query A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. No string listOptions.fieldSelector query A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. No string listOptions.watch query Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. No boolean (boolean) listOptions.allowWatchBookmarks query allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. If the feature gate WatchBookmarks is not enabled in apiserver, this field is ignored. +optional. No boolean (boolean) listOptions.resourceVersion query When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. +optional. No string listOptions.timeoutSeconds query Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. No string (int64) listOptions.limit query limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. No string (int64) listOptions.continue query The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\". This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. No string Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.CronWorkflowList POST \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string body body Yes io.argoproj.workflow.v1alpha1.CreateCronWorkflowRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.CronWorkflow /api/v1/cron-workflows/{namespace}/lint \u00b6 POST \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string body body Yes io.argoproj.workflow.v1alpha1.LintCronWorkflowRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.CronWorkflow /api/v1/cron-workflows/{namespace}/{name} \u00b6 GET \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path Yes string getOptions.resourceVersion query When specified: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. No string Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.CronWorkflow PUT \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path DEPRECATED: This field is ignored. Yes string body body Yes io.argoproj.workflow.v1alpha1.UpdateCronWorkflowRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.CronWorkflow DELETE \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path Yes string deleteOptions.gracePeriodSeconds query The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. No string (int64) deleteOptions.preconditions.uid query Specifies the target UID. +optional. No string deleteOptions.preconditions.resourceVersion query Specifies the target ResourceVersion +optional. No string deleteOptions.orphanDependents query Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. No boolean (boolean) deleteOptions.propagationPolicy query Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. No string deleteOptions.dryRun query When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. No [ string ] Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.CronWorkflowDeletedResponse /api/v1/info \u00b6 GET \u00b6 Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.InfoResponse /api/v1/version \u00b6 GET \u00b6 Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Version /api/v1/workflow-events/{namespace} \u00b6 GET \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string listOptions.labelSelector query A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. No string listOptions.fieldSelector query A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. No string listOptions.watch query Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. No boolean (boolean) listOptions.allowWatchBookmarks query allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. If the feature gate WatchBookmarks is not enabled in apiserver, this field is ignored. +optional. No boolean (boolean) listOptions.resourceVersion query When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. +optional. No string listOptions.timeoutSeconds query Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. No string (int64) listOptions.limit query limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. No string (int64) listOptions.continue query The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\". This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. No string Responses \u00b6 Code Description Schema 200 A successful response.(streaming responses) object /api/v1/workflow-templates/{namespace} \u00b6 GET \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string listOptions.labelSelector query A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. No string listOptions.fieldSelector query A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. No string listOptions.watch query Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. No boolean (boolean) listOptions.allowWatchBookmarks query allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. If the feature gate WatchBookmarks is not enabled in apiserver, this field is ignored. +optional. No boolean (boolean) listOptions.resourceVersion query When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. +optional. No string listOptions.timeoutSeconds query Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. No string (int64) listOptions.limit query limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. No string (int64) listOptions.continue query The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\". This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. No string Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowTemplateList POST \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowTemplateCreateRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowTemplate /api/v1/workflow-templates/{namespace}/lint \u00b6 POST \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowTemplateLintRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowTemplate /api/v1/workflow-templates/{namespace}/{name} \u00b6 GET \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path Yes string getOptions.resourceVersion query When specified: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. No string Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowTemplate PUT \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path DEPRECATED: This field is ignored. Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowTemplateUpdateRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowTemplate DELETE \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path Yes string deleteOptions.gracePeriodSeconds query The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. No string (int64) deleteOptions.preconditions.uid query Specifies the target UID. +optional. No string deleteOptions.preconditions.resourceVersion query Specifies the target ResourceVersion +optional. No string deleteOptions.orphanDependents query Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. No boolean (boolean) deleteOptions.propagationPolicy query Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. No string deleteOptions.dryRun query When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. No [ string ] Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowTemplateDeleteResponse /api/v1/workflows/{namespace} \u00b6 GET \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string listOptions.labelSelector query A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. No string listOptions.fieldSelector query A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. No string listOptions.watch query Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. No boolean (boolean) listOptions.allowWatchBookmarks query allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. If the feature gate WatchBookmarks is not enabled in apiserver, this field is ignored. +optional. No boolean (boolean) listOptions.resourceVersion query When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. +optional. No string listOptions.timeoutSeconds query Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. No string (int64) listOptions.limit query limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. No string (int64) listOptions.continue query The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\". This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. No string fields query Fields to be included or excluded in the response. e.g. \"items.spec,items.status.phase\", \"-items.status.nodes\". No string Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowList POST \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowCreateRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow /api/v1/workflows/{namespace}/lint \u00b6 POST \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowLintRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow /api/v1/workflows/{namespace}/submit \u00b6 POST \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowSubmitRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow /api/v1/workflows/{namespace}/{name} \u00b6 GET \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path Yes string getOptions.resourceVersion query When specified: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. No string fields query Fields to be included or excluded in the response. e.g. \"spec,status.phase\", \"-status.nodes\". No string Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow DELETE \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path Yes string deleteOptions.gracePeriodSeconds query The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. No string (int64) deleteOptions.preconditions.uid query Specifies the target UID. +optional. No string deleteOptions.preconditions.resourceVersion query Specifies the target ResourceVersion +optional. No string deleteOptions.orphanDependents query Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. No boolean (boolean) deleteOptions.propagationPolicy query Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. No string deleteOptions.dryRun query When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. No [ string ] Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowDeleteResponse /api/v1/workflows/{namespace}/{name}/resubmit \u00b6 PUT \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowResubmitRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow /api/v1/workflows/{namespace}/{name}/resume \u00b6 PUT \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowResumeRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow /api/v1/workflows/{namespace}/{name}/retry \u00b6 PUT \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowRetryRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow /api/v1/workflows/{namespace}/{name}/stop \u00b6 PUT \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowStopRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow /api/v1/workflows/{namespace}/{name}/suspend \u00b6 PUT \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowSuspendRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow /api/v1/workflows/{namespace}/{name}/terminate \u00b6 PUT \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowTerminateRequest Responses \u00b6 Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow /api/v1/workflows/{namespace}/{name}/{podName}/log \u00b6 GET \u00b6 Parameters \u00b6 Name Located in Description Required Schema namespace path Yes string name path Yes string podName path Yes string logOptions.container query The container for which to stream logs. Defaults to only container if there is one container in the pod. +optional. No string logOptions.follow query Follow the log stream of the pod. Defaults to false. +optional. No boolean (boolean) logOptions.previous query Return previous terminated container logs. Defaults to false. +optional. No boolean (boolean) logOptions.sinceSeconds query A relative time in seconds before the current time from which to show logs. If this value precedes the time a pod was started, only logs since the pod start will be returned. If this value is in the future, no logs will be returned. Only one of sinceSeconds or sinceTime may be specified. +optional. No string (int64) logOptions.sinceTime.seconds query Represents seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to 9999-12-31T23:59:59Z inclusive. No string (int64) logOptions.sinceTime.nanos query Non-negative fractions of a second at nanosecond resolution. Negative second values with fractions must still have non-negative nanos values that count forward in time. Must be from 0 to 999,999,999 inclusive. This field may be limited in precision depending on context. No integer logOptions.timestamps query If true, add an RFC3339 or RFC3339Nano timestamp at the beginning of every line of log output. Defaults to false. +optional. No boolean (boolean) logOptions.tailLines query If set, the number of lines from the end of the logs to show. If not specified, logs are shown from the creation of the container or sinceSeconds or sinceTime +optional. No string (int64) logOptions.limitBytes query If set, the number of bytes to read from the server before terminating the log output. This may not display a complete final line of logging, and may return slightly more or slightly less than the specified limit. +optional. No string (int64) logOptions.insecureSkipTLSVerifyBackend query insecureSkipTLSVerifyBackend indicates that the apiserver should not confirm the validity of the serving certificate of the backend it is connecting to. This will make the HTTPS connection between the apiserver and the backend insecure. This means the apiserver cannot verify the log data it is receiving came from the real kubelet. If the kubelet is configured to verify the apiserver's TLS credentials, it does not mean the connection to the real kubelet is vulnerable to a man in the middle attack (e.g. an attacker could not intercept the actual log data coming from the real kubelet). +optional. No boolean (boolean) Responses \u00b6 Code Description Schema 200 A successful response.(streaming responses) object Models \u00b6 google.protobuf.Any \u00b6 Name Type Description Required type_url string No value byte No grpc.gateway.runtime.StreamError \u00b6 Name Type Description Required details [ google.protobuf.Any ] No grpc_code integer No http_code integer No http_status string No message string No io.argoproj.workflow.v1alpha1.Amount \u00b6 Amount represent a numeric amount. Name Type Description Required io.argoproj.workflow.v1alpha1.Amount number Amount represent a numeric amount. io.argoproj.workflow.v1alpha1.ArchiveStrategy \u00b6 ArchiveStrategy describes how to archive files/directory when saving artifacts Name Type Description Required none io.argoproj.workflow.v1alpha1.NoneStrategy No tar io.argoproj.workflow.v1alpha1.TarStrategy No io.argoproj.workflow.v1alpha1.ArchivedWorkflowDeletedResponse \u00b6 Name Type Description Required io.argoproj.workflow.v1alpha1.ArchivedWorkflowDeletedResponse object io.argoproj.workflow.v1alpha1.Arguments \u00b6 Arguments to a template Name Type Description Required artifacts [ io.argoproj.workflow.v1alpha1.Artifact ] Artifacts is the list of artifacts to pass to the template or workflow No parameters [ io.argoproj.workflow.v1alpha1.Parameter ] Parameters is the list of parameters to pass to the template or workflow No io.argoproj.workflow.v1alpha1.Artifact \u00b6 Artifact indicates an artifact to place at a specified path Name Type Description Required archive io.argoproj.workflow.v1alpha1.ArchiveStrategy Archive controls how the artifact will be saved to the artifact repository. No archiveLogs boolean ArchiveLogs indicates if the container logs should be archived No artifactory io.argoproj.workflow.v1alpha1.ArtifactoryArtifact Artifactory contains artifactory artifact location details No from string From allows an artifact to reference an artifact from a previous step No gcs io.argoproj.workflow.v1alpha1.GCSArtifact GCS contains GCS artifact location details No git io.argoproj.workflow.v1alpha1.GitArtifact Git contains git artifact location details No globalName string GlobalName exports an output artifact to the global scope, making it available as '{{io.argoproj.workflow.v1alpha1.outputs.artifacts.XXXX}} and in workflow.status.outputs.artifacts No hdfs io.argoproj.workflow.v1alpha1.HDFSArtifact HDFS contains HDFS artifact location details No http io.argoproj.workflow.v1alpha1.HTTPArtifact HTTP contains HTTP artifact location details No mode integer mode bits to use on this file, must be a value between 0 and 0777 set when loading input artifacts. No name string name of the artifact. must be unique within a template's inputs/outputs. Yes optional boolean Make Artifacts optional, if Artifacts doesn't generate or exist No oss io.argoproj.workflow.v1alpha1.OSSArtifact OSS contains OSS artifact location details No path string Path is the container path to the artifact No raw io.argoproj.workflow.v1alpha1.RawArtifact Raw contains raw artifact location details No s3 io.argoproj.workflow.v1alpha1.S3Artifact S3 contains S3 artifact location details No io.argoproj.workflow.v1alpha1.ArtifactLocation \u00b6 ArtifactLocation describes a location for a single or multiple artifacts. It is used as single artifact in the context of inputs/outputs (e.g. outputs.artifacts.artname). It is also used to describe the location of multiple artifacts such as the archive location of a single workflow step, which the executor will use as a default location to store its files. Name Type Description Required archiveLogs boolean ArchiveLogs indicates if the container logs should be archived No artifactory io.argoproj.workflow.v1alpha1.ArtifactoryArtifact Artifactory contains artifactory artifact location details No gcs io.argoproj.workflow.v1alpha1.GCSArtifact GCS contains GCS artifact location details No git io.argoproj.workflow.v1alpha1.GitArtifact Git contains git artifact location details No hdfs io.argoproj.workflow.v1alpha1.HDFSArtifact HDFS contains HDFS artifact location details No http io.argoproj.workflow.v1alpha1.HTTPArtifact HTTP contains HTTP artifact location details No oss io.argoproj.workflow.v1alpha1.OSSArtifact OSS contains OSS artifact location details No raw io.argoproj.workflow.v1alpha1.RawArtifact Raw contains raw artifact location details No s3 io.argoproj.workflow.v1alpha1.S3Artifact S3 contains S3 artifact location details No io.argoproj.workflow.v1alpha1.ArtifactRepositoryRef \u00b6 Name Type Description Required configMap string No key string No io.argoproj.workflow.v1alpha1.ArtifactoryArtifact \u00b6 ArtifactoryArtifact is the location of an artifactory artifact Name Type Description Required passwordSecret io.k8s.api.core.v1.SecretKeySelector PasswordSecret is the secret selector to the repository password No url string URL of the artifact Yes usernameSecret io.k8s.api.core.v1.SecretKeySelector UsernameSecret is the secret selector to the repository username No io.argoproj.workflow.v1alpha1.Backoff \u00b6 Backoff is a backoff strategy to use within retryStrategy Name Type Description Required duration string Duration is the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\") No factor integer Factor is a factor to multiply the base duration after each failed retry No maxDuration string MaxDuration is the maximum amount of time allowed for the backoff strategy No io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate \u00b6 ClusterWorkflowTemplate is the definition of a workflow template resource in cluster scope Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta Yes spec io.argoproj.workflow.v1alpha1.WorkflowTemplateSpec Yes io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateCreateRequest \u00b6 Name Type Description Required createOptions io.k8s.apimachinery.pkg.apis.meta.v1.CreateOptions No template io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate No io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateDeleteResponse \u00b6 Name Type Description Required io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateDeleteResponse object io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateLintRequest \u00b6 Name Type Description Required createOptions io.k8s.apimachinery.pkg.apis.meta.v1.CreateOptions No template io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate No io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateList \u00b6 ClusterWorkflowTemplateList is list of ClusterWorkflowTemplate resources Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No items [ io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate ] Yes kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta Yes io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateUpdateRequest \u00b6 Name Type Description Required name string DEPRECATED: This field is ignored. No template io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate No io.argoproj.workflow.v1alpha1.Condition \u00b6 Name Type Description Required message string Message is the condition message No status string Status is the status of the condition No type string Type is the type of condition No io.argoproj.workflow.v1alpha1.ContinueOn \u00b6 ContinueOn defines if a workflow should continue even if a task or step fails/errors. It can be specified if the workflow should continue when the pod errors, fails or both. Name Type Description Required error boolean No failed boolean No io.argoproj.workflow.v1alpha1.Counter \u00b6 Counter is a Counter prometheus metric Name Type Description Required value string Value is the value of the metric Yes io.argoproj.workflow.v1alpha1.CreateCronWorkflowRequest \u00b6 Name Type Description Required createOptions io.k8s.apimachinery.pkg.apis.meta.v1.CreateOptions No cronWorkflow io.argoproj.workflow.v1alpha1.CronWorkflow No namespace string No io.argoproj.workflow.v1alpha1.CronWorkflow \u00b6 CronWorkflow is the definition of a scheduled workflow resource Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta Yes spec io.argoproj.workflow.v1alpha1.CronWorkflowSpec Yes status io.argoproj.workflow.v1alpha1.CronWorkflowStatus No io.argoproj.workflow.v1alpha1.CronWorkflowDeletedResponse \u00b6 Name Type Description Required io.argoproj.workflow.v1alpha1.CronWorkflowDeletedResponse object io.argoproj.workflow.v1alpha1.CronWorkflowList \u00b6 CronWorkflowList is list of CronWorkflow resources Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No items [ io.argoproj.workflow.v1alpha1.CronWorkflow ] Yes kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta Yes io.argoproj.workflow.v1alpha1.CronWorkflowSpec \u00b6 CronWorkflowSpec is the specification of a CronWorkflow Name Type Description Required concurrencyPolicy string ConcurrencyPolicy is the K8s-style concurrency policy that will be used No failedJobsHistoryLimit integer FailedJobsHistoryLimit is the number of successful jobs to be kept at a time No schedule string Schedule is a schedule to run the Workflow in Cron format Yes startingDeadlineSeconds long StartingDeadlineSeconds is the K8s-style deadline that will limit the time a CronWorkflow will be run after its original scheduled time if it is missed. No successfulJobsHistoryLimit integer SuccessfulJobsHistoryLimit is the number of successful jobs to be kept at a time No suspend boolean Suspend is a flag that will stop new CronWorkflows from running if set to true No timezone string Timezone is the timezone against which the cron schedule will be calculated, e.g. \"Asia/Tokyo\". Default is machine's local time. No workflowMetadata io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta WorkflowMetadata contains some metadata of the workflow to be run No workflowSpec io.argoproj.workflow.v1alpha1.WorkflowSpec WorkflowSpec is the spec of the workflow to be run Yes io.argoproj.workflow.v1alpha1.CronWorkflowStatus \u00b6 CronWorkflowStatus is the status of a CronWorkflow Name Type Description Required active [ io.k8s.api.core.v1.ObjectReference ] Active is a list of active workflows stemming from this CronWorkflow No conditions [ io.argoproj.workflow.v1alpha1.Condition ] Conditions is a list of conditions the CronWorkflow may have No lastScheduledTime io.k8s.apimachinery.pkg.apis.meta.v1.Time LastScheduleTime is the last time the CronWorkflow was scheduled No io.argoproj.workflow.v1alpha1.DAGTask \u00b6 DAGTask represents a node in the graph during DAG execution Name Type Description Required arguments io.argoproj.workflow.v1alpha1.Arguments Arguments are the parameter and artifact arguments to the template No continueOn io.argoproj.workflow.v1alpha1.ContinueOn ContinueOn makes argo to proceed with the following step even if this step fails. Errors and Failed states can be specified No dependencies [ string ] Dependencies are name of other targets which this depends on No depends string Depends are name of other targets which this depends on No name string Name is the name of the target Yes onExit string OnExit is a template reference which is invoked at the end of the template, irrespective of the success, failure, or error of the primary template. No template string Name of template to execute Yes templateRef io.argoproj.workflow.v1alpha1.TemplateRef TemplateRef is the reference to the template resource to execute. No when string When is an expression in which the task should conditionally execute No withItems [ io.argoproj.workflow.v1alpha1.Item ] WithItems expands a task into multiple parallel tasks from the items in the list No withParam string WithParam expands a task into multiple parallel tasks from the value in the parameter, which is expected to be a JSON list. No withSequence io.argoproj.workflow.v1alpha1.Sequence WithSequence expands a task into a numeric sequence No io.argoproj.workflow.v1alpha1.DAGTemplate \u00b6 DAGTemplate is a template subtype for directed acyclic graph templates Name Type Description Required failFast boolean This flag is for DAG logic. The DAG logic has a built-in \"fail fast\" feature to stop scheduling new steps, as soon as it detects that one of the DAG nodes is failed. Then it waits until all DAG nodes are completed before failing the DAG itself. The FailFast flag default is true, if set to false, it will allow a DAG to run all branches of the DAG to completion (either success or failure), regardless of the failed outcomes of branches in the DAG. More info and example about this feature at https://github.com/argoproj/argo/issues/1442 No target string Target are one or more names of targets to execute in a DAG No tasks [ io.argoproj.workflow.v1alpha1.DAGTask ] Tasks are a list of DAG tasks Yes io.argoproj.workflow.v1alpha1.ExecutorConfig \u00b6 ExecutorConfig holds configurations of an executor container. Name Type Description Required serviceAccountName string ServiceAccountName specifies the service account name of the executor container. No io.argoproj.workflow.v1alpha1.GCSArtifact \u00b6 GCSArtifact is the location of a GCS artifact Name Type Description Required bucket string Bucket is the name of the bucket Yes key string Key is the path in the bucket where the artifact resides Yes serviceAccountKeySecret io.k8s.api.core.v1.SecretKeySelector ServiceAccountKeySecret is the secret selector to the bucket's service account key No io.argoproj.workflow.v1alpha1.Gauge \u00b6 Gauge is a Gauge prometheus metric Name Type Description Required realtime boolean Realtime emits this metric in real time if applicable Yes value string Value is the value of the metric Yes io.argoproj.workflow.v1alpha1.GitArtifact \u00b6 GitArtifact is the location of an git artifact Name Type Description Required depth long Depth specifies clones/fetches should be shallow and include the given number of commits from the branch tip No fetch [ string ] Fetch specifies a number of refs that should be fetched before checkout No insecureIgnoreHostKey boolean InsecureIgnoreHostKey disables SSH strict host key checking during git clone No passwordSecret io.k8s.api.core.v1.SecretKeySelector PasswordSecret is the secret selector to the repository password No repo string Repo is the git repository Yes revision string Revision is the git commit, tag, branch to checkout No sshPrivateKeySecret io.k8s.api.core.v1.SecretKeySelector SSHPrivateKeySecret is the secret selector to the repository ssh private key No usernameSecret io.k8s.api.core.v1.SecretKeySelector UsernameSecret is the secret selector to the repository username No io.argoproj.workflow.v1alpha1.HDFSArtifact \u00b6 HDFSArtifact is the location of an HDFS artifact Name Type Description Required addresses [ string ] Addresses is accessible addresses of HDFS name nodes Yes force boolean Force copies a file forcibly even if it exists (default: false) No hdfsUser string HDFSUser is the user to access HDFS file system. It is ignored if either ccache or keytab is used. No krbCCacheSecret io.k8s.api.core.v1.SecretKeySelector KrbCCacheSecret is the secret selector for Kerberos ccache Either ccache or keytab can be set to use Kerberos. No krbConfigConfigMap io.k8s.api.core.v1.ConfigMapKeySelector KrbConfig is the configmap selector for Kerberos config as string It must be set if either ccache or keytab is used. No krbKeytabSecret io.k8s.api.core.v1.SecretKeySelector KrbKeytabSecret is the secret selector for Kerberos keytab Either ccache or keytab can be set to use Kerberos. No krbRealm string KrbRealm is the Kerberos realm used with Kerberos keytab It must be set if keytab is used. No krbServicePrincipalName string KrbServicePrincipalName is the principal name of Kerberos service It must be set if either ccache or keytab is used. No krbUsername string KrbUsername is the Kerberos username used with Kerberos keytab It must be set if keytab is used. No path string Path is a file path in HDFS Yes io.argoproj.workflow.v1alpha1.HTTPArtifact \u00b6 HTTPArtifact allows an file served on HTTP to be placed as an input artifact in a container Name Type Description Required url string URL of the artifact Yes io.argoproj.workflow.v1alpha1.Histogram \u00b6 Histogram is a Histogram prometheus metric Name Type Description Required buckets [ io.argoproj.workflow.v1alpha1.Amount ] Buckets is a list of bucket divisors for the histogram Yes value string Value is the value of the metric Yes io.argoproj.workflow.v1alpha1.InfoResponse \u00b6 Name Type Description Required links [ io.argoproj.workflow.v1alpha1.Link ] No managedNamespace string No io.argoproj.workflow.v1alpha1.Inputs \u00b6 Inputs are the mechanism for passing parameters, artifacts, volumes from one template to another Name Type Description Required artifacts [ io.argoproj.workflow.v1alpha1.Artifact ] Artifact are a list of artifacts passed as inputs No parameters [ io.argoproj.workflow.v1alpha1.Parameter ] Parameters are a list of parameters passed as inputs No io.argoproj.workflow.v1alpha1.Item \u00b6 Item expands a single workflow step into multiple parallel steps The value of Item can be a map, string, bool, or number Name Type Description Required io.argoproj.workflow.v1alpha1.Item string,number,boolean,array,object Item expands a single workflow step into multiple parallel steps The value of Item can be a map, string, bool, or number io.argoproj.workflow.v1alpha1.Link \u00b6 A link to another app. Name Type Description Required name string The name of the link, E.g. \"Workflow Logs\" or \"Pod Logs\" Yes scope string Either \"workflow\" or \"pod\" Yes url string The URL. May contain \"${metadata.namespace}\" and \"${metadata.name}\". Yes io.argoproj.workflow.v1alpha1.LintCronWorkflowRequest \u00b6 Name Type Description Required cronWorkflow io.argoproj.workflow.v1alpha1.CronWorkflow No namespace string No io.argoproj.workflow.v1alpha1.LogEntry \u00b6 Name Type Description Required content string No podName string No io.argoproj.workflow.v1alpha1.Metadata \u00b6 Pod metdata Name Type Description Required annotations object No labels object No io.argoproj.workflow.v1alpha1.MetricLabel \u00b6 MetricLabel is a single label for a prometheus metric Name Type Description Required key string Yes value string Yes io.argoproj.workflow.v1alpha1.Metrics \u00b6 Metrics are a list of metrics emitted from a Workflow/Template Name Type Description Required prometheus [ io.argoproj.workflow.v1alpha1.Prometheus ] Prometheus is a list of prometheus metrics to be emitted Yes io.argoproj.workflow.v1alpha1.NodeStatus \u00b6 NodeStatus contains status information about an individual node in the workflow Name Type Description Required boundaryID string BoundaryID indicates the node ID of the associated template root node in which this node belongs to No children [ string ] Children is a list of child node IDs No daemoned boolean Daemoned tracks whether or not this node was daemoned and need to be terminated No displayName string DisplayName is a human readable representation of the node. Unique within a template boundary No finishedAt io.k8s.apimachinery.pkg.apis.meta.v1.Time Time at which this node completed No hostNodeName string HostNodeName name of the Kubernetes node on which the Pod is running, if applicable No id string ID is a unique identifier of a node within the worklow It is implemented as a hash of the node name, which makes the ID deterministic Yes inputs io.argoproj.workflow.v1alpha1.Inputs Inputs captures input parameter values and artifact locations supplied to this template invocation No message string A human readable message indicating details about why the node is in this condition. No name string Name is unique name in the node tree used to generate the node ID Yes outboundNodes [ string ] OutboundNodes tracks the node IDs which are considered \"outbound\" nodes to a template invocation. For every invocation of a template, there are nodes which we considered as \"outbound\". Essentially, these are last nodes in the execution sequence to run, before the template is considered completed. These nodes are then connected as parents to a following step. In the case of single pod steps (i.e. container, script, resource templates), this list will be nil since the pod itself is already considered the \"outbound\" node. In the case of DAGs, outbound nodes are the \"target\" tasks (tasks with no children). In the case of steps, outbound nodes are all the containers involved in the last step group. NOTE: since templates are composable, the list of outbound nodes are carried upwards when a DAG/steps template invokes another DAG/steps template. In other words, the outbound nodes of a template, will be a superset of the outbound nodes of its last children. No outputs io.argoproj.workflow.v1alpha1.Outputs Outputs captures output parameter values and artifact locations produced by this template invocation No phase string Phase a simple, high-level summary of where the node is in its lifecycle. Can be used as a state machine. No podIP string PodIP captures the IP of the pod for daemoned steps No resourcesDuration object ResourcesDuration is indicative, but not accurate, resource duration. This is populated when the nodes completes. No startedAt io.k8s.apimachinery.pkg.apis.meta.v1.Time Time at which this node started No storedTemplateID string StoredTemplateID is the ID of stored template. DEPRECATED: This value is not used anymore. No templateName string TemplateName is the template name which this node corresponds to. Not applicable to virtual nodes (e.g. Retry, StepGroup) No templateRef io.argoproj.workflow.v1alpha1.TemplateRef TemplateRef is the reference to the template resource which this node corresponds to. Not applicable to virtual nodes (e.g. Retry, StepGroup) No templateScope string TemplateScope is the template scope in which the template of this node was retrieved. No type string Type indicates type of node Yes workflowTemplateName string WorkflowTemplateName is the WorkflowTemplate resource name on which the resolved template of this node is retrieved. DEPRECATED: This value is not used anymore. No io.argoproj.workflow.v1alpha1.NoneStrategy \u00b6 NoneStrategy indicates to skip tar process and upload the files or directory tree as independent files. Note that if the artifact is a directory, the artifact driver must support the ability to save/load the directory appropriately. Name Type Description Required io.argoproj.workflow.v1alpha1.NoneStrategy object NoneStrategy indicates to skip tar process and upload the files or directory tree as independent files. Note that if the artifact is a directory, the artifact driver must support the ability to save/load the directory appropriately. io.argoproj.workflow.v1alpha1.OSSArtifact \u00b6 OSSArtifact is the location of an Alibaba Cloud OSS artifact Name Type Description Required accessKeySecret io.k8s.api.core.v1.SecretKeySelector AccessKeySecret is the secret selector to the bucket's access key Yes bucket string Bucket is the name of the bucket Yes endpoint string Endpoint is the hostname of the bucket endpoint Yes key string Key is the path in the bucket where the artifact resides Yes secretKeySecret io.k8s.api.core.v1.SecretKeySelector SecretKeySecret is the secret selector to the bucket's secret key Yes io.argoproj.workflow.v1alpha1.Outputs \u00b6 Outputs hold parameters, artifacts, and results from a step Name Type Description Required artifacts [ io.argoproj.workflow.v1alpha1.Artifact ] Artifacts holds the list of output artifacts produced by a step No exitCode string ExitCode holds the exit code of a script template No parameters [ io.argoproj.workflow.v1alpha1.Parameter ] Parameters holds the list of output parameters produced by a step No result string Result holds the result (stdout) of a script template No io.argoproj.workflow.v1alpha1.ParallelSteps \u00b6 Name Type Description Required io.argoproj.workflow.v1alpha1.ParallelSteps array io.argoproj.workflow.v1alpha1.Parameter \u00b6 Parameter indicate a passed string parameter to a service template with an optional default value Name Type Description Required default io.k8s.apimachinery.pkg.util.intstr.IntOrString Default is the default value to use for an input parameter if a value was not supplied No globalName string GlobalName exports an output parameter to the global scope, making it available as '{{io.argoproj.workflow.v1alpha1.outputs.parameters.XXXX}} and in workflow.status.outputs.parameters No name string Name is the parameter name Yes value io.k8s.apimachinery.pkg.util.intstr.IntOrString Value is the literal value to use for the parameter. If specified in the context of an input parameter, the value takes precedence over any passed values No valueFrom io.argoproj.workflow.v1alpha1.ValueFrom ValueFrom is the source for the output parameter's value No io.argoproj.workflow.v1alpha1.PodGC \u00b6 PodGC describes how to delete completed pods as they complete Name Type Description Required strategy string Strategy is the strategy to use. One of \"OnPodCompletion\", \"OnPodSuccess\", \"OnWorkflowCompletion\", \"OnWorkflowSuccess\" No io.argoproj.workflow.v1alpha1.Prometheus \u00b6 Prometheus is a prometheus metric to be emitted Name Type Description Required counter io.argoproj.workflow.v1alpha1.Counter Counter is a counter metric No gauge io.argoproj.workflow.v1alpha1.Gauge Gauge is a gauge metric No help string Help is a string that describes the metric Yes histogram io.argoproj.workflow.v1alpha1.Histogram Histogram is a histogram metric No labels [ io.argoproj.workflow.v1alpha1.MetricLabel ] Labels is a list of metric labels No name string Name is the name of the metric Yes when string When is a conditional statement that decides when to emit the metric No io.argoproj.workflow.v1alpha1.RawArtifact \u00b6 RawArtifact allows raw string content to be placed as an artifact in a container Name Type Description Required data string Data is the string contents of the artifact Yes io.argoproj.workflow.v1alpha1.ResourceTemplate \u00b6 ResourceTemplate is a template subtype to manipulate kubernetes resources Name Type Description Required action string Action is the action to perform to the resource. Must be one of: get, create, apply, delete, replace, patch Yes failureCondition string FailureCondition is a label selector expression which describes the conditions of the k8s resource in which the step was considered failed No flags [ string ] Flags is a set of additional options passed to kubectl before submitting a resource I.e. to disable resource validation: flags: [ \"--validate=false\" # disable resource validation ] No manifest string Manifest contains the kubernetes manifest No mergeStrategy string MergeStrategy is the strategy used to merge a patch. It defaults to \"strategic\" Must be one of: strategic, merge, json No setOwnerReference boolean SetOwnerReference sets the reference to the workflow on the OwnerReference of generated resource. No successCondition string SuccessCondition is a label selector expression which describes the conditions of the k8s resource in which it is acceptable to proceed to the following step No io.argoproj.workflow.v1alpha1.RetryStrategy \u00b6 RetryStrategy provides controls on how to retry a workflow step Name Type Description Required backoff io.argoproj.workflow.v1alpha1.Backoff Backoff is a backoff strategy No limit integer Limit is the maximum number of attempts when retrying a container No retryPolicy string RetryPolicy is a policy of NodePhase statuses that will be retried No io.argoproj.workflow.v1alpha1.S3Artifact \u00b6 S3Artifact is the location of an S3 artifact Name Type Description Required accessKeySecret io.k8s.api.core.v1.SecretKeySelector AccessKeySecret is the secret selector to the bucket's access key Yes bucket string Bucket is the name of the bucket Yes endpoint string Endpoint is the hostname of the bucket endpoint Yes insecure boolean Insecure will connect to the service with TLS No key string Key is the key in the bucket where the artifact resides Yes region string Region contains the optional bucket region No roleARN string RoleARN is the Amazon Resource Name (ARN) of the role to assume. No secretKeySecret io.k8s.api.core.v1.SecretKeySelector SecretKeySecret is the secret selector to the bucket's secret key Yes useSDKCreds boolean UseSDKCreds tells the driver to figure out credentials based on sdk defaults. No io.argoproj.workflow.v1alpha1.ScriptTemplate \u00b6 ScriptTemplate is a template subtype to enable scripting through code steps Name Type Description Required args [ string ] Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell No command [ string ] Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell No env [ io.k8s.api.core.v1.EnvVar ] List of environment variables to set in the container. Cannot be updated. No envFrom [ io.k8s.api.core.v1.EnvFromSource ] List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. No image string Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets. Yes imagePullPolicy string Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images No lifecycle io.k8s.api.core.v1.Lifecycle Actions that the management system should take in response to container lifecycle events. Cannot be updated. No livenessProbe io.k8s.api.core.v1.Probe Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No name string Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated. No ports [ io.k8s.api.core.v1.ContainerPort ] List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \"0.0.0.0\" address inside a container will be accessible from the network. Cannot be updated. No readinessProbe io.k8s.api.core.v1.Probe Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No resources io.k8s.api.core.v1.ResourceRequirements Compute Resources required by this container. Cannot be updated. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ No securityContext io.k8s.api.core.v1.SecurityContext Security options the pod should run with. More info: https://kubernetes.io/docs/concepts/policy/security-context/ More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ No source string Source contains the source code of the script to execute Yes startupProbe io.k8s.api.core.v1.Probe StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state operation. This cannot be updated. This is an alpha feature enabled by the StartupProbe feature flag. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No stdin boolean Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false. No stdinOnce boolean Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false No terminationMessagePath string Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated. No terminationMessagePolicy string Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated. No tty boolean Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false. No volumeDevices [ io.k8s.api.core.v1.VolumeDevice ] volumeDevices is the list of block devices to be used by the container. This is a beta feature. No volumeMounts [ io.k8s.api.core.v1.VolumeMount ] Pod volumes to mount into the container's filesystem. Cannot be updated. No workingDir string Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated. No io.argoproj.workflow.v1alpha1.Sequence \u00b6 Sequence expands a workflow step into numeric range Name Type Description Required count string Count is number of elements in the sequence (default: 0). Not to be used with end No end string Number at which to end the sequence (default: 0). Not to be used with Count No format string Format is a printf format string to format the value in the sequence No start string Number at which to start the sequence (default: 0) No io.argoproj.workflow.v1alpha1.SubmitOpts \u00b6 SubmitOpts are workflow submission options Name Type Description Required dryRun boolean DryRun validates the workflow on the client-side without creating it. This option is not supported in API No entryPoint string Entrypoint overrides spec.entrypoint No generateName string GenerateName overrides metadata.generateName No labels string Labels adds to metadata.labels No name string Name overrides metadata.name No ownerReference io.k8s.apimachinery.pkg.apis.meta.v1.OwnerReference OwnerReference creates a metadata.ownerReference No parameterFile string ParameterFile holds a reference to a parameter file. This option is not supported in API No parameters [ string ] Parameters passes input parameters to workflow No serverDryRun boolean ServerDryRun validates the workflow on the server-side without creating it No serviceAccount string ServiceAccount runs all pods in the workflow using specified ServiceAccount. No io.argoproj.workflow.v1alpha1.SuspendTemplate \u00b6 SuspendTemplate is a template subtype to suspend a workflow at a predetermined point in time Name Type Description Required duration string Duration is the seconds to wait before automatically resuming a template No io.argoproj.workflow.v1alpha1.TTLStrategy \u00b6 TTLStrategy is the strategy for the time to live depending on if the workflow succeeded or failed Name Type Description Required secondsAfterCompletion integer SecondsAfterCompletion is the number of seconds to live after completion No secondsAfterFailure integer SecondsAfterFailure is the number of seconds to live after failure No secondsAfterSuccess integer SecondsAfterSuccess is the number of seconds to live after success No io.argoproj.workflow.v1alpha1.TarStrategy \u00b6 TarStrategy will tar and gzip the file or directory when saving Name Type Description Required compressionLevel integer CompressionLevel specifies the gzip compression level to use for the artifact. Defaults to gzip.DefaultCompression. No io.argoproj.workflow.v1alpha1.Template \u00b6 Template is a reusable and composable unit of execution in a workflow Name Type Description Required activeDeadlineSeconds long Optional duration in seconds relative to the StartTime that the pod may be active on a node before the system actively tries to terminate the pod; value must be positive integer This field is only applicable to container and script templates. No affinity io.k8s.api.core.v1.Affinity Affinity sets the pod's scheduling constraints Overrides the affinity set at the workflow level (if any) No archiveLocation io.argoproj.workflow.v1alpha1.ArtifactLocation Location in which all files related to the step will be stored (logs, artifacts, etc...). Can be overridden by individual items in Outputs. If omitted, will use the default artifact repository location configured in the controller, appended with the / in the key. No arguments io.argoproj.workflow.v1alpha1.Arguments Arguments hold arguments to the template. DEPRECATED: This field is not used. No automountServiceAccountToken boolean AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false. No container io.k8s.api.core.v1.Container Container is the main container image to run in the pod No daemon boolean Deamon will allow a workflow to proceed to the next step so long as the container reaches readiness No dag io.argoproj.workflow.v1alpha1.DAGTemplate DAG template subtype which runs a DAG No executor io.argoproj.workflow.v1alpha1.ExecutorConfig Executor holds configurations of the executor container. No hostAliases [ io.k8s.api.core.v1.HostAlias ] HostAliases is an optional list of hosts and IPs that will be injected into the pod spec No initContainers [ io.argoproj.workflow.v1alpha1.UserContainer ] InitContainers is a list of containers which run before the main container. No inputs io.argoproj.workflow.v1alpha1.Inputs Inputs describe what inputs parameters and artifacts are supplied to this template No metadata io.argoproj.workflow.v1alpha1.Metadata Metdata sets the pods's metadata, i.e. annotations and labels No metrics io.argoproj.workflow.v1alpha1.Metrics Metrics are a list of metrics emitted from this template No name string Name is the name of the template Yes nodeSelector object NodeSelector is a selector to schedule this step of the workflow to be run on the selected node(s). Overrides the selector set at the workflow level. No outputs io.argoproj.workflow.v1alpha1.Outputs Outputs describe the parameters and artifacts that this template produces No parallelism long Parallelism limits the max total parallel pods that can execute at the same time within the boundaries of this template invocation. If additional steps/dag templates are invoked, the pods created by those templates will not be counted towards this total. No podSpecPatch string PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits). No priority integer Priority to apply to workflow pods. No priorityClassName string PriorityClassName to apply to workflow pods. No resource io.argoproj.workflow.v1alpha1.ResourceTemplate Resource template subtype which can run k8s resources No resubmitPendingPods boolean ResubmitPendingPods is a flag to enable resubmitting pods that remain Pending after initial submission No retryStrategy io.argoproj.workflow.v1alpha1.RetryStrategy RetryStrategy describes how to retry a template when it fails No schedulerName string If specified, the pod will be dispatched by specified scheduler. Or it will be dispatched by workflow scope scheduler if specified. If neither specified, the pod will be dispatched by default scheduler. No script io.argoproj.workflow.v1alpha1.ScriptTemplate Script runs a portion of code against an interpreter No securityContext io.k8s.api.core.v1.PodSecurityContext SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to empty. See type description for default values of each field. No serviceAccountName string ServiceAccountName to apply to workflow pods No sidecars [ io.argoproj.workflow.v1alpha1.UserContainer ] Sidecars is a list of containers which run alongside the main container Sidecars are automatically killed when the main container completes No steps [ io.argoproj.workflow.v1alpha1.ParallelSteps ] Steps define a series of sequential/parallel workflow steps No suspend io.argoproj.workflow.v1alpha1.SuspendTemplate Suspend template subtype which can suspend a workflow when reaching the step No template string Template is the name of the template which is used as the base of this template. DEPRECATED: This field is not used. No templateRef io.argoproj.workflow.v1alpha1.TemplateRef TemplateRef is the reference to the template resource which is used as the base of this template. DEPRECATED: This field is not used. No tolerations [ io.k8s.api.core.v1.Toleration ] Tolerations to apply to workflow pods. No volumes [ io.k8s.api.core.v1.Volume ] Volumes is a list of volumes that can be mounted by containers in a template. No io.argoproj.workflow.v1alpha1.TemplateRef \u00b6 TemplateRef is a reference of template resource. Name Type Description Required clusterScope boolean ClusterScope indicates the referred template is cluster scoped (i.e. a ClusterWorkflowTemplate). No name string Name is the resource name of the template. No runtimeResolution boolean RuntimeResolution skips validation at creation time. By enabling this option, you can create the referred workflow template before the actual runtime. No template string Template is the name of referred template in the resource. No io.argoproj.workflow.v1alpha1.UpdateCronWorkflowRequest \u00b6 Name Type Description Required cronWorkflow io.argoproj.workflow.v1alpha1.CronWorkflow No name string DEPRECATED: This field is ignored. No namespace string No io.argoproj.workflow.v1alpha1.UserContainer \u00b6 UserContainer is a container specified by a user. Name Type Description Required args [ string ] Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell No command [ string ] Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell No env [ io.k8s.api.core.v1.EnvVar ] List of environment variables to set in the container. Cannot be updated. No envFrom [ io.k8s.api.core.v1.EnvFromSource ] List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. No image string Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets. No imagePullPolicy string Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images No lifecycle io.k8s.api.core.v1.Lifecycle Actions that the management system should take in response to container lifecycle events. Cannot be updated. No livenessProbe io.k8s.api.core.v1.Probe Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No mirrorVolumeMounts boolean MirrorVolumeMounts will mount the same volumes specified in the main container to the container (including artifacts), at the same mountPaths. This enables dind daemon to partially see the same filesystem as the main container in order to use features such as docker volume binding No name string Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated. Yes ports [ io.k8s.api.core.v1.ContainerPort ] List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \"0.0.0.0\" address inside a container will be accessible from the network. Cannot be updated. No readinessProbe io.k8s.api.core.v1.Probe Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No resources io.k8s.api.core.v1.ResourceRequirements Compute Resources required by this container. Cannot be updated. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ No securityContext io.k8s.api.core.v1.SecurityContext Security options the pod should run with. More info: https://kubernetes.io/docs/concepts/policy/security-context/ More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ No startupProbe io.k8s.api.core.v1.Probe StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state operation. This cannot be updated. This is an alpha feature enabled by the StartupProbe feature flag. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No stdin boolean Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false. No stdinOnce boolean Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false No terminationMessagePath string Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated. No terminationMessagePolicy string Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated. No tty boolean Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false. No volumeDevices [ io.k8s.api.core.v1.VolumeDevice ] volumeDevices is the list of block devices to be used by the container. This is a beta feature. No volumeMounts [ io.k8s.api.core.v1.VolumeMount ] Pod volumes to mount into the container's filesystem. Cannot be updated. No workingDir string Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated. No io.argoproj.workflow.v1alpha1.ValueFrom \u00b6 ValueFrom describes a location in which to obtain the value to a parameter Name Type Description Required default io.k8s.apimachinery.pkg.util.intstr.IntOrString Default specifies a value to be used if retrieving the value from the specified source fails No jqFilter string JQFilter expression against the resource object in resource templates No jsonPath string JSONPath of a resource to retrieve an output parameter value from in resource templates No parameter string Parameter reference to a step or dag task in which to retrieve an output parameter value from (e.g. '{{steps.mystep.outputs.myparam}}') No path string Path in the container to retrieve an output parameter value from in container templates No io.argoproj.workflow.v1alpha1.Version \u00b6 Name Type Description Required buildDate string Yes compiler string Yes gitCommit string Yes gitTag string Yes gitTreeState string Yes goVersion string Yes platform string Yes version string Yes io.argoproj.workflow.v1alpha1.Workflow \u00b6 Workflow is the definition of a workflow resource Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta Yes spec io.argoproj.workflow.v1alpha1.WorkflowSpec Yes status io.argoproj.workflow.v1alpha1.WorkflowStatus No io.argoproj.workflow.v1alpha1.WorkflowCreateRequest \u00b6 Name Type Description Required createOptions io.k8s.apimachinery.pkg.apis.meta.v1.CreateOptions No instanceID string This field is no longer used. No namespace string No serverDryRun boolean (boolean) No workflow io.argoproj.workflow.v1alpha1.Workflow No io.argoproj.workflow.v1alpha1.WorkflowDeleteResponse \u00b6 Name Type Description Required io.argoproj.workflow.v1alpha1.WorkflowDeleteResponse object io.argoproj.workflow.v1alpha1.WorkflowLintRequest \u00b6 Name Type Description Required namespace string No workflow io.argoproj.workflow.v1alpha1.Workflow No io.argoproj.workflow.v1alpha1.WorkflowList \u00b6 WorkflowList is list of Workflow resources Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No items [ io.argoproj.workflow.v1alpha1.Workflow ] Yes kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta Yes io.argoproj.workflow.v1alpha1.WorkflowResubmitRequest \u00b6 Name Type Description Required memoized boolean (boolean) No name string No namespace string No io.argoproj.workflow.v1alpha1.WorkflowResumeRequest \u00b6 Name Type Description Required name string No namespace string No nodeFieldSelector string No io.argoproj.workflow.v1alpha1.WorkflowRetryRequest \u00b6 Name Type Description Required name string No namespace string No nodeFieldSelector string No restartSuccessful boolean (boolean) No io.argoproj.workflow.v1alpha1.WorkflowSpec \u00b6 WorkflowSpec is the specification of a Workflow. Name Type Description Required activeDeadlineSeconds long Optional duration in seconds relative to the workflow start time which the workflow is allowed to run before the controller terminates the io.argoproj.workflow.v1alpha1. A value of zero is used to terminate a Running workflow No affinity io.k8s.api.core.v1.Affinity Affinity sets the scheduling constraints for all pods in the io.argoproj.workflow.v1alpha1. Can be overridden by an affinity specified in the template No arguments io.argoproj.workflow.v1alpha1.Arguments Arguments contain the parameters and artifacts sent to the workflow entrypoint Parameters are referencable globally using the 'workflow' variable prefix. e.g. {{io.argoproj.workflow.v1alpha1.parameters.myparam}} No artifactRepositoryRef io.argoproj.workflow.v1alpha1.ArtifactRepositoryRef ArtifactRepositoryRef specifies the configMap name and key containing the artifact repository config. No automountServiceAccountToken boolean AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false. No dnsConfig io.k8s.api.core.v1.PodDNSConfig PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. No dnsPolicy string Set DNS policy for the pod. Defaults to \"ClusterFirst\". Valid values are 'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to 'ClusterFirstWithHostNet'. No entrypoint string Entrypoint is a template reference to the starting point of the io.argoproj.workflow.v1alpha1. No executor io.argoproj.workflow.v1alpha1.ExecutorConfig Executor holds configurations of executor containers of the io.argoproj.workflow.v1alpha1. No hostAliases [ io.k8s.api.core.v1.HostAlias ] No hostNetwork boolean Host networking requested for this workflow pod. Default to false. No imagePullSecrets [ io.k8s.api.core.v1.LocalObjectReference ] ImagePullSecrets is a list of references to secrets in the same namespace to use for pulling any images in pods that reference this ServiceAccount. ImagePullSecrets are distinct from Secrets because Secrets can be mounted in the pod, but ImagePullSecrets are only accessed by the kubelet. More info: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod No metrics io.argoproj.workflow.v1alpha1.Metrics Metrics are a list of metrics emitted from this Workflow No nodeSelector object NodeSelector is a selector which will result in all pods of the workflow to be scheduled on the selected node(s). This is able to be overridden by a nodeSelector specified in the template. No onExit string OnExit is a template reference which is invoked at the end of the workflow, irrespective of the success, failure, or error of the primary io.argoproj.workflow.v1alpha1. No parallelism long Parallelism limits the max total parallel pods that can execute at the same time in a workflow No podDisruptionBudget io.k8s.api.policy.v1beta1.PodDisruptionBudgetSpec PodDisruptionBudget holds the number of concurrent disruptions that you allow for Workflow's Pods. Controller will automatically add the selector with workflow name, if selector is empty. Optional: Defaults to empty. No podGC io.argoproj.workflow.v1alpha1.PodGC PodGC describes the strategy to use when to deleting completed pods No podPriority integer Priority to apply to workflow pods. No podPriorityClassName string PriorityClassName to apply to workflow pods. No podSpecPatch string PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits). No priority integer Priority is used if controller is configured to process limited number of workflows in parallel. Workflows with higher priority are processed first. No schedulerName string Set scheduler name for all pods. Will be overridden if container/script template's scheduler name is set. Default scheduler will be used if neither specified. No securityContext io.k8s.api.core.v1.PodSecurityContext SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to empty. See type description for default values of each field. No serviceAccountName string ServiceAccountName is the name of the ServiceAccount to run all pods of the workflow as. No shutdown string Shutdown will shutdown the workflow according to its ShutdownStrategy No suspend boolean Suspend will suspend the workflow and prevent execution of any future steps in the workflow No templates [ io.argoproj.workflow.v1alpha1.Template ] Templates is a list of workflow templates used in a workflow No tolerations [ io.k8s.api.core.v1.Toleration ] Tolerations to apply to workflow pods. No ttlSecondsAfterFinished integer TTLSecondsAfterFinished limits the lifetime of a Workflow that has finished execution (Succeeded, Failed, Error). If this field is set, once the Workflow finishes, it will be deleted after ttlSecondsAfterFinished expires. If this field is unset, ttlSecondsAfterFinished will not expire. If this field is set to zero, ttlSecondsAfterFinished expires immediately after the Workflow finishes. DEPRECATED: Use TTLStrategy.SecondsAfterCompletion instead. No ttlStrategy io.argoproj.workflow.v1alpha1.TTLStrategy TTLStrategy limits the lifetime of a Workflow that has finished execution depending on if it Succeeded or Failed. If this struct is set, once the Workflow finishes, it will be deleted after the time to live expires. If this field is unset, the controller config map will hold the default values. No volumeClaimTemplates [ io.k8s.api.core.v1.PersistentVolumeClaim ] VolumeClaimTemplates is a list of claims that containers are allowed to reference. The Workflow controller will create the claims at the beginning of the workflow and delete the claims upon completion of the workflow No volumes [ io.k8s.api.core.v1.Volume ] Volumes is a list of volumes that can be mounted by containers in a io.argoproj.workflow.v1alpha1. No workflowTemplateRef io.argoproj.workflow.v1alpha1.WorkflowTemplateRef WorkflowTemplateRef holds a reference to a WorkflowTemplate for execution No io.argoproj.workflow.v1alpha1.WorkflowStatus \u00b6 WorkflowStatus contains overall status information about a workflow Name Type Description Required compressedNodes string Compressed and base64 decoded Nodes map No conditions [ io.argoproj.workflow.v1alpha1.Condition ] Conditions is a list of conditions the Workflow may have No finishedAt io.k8s.apimachinery.pkg.apis.meta.v1.Time Time at which this workflow completed No message string A human readable message indicating details about why the workflow is in this condition. No nodes object Nodes is a mapping between a node ID and the node's status. No offloadNodeStatusVersion string Whether on not node status has been offloaded to a database. If exists, then Nodes and CompressedNodes will be empty. This will actually be populated with a hash of the offloaded data. No outputs io.argoproj.workflow.v1alpha1.Outputs Outputs captures output values and artifact locations produced by the workflow via global outputs No persistentVolumeClaims [ io.k8s.api.core.v1.Volume ] PersistentVolumeClaims tracks all PVCs that were created as part of the io.argoproj.workflow.v1alpha1. The contents of this list are drained at the end of the workflow. No phase string Phase a simple, high-level summary of where the workflow is in its lifecycle. No resourcesDuration object ResourcesDuration is the total for the workflow No startedAt io.k8s.apimachinery.pkg.apis.meta.v1.Time Time at which this workflow started No storedTemplates object StoredTemplates is a mapping between a template ref and the node's status. No storedWorkflowTemplateSpec io.argoproj.workflow.v1alpha1.WorkflowSpec StoredWorkflowSpec stores the WorkflowTemplate spec for future execution. No io.argoproj.workflow.v1alpha1.WorkflowStep \u00b6 WorkflowStep is a reference to a template to execute in a series of step Name Type Description Required arguments io.argoproj.workflow.v1alpha1.Arguments Arguments hold arguments to the template No continueOn io.argoproj.workflow.v1alpha1.ContinueOn ContinueOn makes argo to proceed with the following step even if this step fails. Errors and Failed states can be specified No name string Name of the step No onExit string OnExit is a template reference which is invoked at the end of the template, irrespective of the success, failure, or error of the primary template. No template string Template is the name of the template to execute as the step No templateRef io.argoproj.workflow.v1alpha1.TemplateRef TemplateRef is the reference to the template resource to execute as the step. No when string When is an expression in which the step should conditionally execute No withItems [ io.argoproj.workflow.v1alpha1.Item ] WithItems expands a step into multiple parallel steps from the items in the list No withParam string WithParam expands a step into multiple parallel steps from the value in the parameter, which is expected to be a JSON list. No withSequence io.argoproj.workflow.v1alpha1.Sequence WithSequence expands a step into a numeric sequence No io.argoproj.workflow.v1alpha1.WorkflowStopRequest \u00b6 Name Type Description Required message string No name string No namespace string No nodeFieldSelector string No io.argoproj.workflow.v1alpha1.WorkflowSubmitRequest \u00b6 Name Type Description Required namespace string No resourceKind string No resourceName string No submitOptions io.argoproj.workflow.v1alpha1.SubmitOpts No io.argoproj.workflow.v1alpha1.WorkflowSuspendRequest \u00b6 Name Type Description Required name string No namespace string No io.argoproj.workflow.v1alpha1.WorkflowTemplate \u00b6 WorkflowTemplate is the definition of a workflow template resource Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta Yes spec io.argoproj.workflow.v1alpha1.WorkflowTemplateSpec Yes io.argoproj.workflow.v1alpha1.WorkflowTemplateCreateRequest \u00b6 Name Type Description Required createOptions io.k8s.apimachinery.pkg.apis.meta.v1.CreateOptions No namespace string No template io.argoproj.workflow.v1alpha1.WorkflowTemplate No io.argoproj.workflow.v1alpha1.WorkflowTemplateDeleteResponse \u00b6 Name Type Description Required io.argoproj.workflow.v1alpha1.WorkflowTemplateDeleteResponse object io.argoproj.workflow.v1alpha1.WorkflowTemplateLintRequest \u00b6 Name Type Description Required createOptions io.k8s.apimachinery.pkg.apis.meta.v1.CreateOptions No namespace string No template io.argoproj.workflow.v1alpha1.WorkflowTemplate No io.argoproj.workflow.v1alpha1.WorkflowTemplateList \u00b6 WorkflowTemplateList is list of WorkflowTemplate resources Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No items [ io.argoproj.workflow.v1alpha1.WorkflowTemplate ] Yes kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta Yes io.argoproj.workflow.v1alpha1.WorkflowTemplateRef \u00b6 WorkflowTemplateRef is a reference to a WorkflowTemplate resource. Name Type Description Required clusterScope boolean ClusterScope indicates the referred template is cluster scoped (i.e. a ClusterWorkflowTemplate). No name string Name is the resource name of the workflow template. No io.argoproj.workflow.v1alpha1.WorkflowTemplateSpec \u00b6 WorkflowTemplateSpec is a spec of WorkflowTemplate. Name Type Description Required activeDeadlineSeconds long Optional duration in seconds relative to the workflow start time which the workflow is allowed to run before the controller terminates the io.argoproj.workflow.v1alpha1. A value of zero is used to terminate a Running workflow No affinity io.k8s.api.core.v1.Affinity Affinity sets the scheduling constraints for all pods in the io.argoproj.workflow.v1alpha1. Can be overridden by an affinity specified in the template No arguments io.argoproj.workflow.v1alpha1.Arguments Arguments contain the parameters and artifacts sent to the workflow entrypoint Parameters are referencable globally using the 'workflow' variable prefix. e.g. {{io.argoproj.workflow.v1alpha1.parameters.myparam}} No artifactRepositoryRef io.argoproj.workflow.v1alpha1.ArtifactRepositoryRef ArtifactRepositoryRef specifies the configMap name and key containing the artifact repository config. No automountServiceAccountToken boolean AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false. No dnsConfig io.k8s.api.core.v1.PodDNSConfig PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. No dnsPolicy string Set DNS policy for the pod. Defaults to \"ClusterFirst\". Valid values are 'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to 'ClusterFirstWithHostNet'. No entrypoint string Entrypoint is a template reference to the starting point of the io.argoproj.workflow.v1alpha1. No executor io.argoproj.workflow.v1alpha1.ExecutorConfig Executor holds configurations of executor containers of the io.argoproj.workflow.v1alpha1. No hostAliases [ io.k8s.api.core.v1.HostAlias ] No hostNetwork boolean Host networking requested for this workflow pod. Default to false. No imagePullSecrets [ io.k8s.api.core.v1.LocalObjectReference ] ImagePullSecrets is a list of references to secrets in the same namespace to use for pulling any images in pods that reference this ServiceAccount. ImagePullSecrets are distinct from Secrets because Secrets can be mounted in the pod, but ImagePullSecrets are only accessed by the kubelet. More info: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod No metrics io.argoproj.workflow.v1alpha1.Metrics Metrics are a list of metrics emitted from this Workflow No nodeSelector object NodeSelector is a selector which will result in all pods of the workflow to be scheduled on the selected node(s). This is able to be overridden by a nodeSelector specified in the template. No onExit string OnExit is a template reference which is invoked at the end of the workflow, irrespective of the success, failure, or error of the primary io.argoproj.workflow.v1alpha1. No parallelism long Parallelism limits the max total parallel pods that can execute at the same time in a workflow No podDisruptionBudget io.k8s.api.policy.v1beta1.PodDisruptionBudgetSpec PodDisruptionBudget holds the number of concurrent disruptions that you allow for Workflow's Pods. Controller will automatically add the selector with workflow name, if selector is empty. Optional: Defaults to empty. No podGC io.argoproj.workflow.v1alpha1.PodGC PodGC describes the strategy to use when to deleting completed pods No podPriority integer Priority to apply to workflow pods. No podPriorityClassName string PriorityClassName to apply to workflow pods. No podSpecPatch string PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits). No priority integer Priority is used if controller is configured to process limited number of workflows in parallel. Workflows with higher priority are processed first. No schedulerName string Set scheduler name for all pods. Will be overridden if container/script template's scheduler name is set. Default scheduler will be used if neither specified. No securityContext io.k8s.api.core.v1.PodSecurityContext SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to empty. See type description for default values of each field. No serviceAccountName string ServiceAccountName is the name of the ServiceAccount to run all pods of the workflow as. No shutdown string Shutdown will shutdown the workflow according to its ShutdownStrategy No suspend boolean Suspend will suspend the workflow and prevent execution of any future steps in the workflow No templates [ io.argoproj.workflow.v1alpha1.Template ] Templates is a list of workflow templates used in a workflow No tolerations [ io.k8s.api.core.v1.Toleration ] Tolerations to apply to workflow pods. No ttlSecondsAfterFinished integer TTLSecondsAfterFinished limits the lifetime of a Workflow that has finished execution (Succeeded, Failed, Error). If this field is set, once the Workflow finishes, it will be deleted after ttlSecondsAfterFinished expires. If this field is unset, ttlSecondsAfterFinished will not expire. If this field is set to zero, ttlSecondsAfterFinished expires immediately after the Workflow finishes. DEPRECATED: Use TTLStrategy.SecondsAfterCompletion instead. No ttlStrategy io.argoproj.workflow.v1alpha1.TTLStrategy TTLStrategy limits the lifetime of a Workflow that has finished execution depending on if it Succeeded or Failed. If this struct is set, once the Workflow finishes, it will be deleted after the time to live expires. If this field is unset, the controller config map will hold the default values. No volumeClaimTemplates [ io.k8s.api.core.v1.PersistentVolumeClaim ] VolumeClaimTemplates is a list of claims that containers are allowed to reference. The Workflow controller will create the claims at the beginning of the workflow and delete the claims upon completion of the workflow No volumes [ io.k8s.api.core.v1.Volume ] Volumes is a list of volumes that can be mounted by containers in a io.argoproj.workflow.v1alpha1. No workflowTemplateRef io.argoproj.workflow.v1alpha1.WorkflowTemplateRef WorkflowTemplateRef holds a reference to a WorkflowTemplate for execution No io.argoproj.workflow.v1alpha1.WorkflowTemplateUpdateRequest \u00b6 Name Type Description Required name string DEPRECATED: This field is ignored. No namespace string No template io.argoproj.workflow.v1alpha1.WorkflowTemplate No io.argoproj.workflow.v1alpha1.WorkflowTerminateRequest \u00b6 Name Type Description Required name string No namespace string No io.argoproj.workflow.v1alpha1.WorkflowWatchEvent \u00b6 Name Type Description Required object io.argoproj.workflow.v1alpha1.Workflow No type string No io.k8s.api.core.v1.AWSElasticBlockStoreVolumeSource \u00b6 Represents a Persistent Disk resource in AWS. An AWS EBS disk must exist before mounting to a container. The disk must also be in the same AWS zone as the kubelet. An AWS EBS disk can only be mounted as read/write once. AWS EBS volumes support ownership management and SELinux relabeling. Name Type Description Required fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore No partition integer The partition in the volume that you want to mount. If omitted, the default is to mount by volume name. Examples: For volume /dev/sda1, you specify the partition as \"1\". Similarly, the volume partition for /dev/sda is \"0\" (or you can leave the property empty). No readOnly boolean Specify \"true\" to force and set the ReadOnly property in VolumeMounts to \"true\". If omitted, the default is \"false\". More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore No volumeID string Unique ID of the persistent disk resource in AWS (Amazon EBS volume). More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore Yes io.k8s.api.core.v1.Affinity \u00b6 Affinity is a group of affinity scheduling rules. Name Type Description Required nodeAffinity io.k8s.api.core.v1.NodeAffinity Describes node affinity scheduling rules for the pod. No podAffinity io.k8s.api.core.v1.PodAffinity Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). No podAntiAffinity io.k8s.api.core.v1.PodAntiAffinity Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). No io.k8s.api.core.v1.AzureDiskVolumeSource \u00b6 AzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod. Name Type Description Required cachingMode string Host Caching mode: None, Read Only, Read Write. No diskName string The Name of the data disk in the blob storage Yes diskURI string The URI the data disk in the blob storage Yes fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. No kind string Expected values Shared: multiple blob disks per storage account Dedicated: single blob disk per storage account Managed: azure managed data disk (only in managed availability set). defaults to shared No readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. No io.k8s.api.core.v1.AzureFileVolumeSource \u00b6 AzureFile represents an Azure File Service mount on the host and bind mount to the pod. Name Type Description Required readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. No secretName string the name of secret that contains Azure Storage Account Name and Key Yes shareName string Share Name Yes io.k8s.api.core.v1.CSIVolumeSource \u00b6 Represents a source location of a volume to mount, managed by an external CSI driver Name Type Description Required driver string Driver is the name of the CSI driver that handles this volume. Consult with your admin for the correct name as registered in the cluster. Yes fsType string Filesystem type to mount. Ex. \"ext4\", \"xfs\", \"ntfs\". If not provided, the empty value is passed to the associated CSI driver which will determine the default filesystem to apply. No nodePublishSecretRef io.k8s.api.core.v1.LocalObjectReference NodePublishSecretRef is a reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI NodePublishVolume and NodeUnpublishVolume calls. This field is optional, and may be empty if no secret is required. If the secret object contains more than one secret, all secret references are passed. No readOnly boolean Specifies a read-only configuration for the volume. Defaults to false (read/write). No volumeAttributes object VolumeAttributes stores driver-specific properties that are passed to the CSI driver. Consult your driver's documentation for supported values. No io.k8s.api.core.v1.Capabilities \u00b6 Adds and removes POSIX capabilities from running containers. Name Type Description Required add [ string ] Added capabilities No drop [ string ] Removed capabilities No io.k8s.api.core.v1.CephFSVolumeSource \u00b6 Represents a Ceph Filesystem mount that lasts the lifetime of a pod Cephfs volumes do not support ownership management or SELinux relabeling. Name Type Description Required monitors [ string ] Required: Monitors is a collection of Ceph monitors More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it Yes path string Optional: Used as the mounted root, rather than the full Ceph tree, default is / No readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it No secretFile string Optional: SecretFile is the path to key ring for User, default is /etc/ceph/user.secret More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it No secretRef io.k8s.api.core.v1.LocalObjectReference Optional: SecretRef is reference to the authentication secret for User, default is empty. More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it No user string Optional: User is the rados user name, default is admin More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it No io.k8s.api.core.v1.CinderVolumeSource \u00b6 Represents a cinder volume resource in Openstack. A Cinder volume must exist before mounting to a container. The volume must also be in the same region as the kubelet. Cinder volumes support ownership management and SELinux relabeling. Name Type Description Required fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md No readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md No secretRef io.k8s.api.core.v1.LocalObjectReference Optional: points to a secret object containing parameters used to connect to OpenStack. No volumeID string volume id used to identify the volume in cinder More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md Yes io.k8s.api.core.v1.ConfigMapEnvSource \u00b6 ConfigMapEnvSource selects a ConfigMap to populate the environment variables with. The contents of the target ConfigMap's Data field will represent the key-value pairs as environment variables. Name Type Description Required name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No optional boolean Specify whether the ConfigMap must be defined No io.k8s.api.core.v1.ConfigMapKeySelector \u00b6 Selects a key from a ConfigMap. Name Type Description Required key string The key to select. Yes name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No optional boolean Specify whether the ConfigMap or its key must be defined No io.k8s.api.core.v1.ConfigMapProjection \u00b6 Adapts a ConfigMap into a projected volume. The contents of the target ConfigMap's Data field will be presented in a projected volume as files using the keys in the Data field as the file names, unless the items element is populated with specific mappings of keys to paths. Note that this is identical to a configmap volume source without the default mode. Name Type Description Required items [ io.k8s.api.core.v1.KeyToPath ] If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. No name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No optional boolean Specify whether the ConfigMap or its keys must be defined No io.k8s.api.core.v1.ConfigMapVolumeSource \u00b6 Adapts a ConfigMap into a volume. The contents of the target ConfigMap's Data field will be presented in a volume as files using the keys in the Data field as the file names, unless the items element is populated with specific mappings of keys to paths. ConfigMap volumes support ownership management and SELinux relabeling. Name Type Description Required defaultMode integer Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. No items [ io.k8s.api.core.v1.KeyToPath ] If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. No name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No optional boolean Specify whether the ConfigMap or its keys must be defined No io.k8s.api.core.v1.Container \u00b6 A single application container that you want to run within a pod. Name Type Description Required args [ string ] Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell No command [ string ] Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell No env [ io.k8s.api.core.v1.EnvVar ] List of environment variables to set in the container. Cannot be updated. No envFrom [ io.k8s.api.core.v1.EnvFromSource ] List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. No image string Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets. Yes imagePullPolicy string Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images No lifecycle io.k8s.api.core.v1.Lifecycle Actions that the management system should take in response to container lifecycle events. Cannot be updated. No livenessProbe io.k8s.api.core.v1.Probe Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No name string Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated. No ports [ io.k8s.api.core.v1.ContainerPort ] List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \"0.0.0.0\" address inside a container will be accessible from the network. Cannot be updated. No readinessProbe io.k8s.api.core.v1.Probe Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No resources io.k8s.api.core.v1.ResourceRequirements Compute Resources required by this container. Cannot be updated. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ No securityContext io.k8s.api.core.v1.SecurityContext Security options the pod should run with. More info: https://kubernetes.io/docs/concepts/policy/security-context/ More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ No stdin boolean Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false. No stdinOnce boolean Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false No terminationMessagePath string Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated. No terminationMessagePolicy string Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated. No tty boolean Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false. No volumeDevices [ io.k8s.api.core.v1.VolumeDevice ] volumeDevices is the list of block devices to be used by the container. This is a beta feature. No volumeMounts [ io.k8s.api.core.v1.VolumeMount ] Pod volumes to mount into the container's filesystem. Cannot be updated. No workingDir string Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated. No io.k8s.api.core.v1.ContainerPort \u00b6 ContainerPort represents a network port in a single container. Name Type Description Required containerPort integer Number of port to expose on the pod's IP address. This must be a valid port number, 0 < x < 65536. Yes hostIP string What host IP to bind the external port to. No hostPort integer Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. If HostNetwork is specified, this must match ContainerPort. Most containers do not need this. No name string If specified, this must be an IANA_SVC_NAME and unique within the pod. Each named port in a pod must have a unique name. Name for the port that can be referred to by services. No protocol string Protocol for port. Must be UDP, TCP, or SCTP. Defaults to \"TCP\". No io.k8s.api.core.v1.DownwardAPIProjection \u00b6 Represents downward API info for projecting into a projected volume. Note that this is identical to a downwardAPI volume source without the default mode. Name Type Description Required items [ io.k8s.api.core.v1.DownwardAPIVolumeFile ] Items is a list of DownwardAPIVolume file No io.k8s.api.core.v1.DownwardAPIVolumeFile \u00b6 DownwardAPIVolumeFile represents information to create the file containing the pod field Name Type Description Required fieldRef io.k8s.api.core.v1.ObjectFieldSelector Required: Selects a field of the pod: only annotations, labels, name and namespace are supported. No mode integer Optional: mode bits to use on this file, must be a value between 0 and 0777. If not specified, the volume defaultMode will be used. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. No path string Required: Path is the relative path name of the file to be created. Must not be absolute or contain the '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..' Yes resourceFieldRef io.k8s.api.core.v1.ResourceFieldSelector Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported. No io.k8s.api.core.v1.DownwardAPIVolumeSource \u00b6 DownwardAPIVolumeSource represents a volume containing downward API info. Downward API volumes support ownership management and SELinux relabeling. Name Type Description Required defaultMode integer Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. No items [ io.k8s.api.core.v1.DownwardAPIVolumeFile ] Items is a list of downward API volume file No io.k8s.api.core.v1.EmptyDirVolumeSource \u00b6 Represents an empty directory for a pod. Empty directory volumes support ownership management and SELinux relabeling. Name Type Description Required medium string What type of storage medium should back this directory. The default is \"\" which means to use the node's default medium. Must be an empty string (default) or Memory. More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir No sizeLimit io.k8s.apimachinery.pkg.api.resource.Quantity Total amount of local storage required for this EmptyDir volume. The size limit is also applicable for memory medium. The maximum usage on memory medium EmptyDir would be the minimum value between the SizeLimit specified here and the sum of memory limits of all containers in a pod. The default is nil which means that the limit is undefined. More info: http://kubernetes.io/docs/user-guide/volumes#emptydir No io.k8s.api.core.v1.EnvFromSource \u00b6 EnvFromSource represents the source of a set of ConfigMaps Name Type Description Required configMapRef io.k8s.api.core.v1.ConfigMapEnvSource The ConfigMap to select from No prefix string An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER. No secretRef io.k8s.api.core.v1.SecretEnvSource The Secret to select from No io.k8s.api.core.v1.EnvVar \u00b6 EnvVar represents an environment variable present in a Container. Name Type Description Required name string Name of the environment variable. Must be a C_IDENTIFIER. Yes value string Variable references $(VAR_NAME) are expanded using the previous defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to \"\". No valueFrom io.k8s.api.core.v1.EnvVarSource Source for the environment variable's value. Cannot be used if value is not empty. No io.k8s.api.core.v1.EnvVarSource \u00b6 EnvVarSource represents a source for the value of an EnvVar. Name Type Description Required configMapKeyRef io.k8s.api.core.v1.ConfigMapKeySelector Selects a key of a ConfigMap. No fieldRef io.k8s.api.core.v1.ObjectFieldSelector Selects a field of the pod: supports metadata.name, metadata.namespace, metadata.labels, metadata.annotations, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP. No resourceFieldRef io.k8s.api.core.v1.ResourceFieldSelector Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. No secretKeyRef io.k8s.api.core.v1.SecretKeySelector Selects a key of a secret in the pod's namespace No io.k8s.api.core.v1.ExecAction \u00b6 ExecAction describes a \"run in container\" action. Name Type Description Required command [ string ] Command is the command line to execute inside the container, the working directory for the command is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions (' ', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy. io.k8s.api.core.v1.FCVolumeSource \u00b6 Represents a Fibre Channel volume. Fibre Channel volumes can only be mounted as read/write once. Fibre Channel volumes support ownership management and SELinux relabeling. Name Type Description Required fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. No lun integer Optional: FC target lun number No readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. No targetWWNs [ string ] Optional: FC target worldwide names (WWNs) No wwids [ string ] Optional: FC volume world wide identifiers (wwids) Either wwids or combination of targetWWNs and lun must be set, but not both simultaneously. No io.k8s.api.core.v1.FlexVolumeSource \u00b6 FlexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin. Name Type Description Required driver string Driver is the name of the driver to use for this volume. Yes fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". The default filesystem depends on FlexVolume script. No options object Optional: Extra command options if any. No readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. No secretRef io.k8s.api.core.v1.LocalObjectReference Optional: SecretRef is reference to the secret object containing sensitive information to pass to the plugin scripts. This may be empty if no secret object is specified. If the secret object contains more than one secret, all secrets are passed to the plugin scripts. No io.k8s.api.core.v1.FlockerVolumeSource \u00b6 Represents a Flocker volume mounted by the Flocker agent. One and only one of datasetName and datasetUUID should be set. Flocker volumes do not support ownership management or SELinux relabeling. Name Type Description Required datasetName string Name of the dataset stored as metadata -> name on the dataset for Flocker should be considered as deprecated No datasetUUID string UUID of the dataset. This is unique identifier of a Flocker dataset No io.k8s.api.core.v1.GCEPersistentDiskVolumeSource \u00b6 Represents a Persistent Disk resource in Google Compute Engine. A GCE PD must exist before mounting to a container. The disk must also be in the same GCE project and zone as the kubelet. A GCE PD can only be mounted as read/write once or read-only many times. GCE PDs support ownership management and SELinux relabeling. Name Type Description Required fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk No partition integer The partition in the volume that you want to mount. If omitted, the default is to mount by volume name. Examples: For volume /dev/sda1, you specify the partition as \"1\". Similarly, the volume partition for /dev/sda is \"0\" (or you can leave the property empty). More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk No pdName string Unique name of the PD resource in GCE. Used to identify the disk in GCE. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk Yes readOnly boolean ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk No io.k8s.api.core.v1.GitRepoVolumeSource \u00b6 Represents a volume that is populated with the contents of a git repository. Git repo volumes do not support ownership management. Git repo volumes support SELinux relabeling. DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir into the Pod's container. Name Type Description Required directory string Target directory name. Must not contain or start with '..'. If '.' is supplied, the volume directory will be the git repository. Otherwise, if specified, the volume will contain the git repository in the subdirectory with the given name. No repository string Repository URL Yes revision string Commit hash for the specified revision. No io.k8s.api.core.v1.GlusterfsVolumeSource \u00b6 Represents a Glusterfs mount that lasts the lifetime of a pod. Glusterfs volumes do not support ownership management or SELinux relabeling. Name Type Description Required endpoints string EndpointsName is the endpoint name that details Glusterfs topology. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md#create-a-pod Yes path string Path is the Glusterfs volume path. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md#create-a-pod Yes readOnly boolean ReadOnly here will force the Glusterfs volume to be mounted with read-only permissions. Defaults to false. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md#create-a-pod No io.k8s.api.core.v1.HTTPGetAction \u00b6 HTTPGetAction describes an action based on HTTP Get requests. Name Type Description Required host string Host name to connect to, defaults to the pod IP. You probably want to set \"Host\" in httpHeaders instead. No httpHeaders [ io.k8s.api.core.v1.HTTPHeader ] Custom headers to set in the request. HTTP allows repeated headers. No path string Path to access on the HTTP server. No port io.k8s.apimachinery.pkg.util.intstr.IntOrString Name or number of the port to access on the container. Number must be in the range 1 to 65535. Name must be an IANA_SVC_NAME. Yes scheme string Scheme to use for connecting to the host. Defaults to HTTP. No io.k8s.api.core.v1.HTTPHeader \u00b6 HTTPHeader describes a custom header to be used in HTTP probes Name Type Description Required name string The header field name Yes value string The header field value Yes io.k8s.api.core.v1.Handler \u00b6 Handler defines a specific action that should be taken Name Type Description Required exec io.k8s.api.core.v1.ExecAction One and only one of the following should be specified. Exec specifies the action to take. No httpGet io.k8s.api.core.v1.HTTPGetAction HTTPGet specifies the http request to perform. No tcpSocket io.k8s.api.core.v1.TCPSocketAction TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported No io.k8s.api.core.v1.HostAlias \u00b6 HostAlias holds the mapping between IP and hostnames that will be injected as an entry in the pod's hosts file. Name Type Description Required hostnames [ string ] Hostnames for the above IP address. No ip string IP address of the host file entry. No io.k8s.api.core.v1.HostPathVolumeSource \u00b6 Represents a host path mapped into a pod. Host path volumes do not support ownership management or SELinux relabeling. Name Type Description Required path string Path of the directory on the host. If the path is a symlink, it will follow the link to the real path. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath Yes type string Type for HostPath Volume Defaults to \"\" More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath No io.k8s.api.core.v1.ISCSIVolumeSource \u00b6 Represents an ISCSI disk. ISCSI volumes can only be mounted as read/write once. ISCSI volumes support ownership management and SELinux relabeling. Name Type Description Required chapAuthDiscovery boolean whether support iSCSI Discovery CHAP authentication No chapAuthSession boolean whether support iSCSI Session CHAP authentication No fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#iscsi No initiatorName string Custom iSCSI Initiator Name. If initiatorName is specified with iscsiInterface simultaneously, new iSCSI interface : will be created for the connection. No iqn string Target iSCSI Qualified Name. Yes iscsiInterface string iSCSI Interface Name that uses an iSCSI transport. Defaults to 'default' (tcp). No lun integer iSCSI Target Lun number. Yes portals [ string ] iSCSI Target Portal List. The portal is either an IP or ip_addr:port if the port is other than default (typically TCP ports 860 and 3260). No readOnly boolean ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. No secretRef io.k8s.api.core.v1.LocalObjectReference CHAP Secret for iSCSI target and initiator authentication No targetPortal string iSCSI Target Portal. The Portal is either an IP or ip_addr:port if the port is other than default (typically TCP ports 860 and 3260). Yes io.k8s.api.core.v1.KeyToPath \u00b6 Maps a string key to a path within a volume. Name Type Description Required key string The key to project. Yes mode integer Optional: mode bits to use on this file, must be a value between 0 and 0777. If not specified, the volume defaultMode will be used. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. No path string The relative path of the file to map the key to. May not be an absolute path. May not contain the path element '..'. May not start with the string '..'. Yes io.k8s.api.core.v1.Lifecycle \u00b6 Lifecycle describes actions that the management system should take in response to container lifecycle events. For the PostStart and PreStop lifecycle handlers, management of the container blocks until the action is complete, unless the container process fails, in which case the handler is aborted. Name Type Description Required postStart io.k8s.api.core.v1.Handler PostStart is called immediately after a container is created. If the handler fails, the container is terminated and restarted according to its restart policy. Other management of the container blocks until the hook completes. More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks No preStop io.k8s.api.core.v1.Handler PreStop is called immediately before a container is terminated due to an API request or management event such as liveness probe failure, preemption, resource contention, etc. The handler is not called if the container crashes or exits. The reason for termination is passed to the handler. The Pod's termination grace period countdown begins before the PreStop hooked is executed. Regardless of the outcome of the handler, the container will eventually terminate within the Pod's termination grace period. Other management of the container blocks until the hook completes or until the termination grace period is reached. More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks No io.k8s.api.core.v1.LocalObjectReference \u00b6 LocalObjectReference contains enough information to let you locate the referenced object inside the same namespace. Name Type Description Required name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No io.k8s.api.core.v1.NFSVolumeSource \u00b6 Represents an NFS mount that lasts the lifetime of a pod. NFS volumes do not support ownership management or SELinux relabeling. Name Type Description Required path string Path that is exported by the NFS server. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs Yes readOnly boolean ReadOnly here will force the NFS export to be mounted with read-only permissions. Defaults to false. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs No server string Server is the hostname or IP address of the NFS server. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs Yes io.k8s.api.core.v1.NodeAffinity \u00b6 Node affinity is a group of node affinity scheduling rules. Name Type Description Required preferredDuringSchedulingIgnoredDuringExecution [ io.k8s.api.core.v1.PreferredSchedulingTerm ] The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node matches the corresponding matchExpressions; the node(s) with the highest sum are the most preferred. No requiredDuringSchedulingIgnoredDuringExecution io.k8s.api.core.v1.NodeSelector If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to an update), the system may or may not try to eventually evict the pod from its node. No io.k8s.api.core.v1.NodeSelector \u00b6 A node selector represents the union of the results of one or more label queries over a set of nodes; that is, it represents the OR of the selectors represented by the node selector terms. Name Type Description Required nodeSelectorTerms [ io.k8s.api.core.v1.NodeSelectorTerm ] Required. A list of node selector terms. The terms are ORed. Yes io.k8s.api.core.v1.NodeSelectorRequirement \u00b6 A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string The label key that the selector applies to. Yes operator string Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt. Yes values [ string ] An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the values array must have a single element, which will be interpreted as an integer. This array is replaced during a strategic merge patch. No io.k8s.api.core.v1.NodeSelectorTerm \u00b6 A null or empty node selector term matches no objects. The requirements of them are ANDed. The TopologySelectorTerm type implements a subset of the NodeSelectorTerm. Name Type Description Required matchExpressions [ io.k8s.api.core.v1.NodeSelectorRequirement ] A list of node selector requirements by node's labels. No matchFields [ io.k8s.api.core.v1.NodeSelectorRequirement ] A list of node selector requirements by node's fields. No io.k8s.api.core.v1.ObjectFieldSelector \u00b6 ObjectFieldSelector selects an APIVersioned field of an object. Name Type Description Required apiVersion string Version of the schema the FieldPath is written in terms of, defaults to \"v1\". No fieldPath string Path of the field to select in the specified API version. Yes io.k8s.api.core.v1.ObjectReference \u00b6 ObjectReference contains enough information to let you inspect or modify the referred object. Name Type Description Required apiVersion string API version of the referent. No fieldPath string If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. No kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds No name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No namespace string Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ No resourceVersion string Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency No uid string UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids No io.k8s.api.core.v1.PersistentVolumeClaim \u00b6 PersistentVolumeClaim is a user's request for and claim to a persistent volume Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources No kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta Standard object's metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata No spec io.k8s.api.core.v1.PersistentVolumeClaimSpec Spec defines the desired characteristics of a volume requested by a pod author. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims No status io.k8s.api.core.v1.PersistentVolumeClaimStatus Status represents the current information/status of a persistent volume claim. Read-only. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims No io.k8s.api.core.v1.PersistentVolumeClaimCondition \u00b6 PersistentVolumeClaimCondition contails details about state of pvc Name Type Description Required lastProbeTime io.k8s.apimachinery.pkg.apis.meta.v1.Time Last time we probed the condition. No lastTransitionTime io.k8s.apimachinery.pkg.apis.meta.v1.Time Last time the condition transitioned from one status to another. No message string Human-readable message indicating details about last transition. No reason string Unique, this should be a short, machine understandable string that gives the reason for condition's last transition. If it reports \"ResizeStarted\" that means the underlying persistent volume is being resized. No status string Yes type string Yes io.k8s.api.core.v1.PersistentVolumeClaimSpec \u00b6 PersistentVolumeClaimSpec describes the common attributes of storage devices and allows a Source for provider-specific attributes Name Type Description Required accessModes [ string ] AccessModes contains the desired access modes the volume should have. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1 No dataSource io.k8s.api.core.v1.TypedLocalObjectReference This field requires the VolumeSnapshotDataSource alpha feature gate to be enabled and currently VolumeSnapshot is the only supported data source. If the provisioner can support VolumeSnapshot data source, it will create a new volume and data will be restored to the volume at the same time. If the provisioner does not support VolumeSnapshot data source, volume will not be created and the failure will be reported as an event. In the future, we plan to support more data source types and the behavior of the provisioner may change. No resources io.k8s.api.core.v1.ResourceRequirements Resources represents the minimum resources the volume should have. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources No selector io.k8s.apimachinery.pkg.apis.meta.v1.LabelSelector A label query over volumes to consider for binding. No storageClassName string Name of the StorageClass required by the claim. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1 No volumeMode string volumeMode defines what type of volume is required by the claim. Value of Filesystem is implied when not included in claim spec. This is a beta feature. No volumeName string VolumeName is the binding reference to the PersistentVolume backing this claim. No io.k8s.api.core.v1.PersistentVolumeClaimStatus \u00b6 PersistentVolumeClaimStatus is the current status of a persistent volume claim. Name Type Description Required accessModes [ string ] AccessModes contains the actual access modes the volume backing the PVC has. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1 No capacity object Represents the actual resources of the underlying volume. No conditions [ io.k8s.api.core.v1.PersistentVolumeClaimCondition ] Current Condition of persistent volume claim. If underlying persistent volume is being resized then the Condition will be set to 'ResizeStarted'. No phase string Phase represents the current phase of PersistentVolumeClaim. No io.k8s.api.core.v1.PersistentVolumeClaimVolumeSource \u00b6 PersistentVolumeClaimVolumeSource references the user's PVC in the same namespace. This volume finds the bound PV and mounts that volume for the pod. A PersistentVolumeClaimVolumeSource is, essentially, a wrapper around another type of volume that is owned by someone else (the system). Name Type Description Required claimName string ClaimName is the name of a PersistentVolumeClaim in the same namespace as the pod using this volume. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims Yes readOnly boolean Will force the ReadOnly setting in VolumeMounts. Default false. No io.k8s.api.core.v1.PhotonPersistentDiskVolumeSource \u00b6 Represents a Photon Controller persistent disk resource. Name Type Description Required fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. No pdID string ID that identifies Photon Controller persistent disk Yes io.k8s.api.core.v1.PodAffinity \u00b6 Pod affinity is a group of inter pod affinity scheduling rules. Name Type Description Required preferredDuringSchedulingIgnoredDuringExecution [ io.k8s.api.core.v1.WeightedPodAffinityTerm ] The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred. No requiredDuringSchedulingIgnoredDuringExecution [ io.k8s.api.core.v1.PodAffinityTerm ] If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied. No io.k8s.api.core.v1.PodAffinityTerm \u00b6 Defines a set of pods (namely those matching the labelSelector relative to the given namespace(s)) that this pod should be co-located (affinity) or not co-located (anti-affinity) with, where co-located is defined as running on a node whose value of the label with key matches that of any node on which a pod of the set of pods is running Name Type Description Required labelSelector io.k8s.apimachinery.pkg.apis.meta.v1.LabelSelector A label query over a set of resources, in this case pods. No namespaces [ string ] namespaces specifies which namespaces the labelSelector applies to (matches against); null or empty list means \"this pod's namespace\" No topologyKey string This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching the labelSelector in the specified namespaces, where co-located is defined as running on a node whose value of the label with key topologyKey matches that of any node on which any of the selected pods is running. Empty topologyKey is not allowed. Yes io.k8s.api.core.v1.PodAntiAffinity \u00b6 Pod anti affinity is a group of inter pod anti affinity scheduling rules. Name Type Description Required preferredDuringSchedulingIgnoredDuringExecution [ io.k8s.api.core.v1.WeightedPodAffinityTerm ] The scheduler will prefer to schedule pods to nodes that satisfy the anti-affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred. No requiredDuringSchedulingIgnoredDuringExecution [ io.k8s.api.core.v1.PodAffinityTerm ] If the anti-affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the anti-affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied. No io.k8s.api.core.v1.PodDNSConfig \u00b6 PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. Name Type Description Required nameservers [ string ] A list of DNS name server IP addresses. This will be appended to the base nameservers generated from DNSPolicy. Duplicated nameservers will be removed. No options [ io.k8s.api.core.v1.PodDNSConfigOption ] A list of DNS resolver options. This will be merged with the base options generated from DNSPolicy. Duplicated entries will be removed. Resolution options given in Options will override those that appear in the base DNSPolicy. No searches [ string ] A list of DNS search domains for host-name lookup. This will be appended to the base search paths generated from DNSPolicy. Duplicated search paths will be removed. No io.k8s.api.core.v1.PodDNSConfigOption \u00b6 PodDNSConfigOption defines DNS resolver options of a pod. Name Type Description Required name string Required. No value string No io.k8s.api.core.v1.PodSecurityContext \u00b6 PodSecurityContext holds pod-level security attributes and common container settings. Some fields are also present in container.securityContext. Field values of container.securityContext take precedence over field values of PodSecurityContext. Name Type Description Required fsGroup long A special supplemental group that applies to all containers in a pod. Some volume types allow the Kubelet to change the ownership of that volume to be owned by the pod: 1. The owning GID will be the FSGroup 2. The setgid bit is set (new files created in the volume will be owned by FSGroup) 3. The permission bits are OR'd with rw-rw---- If unset, the Kubelet will not modify the ownership and permissions of any volume. No runAsGroup long The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. No runAsNonRoot boolean Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. No runAsUser long The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. No seLinuxOptions io.k8s.api.core.v1.SELinuxOptions The SELinux context to be applied to all containers. If unspecified, the container runtime will allocate a random SELinux context for each container. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. No supplementalGroups [ long ] A list of groups applied to the first process run in each container, in addition to the container's primary GID. If unspecified, no groups will be added to any container. No sysctls [ io.k8s.api.core.v1.Sysctl ] Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported sysctls (by the container runtime) might fail to launch. No windowsOptions io.k8s.api.core.v1.WindowsSecurityContextOptions Windows security options. No io.k8s.api.core.v1.PortworxVolumeSource \u00b6 PortworxVolumeSource represents a Portworx volume resource. Name Type Description Required fsType string FSType represents the filesystem type to mount Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\". Implicitly inferred to be \"ext4\" if unspecified. No readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. No volumeID string VolumeID uniquely identifies a Portworx volume Yes io.k8s.api.core.v1.PreferredSchedulingTerm \u00b6 An empty preferred scheduling term matches all objects with implicit weight 0 (i.e. it's a no-op). A null preferred scheduling term matches no objects (i.e. is also a no-op). Name Type Description Required preference io.k8s.api.core.v1.NodeSelectorTerm A node selector term, associated with the corresponding weight. Yes weight integer Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100. Yes io.k8s.api.core.v1.Probe \u00b6 Probe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic. Name Type Description Required exec io.k8s.api.core.v1.ExecAction One and only one of the following should be specified. Exec specifies the action to take. No failureThreshold integer Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1. No httpGet io.k8s.api.core.v1.HTTPGetAction HTTPGet specifies the http request to perform. No initialDelaySeconds integer Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No periodSeconds integer How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. No successThreshold integer Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness. Minimum value is 1. No tcpSocket io.k8s.api.core.v1.TCPSocketAction TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported No timeoutSeconds integer Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No io.k8s.api.core.v1.ProjectedVolumeSource \u00b6 Represents a projected volume source Name Type Description Required defaultMode integer Mode bits to use on created files by default. Must be a value between 0 and 0777. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. No sources [ io.k8s.api.core.v1.VolumeProjection ] list of volume projections Yes io.k8s.api.core.v1.QuobyteVolumeSource \u00b6 Represents a Quobyte mount that lasts the lifetime of a pod. Quobyte volumes do not support ownership management or SELinux relabeling. Name Type Description Required group string Group to map volume access to Default is no group No readOnly boolean ReadOnly here will force the Quobyte volume to be mounted with read-only permissions. Defaults to false. No registry string Registry represents a single or multiple Quobyte Registry services specified as a string as host:port pair (multiple entries are separated with commas) which acts as the central registry for volumes Yes tenant string Tenant owning the given Quobyte volume in the Backend Used with dynamically provisioned Quobyte volumes, value is set by the plugin No user string User to map volume access to Defaults to serivceaccount user No volume string Volume is a string that references an already created Quobyte volume by name. Yes io.k8s.api.core.v1.RBDVolumeSource \u00b6 Represents a Rados Block Device mount that lasts the lifetime of a pod. RBD volumes support ownership management and SELinux relabeling. Name Type Description Required fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#rbd No image string The rados image name. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it Yes keyring string Keyring is the path to key ring for RBDUser. Default is /etc/ceph/keyring. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it No monitors [ string ] A collection of Ceph monitors. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it Yes pool string The rados pool name. Default is rbd. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it No readOnly boolean ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it No secretRef io.k8s.api.core.v1.LocalObjectReference SecretRef is name of the authentication secret for RBDUser. If provided overrides keyring. Default is nil. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it No user string The rados user name. Default is admin. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it No io.k8s.api.core.v1.ResourceFieldSelector \u00b6 ResourceFieldSelector represents container resources (cpu, memory) and their output format Name Type Description Required containerName string Container name: required for volumes, optional for env vars No divisor io.k8s.apimachinery.pkg.api.resource.Quantity Specifies the output format of the exposed resources, defaults to \"1\" No resource string Required: resource to select Yes io.k8s.api.core.v1.ResourceRequirements \u00b6 ResourceRequirements describes the compute resource requirements. Name Type Description Required limits object Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ No requests object Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ No io.k8s.api.core.v1.SELinuxOptions \u00b6 SELinuxOptions are the labels to be applied to the container Name Type Description Required level string Level is SELinux level label that applies to the container. No role string Role is a SELinux role label that applies to the container. No type string Type is a SELinux type label that applies to the container. No user string User is a SELinux user label that applies to the container. No io.k8s.api.core.v1.ScaleIOVolumeSource \u00b6 ScaleIOVolumeSource represents a persistent ScaleIO volume Name Type Description Required fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Default is \"xfs\". No gateway string The host address of the ScaleIO API Gateway. Yes protectionDomain string The name of the ScaleIO Protection Domain for the configured storage. No readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. No secretRef io.k8s.api.core.v1.LocalObjectReference SecretRef references to the secret for ScaleIO user and other sensitive information. If this is not provided, Login operation will fail. Yes sslEnabled boolean Flag to enable/disable SSL communication with Gateway, default false No storageMode string Indicates whether the storage for a volume should be ThickProvisioned or ThinProvisioned. Default is ThinProvisioned. No storagePool string The ScaleIO Storage Pool associated with the protection domain. No system string The name of the storage system as configured in ScaleIO. Yes volumeName string The name of a volume already created in the ScaleIO system that is associated with this volume source. No io.k8s.api.core.v1.SecretEnvSource \u00b6 SecretEnvSource selects a Secret to populate the environment variables with. The contents of the target Secret's Data field will represent the key-value pairs as environment variables. Name Type Description Required name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No optional boolean Specify whether the Secret must be defined No io.k8s.api.core.v1.SecretKeySelector \u00b6 SecretKeySelector selects a key of a Secret. Name Type Description Required key string The key of the secret to select from. Must be a valid secret key. Yes name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No optional boolean Specify whether the Secret or its key must be defined No io.k8s.api.core.v1.SecretProjection \u00b6 Adapts a secret into a projected volume. The contents of the target Secret's Data field will be presented in a projected volume as files using the keys in the Data field as the file names. Note that this is identical to a secret volume source without the default mode. Name Type Description Required items [ io.k8s.api.core.v1.KeyToPath ] If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. No name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No optional boolean Specify whether the Secret or its key must be defined No io.k8s.api.core.v1.SecretVolumeSource \u00b6 Adapts a Secret into a volume. The contents of the target Secret's Data field will be presented in a volume as files using the keys in the Data field as the file names. Secret volumes support ownership management and SELinux relabeling. Name Type Description Required defaultMode integer Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. No items [ io.k8s.api.core.v1.KeyToPath ] If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. No optional boolean Specify whether the Secret or its keys must be defined No secretName string Name of the secret in the pod's namespace to use. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret No io.k8s.api.core.v1.SecurityContext \u00b6 SecurityContext holds security configuration that will be applied to a container. Some fields are present in both SecurityContext and PodSecurityContext. When both are set, the values in SecurityContext take precedence. Name Type Description Required allowPrivilegeEscalation boolean AllowPrivilegeEscalation controls whether a process can gain more privileges than its parent process. This bool directly controls if the no_new_privs flag will be set on the container process. AllowPrivilegeEscalation is true always when the container is: 1) run as Privileged 2) has CAP_SYS_ADMIN No capabilities io.k8s.api.core.v1.Capabilities The capabilities to add/drop when running containers. Defaults to the default set of capabilities granted by the container runtime. No privileged boolean Run container in privileged mode. Processes in privileged containers are essentially equivalent to root on the host. Defaults to false. No procMount string procMount denotes the type of proc mount to use for the containers. The default is DefaultProcMount which uses the container runtime defaults for readonly paths and masked paths. This requires the ProcMountType feature flag to be enabled. No readOnlyRootFilesystem boolean Whether this container has a read-only root filesystem. Default is false. No runAsGroup long The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. No runAsNonRoot boolean Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. No runAsUser long The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. No seLinuxOptions io.k8s.api.core.v1.SELinuxOptions The SELinux context to be applied to the container. If unspecified, the container runtime will allocate a random SELinux context for each container. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. No windowsOptions io.k8s.api.core.v1.WindowsSecurityContextOptions Windows security options. No io.k8s.api.core.v1.ServiceAccountTokenProjection \u00b6 ServiceAccountTokenProjection represents a projected service account token volume. This projection can be used to insert a service account token into the pods runtime filesystem for use against APIs (Kubernetes API Server or otherwise). Name Type Description Required audience string Audience is the intended audience of the token. A recipient of a token must identify itself with an identifier specified in the audience of the token, and otherwise should reject the token. The audience defaults to the identifier of the apiserver. No expirationSeconds long ExpirationSeconds is the requested duration of validity of the service account token. As the token approaches expiration, the kubelet volume plugin will proactively rotate the service account token. The kubelet will start trying to rotate the token if the token is older than 80 percent of its time to live or if the token is older than 24 hours.Defaults to 1 hour and must be at least 10 minutes. No path string Path is the path relative to the mount point of the file to project the token into. Yes io.k8s.api.core.v1.StorageOSVolumeSource \u00b6 Represents a StorageOS persistent volume resource. Name Type Description Required fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. No readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. No secretRef io.k8s.api.core.v1.LocalObjectReference SecretRef specifies the secret to use for obtaining the StorageOS API credentials. If not specified, default values will be attempted. No volumeName string VolumeName is the human-readable name of the StorageOS volume. Volume names are only unique within a namespace. No volumeNamespace string VolumeNamespace specifies the scope of the volume within StorageOS. If no namespace is specified then the Pod's namespace will be used. This allows the Kubernetes name scoping to be mirrored within StorageOS for tighter integration. Set VolumeName to any name to override the default behaviour. Set to \"default\" if you are not using namespaces within StorageOS. Namespaces that do not pre-exist within StorageOS will be created. No io.k8s.api.core.v1.Sysctl \u00b6 Sysctl defines a kernel parameter to be set Name Type Description Required name string Name of a property to set Yes value string Value of a property to set Yes io.k8s.api.core.v1.TCPSocketAction \u00b6 TCPSocketAction describes an action based on opening a socket Name Type Description Required host string Optional: Host name to connect to, defaults to the pod IP. No port io.k8s.apimachinery.pkg.util.intstr.IntOrString Number or name of the port to access on the container. Number must be in the range 1 to 65535. Name must be an IANA_SVC_NAME. Yes io.k8s.api.core.v1.Toleration \u00b6 The pod this Toleration is attached to tolerates any taint that matches the triple using the matching operator . Name Type Description Required effect string Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. No key string Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. No operator string Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. No tolerationSeconds long TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. No value string Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. No io.k8s.api.core.v1.TypedLocalObjectReference \u00b6 TypedLocalObjectReference contains enough information to let you locate the typed referenced object inside the same namespace. Name Type Description Required apiGroup string APIGroup is the group for the resource being referenced. If APIGroup is not specified, the specified Kind must be in the core API group. For any other third-party types, APIGroup is required. No kind string Kind is the type of resource being referenced Yes name string Name is the name of resource being referenced Yes io.k8s.api.core.v1.Volume \u00b6 Volume represents a named volume in a pod that may be accessed by any container in the pod. Name Type Description Required awsElasticBlockStore io.k8s.api.core.v1.AWSElasticBlockStoreVolumeSource AWSElasticBlockStore represents an AWS Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore No azureDisk io.k8s.api.core.v1.AzureDiskVolumeSource AzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod. No azureFile io.k8s.api.core.v1.AzureFileVolumeSource AzureFile represents an Azure File Service mount on the host and bind mount to the pod. No cephfs io.k8s.api.core.v1.CephFSVolumeSource CephFS represents a Ceph FS mount on the host that shares a pod's lifetime No cinder io.k8s.api.core.v1.CinderVolumeSource Cinder represents a cinder volume attached and mounted on kubelets host machine More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md No configMap io.k8s.api.core.v1.ConfigMapVolumeSource ConfigMap represents a configMap that should populate this volume No csi io.k8s.api.core.v1.CSIVolumeSource CSI (Container Storage Interface) represents storage that is handled by an external CSI driver (Alpha feature). No downwardAPI io.k8s.api.core.v1.DownwardAPIVolumeSource DownwardAPI represents downward API about the pod that should populate this volume No emptyDir io.k8s.api.core.v1.EmptyDirVolumeSource EmptyDir represents a temporary directory that shares a pod's lifetime. More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir No fc io.k8s.api.core.v1.FCVolumeSource FC represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod. No flexVolume io.k8s.api.core.v1.FlexVolumeSource FlexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin. No flocker io.k8s.api.core.v1.FlockerVolumeSource Flocker represents a Flocker volume attached to a kubelet's host machine. This depends on the Flocker control service being running No gcePersistentDisk io.k8s.api.core.v1.GCEPersistentDiskVolumeSource GCEPersistentDisk represents a GCE Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk No gitRepo io.k8s.api.core.v1.GitRepoVolumeSource GitRepo represents a git repository at a particular revision. DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir into the Pod's container. No glusterfs io.k8s.api.core.v1.GlusterfsVolumeSource Glusterfs represents a Glusterfs mount on the host that shares a pod's lifetime. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md No hostPath io.k8s.api.core.v1.HostPathVolumeSource HostPath represents a pre-existing file or directory on the host machine that is directly exposed to the container. This is generally used for system agents or other privileged things that are allowed to see the host machine. Most containers will NOT need this. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath No iscsi io.k8s.api.core.v1.ISCSIVolumeSource ISCSI represents an ISCSI Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://releases.k8s.io/HEAD/examples/volumes/iscsi/README.md No name string Volume's name. Must be a DNS_LABEL and unique within the pod. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names Yes nfs io.k8s.api.core.v1.NFSVolumeSource NFS represents an NFS mount on the host that shares a pod's lifetime More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs No persistentVolumeClaim io.k8s.api.core.v1.PersistentVolumeClaimVolumeSource PersistentVolumeClaimVolumeSource represents a reference to a PersistentVolumeClaim in the same namespace. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims No photonPersistentDisk io.k8s.api.core.v1.PhotonPersistentDiskVolumeSource PhotonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine No portworxVolume io.k8s.api.core.v1.PortworxVolumeSource PortworxVolume represents a portworx volume attached and mounted on kubelets host machine No projected io.k8s.api.core.v1.ProjectedVolumeSource Items for all in one resources secrets, configmaps, and downward API No quobyte io.k8s.api.core.v1.QuobyteVolumeSource Quobyte represents a Quobyte mount on the host that shares a pod's lifetime No rbd io.k8s.api.core.v1.RBDVolumeSource RBD represents a Rados Block Device mount on the host that shares a pod's lifetime. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md No scaleIO io.k8s.api.core.v1.ScaleIOVolumeSource ScaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes. No secret io.k8s.api.core.v1.SecretVolumeSource Secret represents a secret that should populate this volume. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret No storageos io.k8s.api.core.v1.StorageOSVolumeSource StorageOS represents a StorageOS volume attached and mounted on Kubernetes nodes. No vsphereVolume io.k8s.api.core.v1.VsphereVirtualDiskVolumeSource VsphereVolume represents a vSphere volume attached and mounted on kubelets host machine No io.k8s.api.core.v1.VolumeDevice \u00b6 volumeDevice describes a mapping of a raw block device within a container. Name Type Description Required devicePath string devicePath is the path inside of the container that the device will be mapped to. Yes name string name must match the name of a persistentVolumeClaim in the pod Yes io.k8s.api.core.v1.VolumeMount \u00b6 VolumeMount describes a mounting of a Volume within a container. Name Type Description Required mountPath string Path within the container at which the volume should be mounted. Must not contain ':'. Yes mountPropagation string mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. This field is beta in 1.10. No name string This must match the Name of a Volume. Yes readOnly boolean Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. No subPath string Path within the volume from which the container's volume should be mounted. Defaults to \"\" (volume's root). No subPathExpr string Expanded path within the volume from which the container's volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment. Defaults to \"\" (volume's root). SubPathExpr and SubPath are mutually exclusive. This field is beta in 1.15. No io.k8s.api.core.v1.VolumeProjection \u00b6 Projection that may be projected along with other supported volume types Name Type Description Required configMap io.k8s.api.core.v1.ConfigMapProjection information about the configMap data to project No downwardAPI io.k8s.api.core.v1.DownwardAPIProjection information about the downwardAPI data to project No secret io.k8s.api.core.v1.SecretProjection information about the secret data to project No serviceAccountToken io.k8s.api.core.v1.ServiceAccountTokenProjection information about the serviceAccountToken data to project No io.k8s.api.core.v1.VsphereVirtualDiskVolumeSource \u00b6 Represents a vSphere volume resource. Name Type Description Required fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. No storagePolicyID string Storage Policy Based Management (SPBM) profile ID associated with the StoragePolicyName. No storagePolicyName string Storage Policy Based Management (SPBM) profile name. No volumePath string Path that identifies vSphere volume vmdk Yes io.k8s.api.core.v1.WeightedPodAffinityTerm \u00b6 The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s) Name Type Description Required podAffinityTerm io.k8s.api.core.v1.PodAffinityTerm Required. A pod affinity term, associated with the corresponding weight. Yes weight integer weight associated with matching the corresponding podAffinityTerm, in the range 1-100. Yes io.k8s.api.core.v1.WindowsSecurityContextOptions \u00b6 WindowsSecurityContextOptions contain Windows-specific options and credentials. Name Type Description Required gmsaCredentialSpec string GMSACredentialSpec is where the GMSA admission webhook (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the GMSA credential spec named by the GMSACredentialSpecName field. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag. No gmsaCredentialSpecName string GMSACredentialSpecName is the name of the GMSA credential spec to use. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag. No io.k8s.api.policy.v1beta1.PodDisruptionBudgetSpec \u00b6 PodDisruptionBudgetSpec is a description of a PodDisruptionBudget. Name Type Description Required maxUnavailable io.k8s.apimachinery.pkg.util.intstr.IntOrString An eviction is allowed if at most \"maxUnavailable\" pods selected by \"selector\" are unavailable after the eviction, i.e. even in absence of the evicted pod. For example, one can prevent all voluntary evictions by specifying 0. This is a mutually exclusive setting with \"minAvailable\". No minAvailable io.k8s.apimachinery.pkg.util.intstr.IntOrString An eviction is allowed if at least \"minAvailable\" pods selected by \"selector\" will still be available after the eviction, i.e. even in the absence of the evicted pod. So for example you can prevent all voluntary evictions by specifying \"100%\". No selector io.k8s.apimachinery.pkg.apis.meta.v1.LabelSelector Label query over pods whose evictions are managed by the disruption budget. No io.k8s.apimachinery.pkg.api.resource.Quantity \u00b6 Quantity is a fixed-point representation of a number. It provides convenient marshaling/unmarshaling in JSON and YAML, in addition to String() and Int64() accessors. The serialization format is: ::= (Note that may be empty, from the \"\" case in .) ::= 0 | 1 | ... | 9 ::= | ::= | . | . | . ::= \"+\" | \"-\" ::= | ::= | | ::= Ki | Mi | Gi | Ti | Pi | Ei (International System of units; See: http://physics.nist.gov/cuu/Units/binary.html) ::= m | \"\" | k | M | G | T | P | E (Note that 1024 = 1Ki but 1000 = 1k; I didn't choose the capitalization.) ::= \"e\" | \"E\" No matter which of the three exponent forms is used, no quantity may represent a number greater than 2^63-1 in magnitude, nor may it have more than 3 decimal places. Numbers larger or more precise will be capped or rounded up. (E.g.: 0.1m will rounded up to 1m.) This may be extended in the future if we require larger or smaller quantities. When a Quantity is parsed from a string, it will remember the type of suffix it had, and will use the same type again when it is serialized. Before serializing, Quantity will be put in \"canonical form\". This means that Exponent/suffix will be adjusted up or down (with a corresponding increase or decrease in Mantissa) such that: a. No precision is lost b. No fractional digits will be emitted c. The exponent (or suffix) is as large as possible. The sign will be omitted unless the number is negative. Examples: 1.5 will be serialized as \"1500m\" 1.5Gi will be serialized as \"1536Mi\" Note that the quantity will NEVER be internally represented by a floating point number. That is the whole point of this exercise. Non-canonical values will still parse as long as they are well formed, but will be re-emitted in their canonical form. (So always use canonical form, or don't diff.) This format is intended to make it difficult to use these numbers without writing some sort of special handling code in the hopes that that will cause implementors to also use a fixed point implementation. Name Type Description Required io.k8s.apimachinery.pkg.api.resource.Quantity string Quantity is a fixed-point representation of a number. It provides convenient marshaling/unmarshaling in JSON and YAML, in addition to String() and Int64() accessors. The serialization format is: ::= (Note that may be empty, from the \"\" case in .) ::= 0 1 io.k8s.apimachinery.pkg.apis.meta.v1.CreateOptions \u00b6 CreateOptions may be provided when creating an API object. Name Type Description Required dryRun [ string ] No fieldManager string No io.k8s.apimachinery.pkg.apis.meta.v1.Fields \u00b6 Fields stores a set of fields in a data structure like a Trie. To understand how this is used, see: https://github.com/kubernetes-sigs/structured-merge-diff Name Type Description Required io.k8s.apimachinery.pkg.apis.meta.v1.Fields object Fields stores a set of fields in a data structure like a Trie. To understand how this is used, see: https://github.com/kubernetes-sigs/structured-merge-diff io.k8s.apimachinery.pkg.apis.meta.v1.Initializer \u00b6 Initializer is information about an initializer that has not yet completed. Name Type Description Required name string name of the process that is responsible for initializing this object. Yes io.k8s.apimachinery.pkg.apis.meta.v1.Initializers \u00b6 Initializers tracks the progress of initialization. Name Type Description Required pending [ io.k8s.apimachinery.pkg.apis.meta.v1.Initializer ] Pending is a list of initializers that must execute in order before this object is visible. When the last pending initializer is removed, and no failing result is set, the initializers struct will be set to nil and the object is considered as initialized and visible to all clients. Yes result io.k8s.apimachinery.pkg.apis.meta.v1.Status If result is set with the Failure field, the object will be persisted to storage and then deleted, ensuring that other clients can observe the deletion. No io.k8s.apimachinery.pkg.apis.meta.v1.LabelSelector \u00b6 A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects. Name Type Description Required matchExpressions [ io.k8s.apimachinery.pkg.apis.meta.v1.LabelSelectorRequirement ] matchExpressions is a list of label selector requirements. The requirements are ANDed. No matchLabels object matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. No io.k8s.apimachinery.pkg.apis.meta.v1.LabelSelectorRequirement \u00b6 A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. Yes operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. Yes values [ string ] values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. No io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta \u00b6 ListMeta describes metadata that synthetic resources must have, including lists and various status objects. A resource may have only one of {ObjectMeta, ListMeta}. Name Type Description Required continue string continue may be set if the user set a limit on the number of items returned, and indicates that the server has more data available. The value is opaque and may be used to issue another request to the endpoint that served this list to retrieve the next set of available objects. Continuing a consistent list may not be possible if the server configuration has changed or more than a few minutes have passed. The resourceVersion field returned when using this continue value will be identical to the value in the first response, unless you have received this token from an error message. No remainingItemCount long remainingItemCount is the number of subsequent items in the list which are not included in this list response. If the list request contained label or field selectors, then the number of remaining items is unknown and the field will be left unset and omitted during serialization. If the list is complete (either because it is not chunking or because this is the last chunk), then there are no more remaining items and this field will be left unset and omitted during serialization. Servers older than v1.15 do not set this field. The intended use of the remainingItemCount is estimating the size of a collection. Clients should not rely on the remainingItemCount to be set or to be exact. This field is alpha and can be changed or removed without notice. No resourceVersion string String that identifies the server's internal version of this object that can be used by clients to determine when objects have changed. Value must be treated as opaque by clients and passed unmodified back to the server. Populated by the system. Read-only. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency No selfLink string selfLink is a URL representing this object. Populated by the system. Read-only. No io.k8s.apimachinery.pkg.apis.meta.v1.ManagedFieldsEntry \u00b6 ManagedFieldsEntry is a workflow-id, a FieldSet and the group version of the resource that the fieldset applies to. Name Type Description Required apiVersion string APIVersion defines the version of this resource that this field set applies to. The format is \"group/version\" just like the top-level APIVersion field. It is necessary to track the version of a field set because it cannot be automatically converted. No fields io.k8s.apimachinery.pkg.apis.meta.v1.Fields Fields identifies a set of fields. No manager string Manager is an identifier of the workflow managing these fields. No operation string Operation is the type of operation which lead to this ManagedFieldsEntry being created. The only valid values for this field are 'Apply' and 'Update'. No time io.k8s.apimachinery.pkg.apis.meta.v1.Time Time is timestamp of when these fields were set. It should always be empty if Operation is 'Apply' No io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta \u00b6 ObjectMeta is metadata that all persisted resources must have, which includes all objects users must create. Name Type Description Required annotations object Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations No clusterName string The name of the cluster which the object belongs to. This is used to distinguish resources with same name and namespace in different clusters. This field is not set anywhere right now and apiserver is going to ignore it if set in create or update request. No creationTimestamp io.k8s.apimachinery.pkg.apis.meta.v1.Time CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC. Populated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata No deletionGracePeriodSeconds long Number of seconds allowed for this object to gracefully terminate before it will be removed from the system. Only set when deletionTimestamp is also set. May only be shortened. Read-only. No deletionTimestamp io.k8s.apimachinery.pkg.apis.meta.v1.Time DeletionTimestamp is RFC 3339 date and time at which this resource will be deleted. This field is set by the server when a graceful deletion is requested by the user, and is not directly settable by a client. The resource is expected to be deleted (no longer visible from resource lists, and not reachable by name) after the time in this field, once the finalizers list is empty. As long as the finalizers list contains items, deletion is blocked. Once the deletionTimestamp is set, this value may not be unset or be set further into the future, although it may be shortened or the resource may be deleted prior to this time. For example, a user may request that a pod is deleted in 30 seconds. The Kubelet will react by sending a graceful termination signal to the containers in the pod. After that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL) to the container and after cleanup, remove the pod from the API. In the presence of network partitions, this object may still exist after this timestamp, until an administrator or automated process can determine the resource is fully terminated. If not set, graceful deletion of the object has not been requested. Populated by the system when a graceful deletion is requested. Read-only. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata No finalizers [ string ] Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. No generateName string GenerateName is an optional prefix, used by the server, to generate a unique name ONLY IF the Name field has not been provided. If this field is used, the name returned to the client will be different than the name passed. This value will also be combined with a unique suffix. The provided value has the same validation rules as the Name field, and may be truncated by the length of the suffix required to make the value unique on the server. If this field is specified and the generated name exists, the server will NOT return a 409 - instead, it will either return 201 Created or 500 with Reason ServerTimeout indicating a unique name could not be found in the time allotted, and the client should retry (optionally after the time indicated in the Retry-After header). Applied only if Name is not specified. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#idempotency No generation long A sequence number representing a specific generation of the desired state. Populated by the system. Read-only. No initializers io.k8s.apimachinery.pkg.apis.meta.v1.Initializers An initializer is a controller which enforces some system invariant at object creation time. This field is a list of initializers that have not yet acted on this object. If nil or empty, this object has been completely initialized. Otherwise, the object is considered uninitialized and is hidden (in list/watch and get calls) from clients that haven't explicitly asked to observe uninitialized objects. When an object is created, the system will populate this list with the current set of initializers. Only privileged users may set or modify this list. Once it is empty, it may not be modified further by any user. DEPRECATED - initializers are an alpha field and will be removed in v1.15. No labels object Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels No managedFields [ io.k8s.apimachinery.pkg.apis.meta.v1.ManagedFieldsEntry ] ManagedFields maps workflow-id and version to the set of fields that are managed by that workflow. This is mostly for internal housekeeping, and users typically shouldn't need to set or understand this field. A workflow can be the user's name, a controller's name, or the name of a specific apply path like \"ci-cd\". The set of fields is always in the version that the workflow used when modifying the object. This field is alpha and can be changed or removed without notice. No name string Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/identifiers#names No namespace string Namespace defines the space within each name must be unique. An empty namespace is equivalent to the \"default\" namespace, but \"default\" is the canonical representation. Not all objects are required to be scoped to a namespace - the value of this field for those objects will be empty. Must be a DNS_LABEL. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/namespaces No ownerReferences [ io.k8s.apimachinery.pkg.apis.meta.v1.OwnerReference ] List of objects depended by this object. If ALL objects in the list have been deleted, this object will be garbage collected. If this object is managed by a controller, then an entry in this list will point to this controller, with the controller field set to true. There cannot be more than one managing controller. No resourceVersion string An opaque value that represents the internal version of this object that can be used by clients to determine when objects have changed. May be used for optimistic concurrency, change detection, and the watch operation on a resource or set of resources. Clients must treat these values as opaque and passed unmodified back to the server. They may only be valid for a particular resource or set of resources. Populated by the system. Read-only. Value must be treated as opaque by clients and . More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency No selfLink string SelfLink is a URL representing this object. Populated by the system. Read-only. No uid string UID is the unique in time and space value for this object. It is typically generated by the server on successful creation of a resource and is not allowed to change on PUT operations. Populated by the system. Read-only. More info: http://kubernetes.io/docs/user-guide/identifiers#uids No io.k8s.apimachinery.pkg.apis.meta.v1.OwnerReference \u00b6 OwnerReference contains enough information to let you identify an owning object. An owning object must be in the same namespace as the dependent, or be cluster-scoped, so there is no namespace field. Name Type Description Required apiVersion string API version of the referent. Yes blockOwnerDeletion boolean If true, AND if the owner has the \"foregroundDeletion\" finalizer, then the owner cannot be deleted from the key-value store until this reference is removed. Defaults to false. To set this field, a user needs \"delete\" permission of the owner, otherwise 422 (Unprocessable Entity) will be returned. No controller boolean If true, this reference points to the managing controller. No kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds Yes name string Name of the referent. More info: http://kubernetes.io/docs/user-guide/identifiers#names Yes uid string UID of the referent. More info: http://kubernetes.io/docs/user-guide/identifiers#uids Yes io.k8s.apimachinery.pkg.apis.meta.v1.Status \u00b6 Status is a return value for calls that don't return other objects. Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources No code integer Suggested HTTP return code for this status, 0 if not set. No details io.k8s.apimachinery.pkg.apis.meta.v1.StatusDetails Extended data associated with the reason. Each reason may define its own extended details. This field is optional and the data returned is not guaranteed to conform to any schema except that defined by the reason type. No kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds No message string A human-readable description of the status of this operation. No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta Standard list metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds No reason string A machine-readable description of why this operation is in the \"Failure\" status. If this value is empty there is no information available. A Reason clarifies an HTTP status code but does not override it. No status string Status of the operation. One of: \"Success\" or \"Failure\". More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status No io.k8s.apimachinery.pkg.apis.meta.v1.StatusCause \u00b6 StatusCause provides more information about an api.Status failure, including cases when multiple errors are encountered. Name Type Description Required field string The field of the resource that has caused this error, as named by its JSON serialization. May include dot and postfix notation for nested attributes. Arrays are zero-indexed. Fields may appear more than once in an array of causes due to fields having multiple errors. Optional. Examples: \"name\" - the field \"name\" on the current resource \"items[0].name\" - the field \"name\" on the first array entry in \"items\" No message string A human-readable description of the cause of the error. This field may be presented as-is to a reader. No reason string A machine-readable description of the cause of the error. If this value is empty there is no information available. No io.k8s.apimachinery.pkg.apis.meta.v1.StatusDetails \u00b6 StatusDetails is a set of additional properties that MAY be set by the server to provide additional information about a response. The Reason field of a Status object defines what attributes will be set. Clients must ignore fields that do not match the defined type of each attribute, and should assume that any attribute may be empty, invalid, or under defined. Name Type Description Required causes [ io.k8s.apimachinery.pkg.apis.meta.v1.StatusCause ] The Causes array includes more details associated with the StatusReason failure. Not all StatusReasons may provide detailed causes. No group string The group attribute of the resource associated with the status StatusReason. No kind string The kind attribute of the resource associated with the status StatusReason. On some operations may differ from the requested resource Kind. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds No name string The name attribute of the resource associated with the status StatusReason (when there is a single name which can be described). No retryAfterSeconds integer If specified, the time in seconds before the operation should be retried. Some errors may indicate the client must take an alternate action - for those errors this field may indicate how long to wait before taking the alternate action. No uid string UID of the resource. (when there is a single resource which can be described). More info: http://kubernetes.io/docs/user-guide/identifiers#uids No io.k8s.apimachinery.pkg.apis.meta.v1.Time \u00b6 Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON. Wrappers are provided for many of the factory methods that the time package offers. Name Type Description Required io.k8s.apimachinery.pkg.apis.meta.v1.Time string Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON. Wrappers are provided for many of the factory methods that the time package offers. io.k8s.apimachinery.pkg.util.intstr.IntOrString \u00b6 IntOrString is a type that can hold an int32 or a string. When used in JSON or YAML marshalling and unmarshalling, it produces or consumes the inner type. This allows you to have, for example, a JSON field that can accept a name or number. Name Type Description Required io.k8s.apimachinery.pkg.util.intstr.IntOrString string IntOrString is a type that can hold an int32 or a string. When used in JSON or YAML marshalling and unmarshalling, it produces or consumes the inner type. This allows you to have, for example, a JSON field that can accept a name or number.","title":"API Reference"},{"location":"swagger/#argo","text":"Argo","title":"Argo"},{"location":"swagger/#version-latest","text":"","title":"Version: latest"},{"location":"swagger/#security","text":"BearerToken apiKey API Key Description Bearer Token authentication Name authorization In header HTTPBasic basic Basic Description HTTP Basic authentication","title":"Security"},{"location":"swagger/#apiv1archived-workflows","text":"","title":"/api/v1/archived-workflows"},{"location":"swagger/#get","text":"","title":"GET"},{"location":"swagger/#parameters","text":"Name Located in Description Required Schema listOptions.labelSelector query A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. No string listOptions.fieldSelector query A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. No string listOptions.watch query Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. No boolean (boolean) listOptions.allowWatchBookmarks query allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. If the feature gate WatchBookmarks is not enabled in apiserver, this field is ignored. +optional. No boolean (boolean) listOptions.resourceVersion query When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. +optional. No string listOptions.timeoutSeconds query Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. No string (int64) listOptions.limit query limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. No string (int64) listOptions.continue query The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\". This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. No string","title":"Parameters"},{"location":"swagger/#responses","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowList","title":"Responses"},{"location":"swagger/#apiv1archived-workflowsuid","text":"","title":"/api/v1/archived-workflows/{uid}"},{"location":"swagger/#get_1","text":"","title":"GET"},{"location":"swagger/#parameters_1","text":"Name Located in Description Required Schema uid path Yes string","title":"Parameters"},{"location":"swagger/#responses_1","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow","title":"Responses"},{"location":"swagger/#delete","text":"","title":"DELETE"},{"location":"swagger/#parameters_2","text":"Name Located in Description Required Schema uid path Yes string","title":"Parameters"},{"location":"swagger/#responses_2","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.ArchivedWorkflowDeletedResponse","title":"Responses"},{"location":"swagger/#apiv1cluster-workflow-templates","text":"","title":"/api/v1/cluster-workflow-templates"},{"location":"swagger/#get_2","text":"","title":"GET"},{"location":"swagger/#parameters_3","text":"Name Located in Description Required Schema listOptions.labelSelector query A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. No string listOptions.fieldSelector query A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. No string listOptions.watch query Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. No boolean (boolean) listOptions.allowWatchBookmarks query allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. If the feature gate WatchBookmarks is not enabled in apiserver, this field is ignored. +optional. No boolean (boolean) listOptions.resourceVersion query When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. +optional. No string listOptions.timeoutSeconds query Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. No string (int64) listOptions.limit query limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. No string (int64) listOptions.continue query The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\". This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. No string","title":"Parameters"},{"location":"swagger/#responses_3","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateList","title":"Responses"},{"location":"swagger/#post","text":"","title":"POST"},{"location":"swagger/#parameters_4","text":"Name Located in Description Required Schema body body Yes io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateCreateRequest","title":"Parameters"},{"location":"swagger/#responses_4","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate","title":"Responses"},{"location":"swagger/#apiv1cluster-workflow-templateslint","text":"","title":"/api/v1/cluster-workflow-templates/lint"},{"location":"swagger/#post_1","text":"","title":"POST"},{"location":"swagger/#parameters_5","text":"Name Located in Description Required Schema body body Yes io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateLintRequest","title":"Parameters"},{"location":"swagger/#responses_5","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate","title":"Responses"},{"location":"swagger/#apiv1cluster-workflow-templatesname","text":"","title":"/api/v1/cluster-workflow-templates/{name}"},{"location":"swagger/#get_3","text":"","title":"GET"},{"location":"swagger/#parameters_6","text":"Name Located in Description Required Schema name path Yes string getOptions.resourceVersion query When specified: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. No string","title":"Parameters"},{"location":"swagger/#responses_6","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate","title":"Responses"},{"location":"swagger/#put","text":"","title":"PUT"},{"location":"swagger/#parameters_7","text":"Name Located in Description Required Schema name path DEPRECATED: This field is ignored. Yes string body body Yes io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateUpdateRequest","title":"Parameters"},{"location":"swagger/#responses_7","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate","title":"Responses"},{"location":"swagger/#delete_1","text":"","title":"DELETE"},{"location":"swagger/#parameters_8","text":"Name Located in Description Required Schema name path Yes string deleteOptions.gracePeriodSeconds query The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. No string (int64) deleteOptions.preconditions.uid query Specifies the target UID. +optional. No string deleteOptions.preconditions.resourceVersion query Specifies the target ResourceVersion +optional. No string deleteOptions.orphanDependents query Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. No boolean (boolean) deleteOptions.propagationPolicy query Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. No string deleteOptions.dryRun query When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. No [ string ]","title":"Parameters"},{"location":"swagger/#responses_8","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateDeleteResponse","title":"Responses"},{"location":"swagger/#apiv1cron-workflowsnamespace","text":"","title":"/api/v1/cron-workflows/{namespace}"},{"location":"swagger/#get_4","text":"","title":"GET"},{"location":"swagger/#parameters_9","text":"Name Located in Description Required Schema namespace path Yes string listOptions.labelSelector query A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. No string listOptions.fieldSelector query A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. No string listOptions.watch query Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. No boolean (boolean) listOptions.allowWatchBookmarks query allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. If the feature gate WatchBookmarks is not enabled in apiserver, this field is ignored. +optional. No boolean (boolean) listOptions.resourceVersion query When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. +optional. No string listOptions.timeoutSeconds query Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. No string (int64) listOptions.limit query limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. No string (int64) listOptions.continue query The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\". This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. No string","title":"Parameters"},{"location":"swagger/#responses_9","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.CronWorkflowList","title":"Responses"},{"location":"swagger/#post_2","text":"","title":"POST"},{"location":"swagger/#parameters_10","text":"Name Located in Description Required Schema namespace path Yes string body body Yes io.argoproj.workflow.v1alpha1.CreateCronWorkflowRequest","title":"Parameters"},{"location":"swagger/#responses_10","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.CronWorkflow","title":"Responses"},{"location":"swagger/#apiv1cron-workflowsnamespacelint","text":"","title":"/api/v1/cron-workflows/{namespace}/lint"},{"location":"swagger/#post_3","text":"","title":"POST"},{"location":"swagger/#parameters_11","text":"Name Located in Description Required Schema namespace path Yes string body body Yes io.argoproj.workflow.v1alpha1.LintCronWorkflowRequest","title":"Parameters"},{"location":"swagger/#responses_11","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.CronWorkflow","title":"Responses"},{"location":"swagger/#apiv1cron-workflowsnamespacename","text":"","title":"/api/v1/cron-workflows/{namespace}/{name}"},{"location":"swagger/#get_5","text":"","title":"GET"},{"location":"swagger/#parameters_12","text":"Name Located in Description Required Schema namespace path Yes string name path Yes string getOptions.resourceVersion query When specified: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. No string","title":"Parameters"},{"location":"swagger/#responses_12","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.CronWorkflow","title":"Responses"},{"location":"swagger/#put_1","text":"","title":"PUT"},{"location":"swagger/#parameters_13","text":"Name Located in Description Required Schema namespace path Yes string name path DEPRECATED: This field is ignored. Yes string body body Yes io.argoproj.workflow.v1alpha1.UpdateCronWorkflowRequest","title":"Parameters"},{"location":"swagger/#responses_13","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.CronWorkflow","title":"Responses"},{"location":"swagger/#delete_2","text":"","title":"DELETE"},{"location":"swagger/#parameters_14","text":"Name Located in Description Required Schema namespace path Yes string name path Yes string deleteOptions.gracePeriodSeconds query The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. No string (int64) deleteOptions.preconditions.uid query Specifies the target UID. +optional. No string deleteOptions.preconditions.resourceVersion query Specifies the target ResourceVersion +optional. No string deleteOptions.orphanDependents query Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. No boolean (boolean) deleteOptions.propagationPolicy query Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. No string deleteOptions.dryRun query When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. No [ string ]","title":"Parameters"},{"location":"swagger/#responses_14","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.CronWorkflowDeletedResponse","title":"Responses"},{"location":"swagger/#apiv1info","text":"","title":"/api/v1/info"},{"location":"swagger/#get_6","text":"","title":"GET"},{"location":"swagger/#responses_15","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.InfoResponse","title":"Responses"},{"location":"swagger/#apiv1version","text":"","title":"/api/v1/version"},{"location":"swagger/#get_7","text":"","title":"GET"},{"location":"swagger/#responses_16","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Version","title":"Responses"},{"location":"swagger/#apiv1workflow-eventsnamespace","text":"","title":"/api/v1/workflow-events/{namespace}"},{"location":"swagger/#get_8","text":"","title":"GET"},{"location":"swagger/#parameters_15","text":"Name Located in Description Required Schema namespace path Yes string listOptions.labelSelector query A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. No string listOptions.fieldSelector query A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. No string listOptions.watch query Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. No boolean (boolean) listOptions.allowWatchBookmarks query allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. If the feature gate WatchBookmarks is not enabled in apiserver, this field is ignored. +optional. No boolean (boolean) listOptions.resourceVersion query When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. +optional. No string listOptions.timeoutSeconds query Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. No string (int64) listOptions.limit query limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. No string (int64) listOptions.continue query The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\". This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. No string","title":"Parameters"},{"location":"swagger/#responses_17","text":"Code Description Schema 200 A successful response.(streaming responses) object","title":"Responses"},{"location":"swagger/#apiv1workflow-templatesnamespace","text":"","title":"/api/v1/workflow-templates/{namespace}"},{"location":"swagger/#get_9","text":"","title":"GET"},{"location":"swagger/#parameters_16","text":"Name Located in Description Required Schema namespace path Yes string listOptions.labelSelector query A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. No string listOptions.fieldSelector query A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. No string listOptions.watch query Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. No boolean (boolean) listOptions.allowWatchBookmarks query allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. If the feature gate WatchBookmarks is not enabled in apiserver, this field is ignored. +optional. No boolean (boolean) listOptions.resourceVersion query When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. +optional. No string listOptions.timeoutSeconds query Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. No string (int64) listOptions.limit query limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. No string (int64) listOptions.continue query The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\". This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. No string","title":"Parameters"},{"location":"swagger/#responses_18","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowTemplateList","title":"Responses"},{"location":"swagger/#post_4","text":"","title":"POST"},{"location":"swagger/#parameters_17","text":"Name Located in Description Required Schema namespace path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowTemplateCreateRequest","title":"Parameters"},{"location":"swagger/#responses_19","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowTemplate","title":"Responses"},{"location":"swagger/#apiv1workflow-templatesnamespacelint","text":"","title":"/api/v1/workflow-templates/{namespace}/lint"},{"location":"swagger/#post_5","text":"","title":"POST"},{"location":"swagger/#parameters_18","text":"Name Located in Description Required Schema namespace path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowTemplateLintRequest","title":"Parameters"},{"location":"swagger/#responses_20","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowTemplate","title":"Responses"},{"location":"swagger/#apiv1workflow-templatesnamespacename","text":"","title":"/api/v1/workflow-templates/{namespace}/{name}"},{"location":"swagger/#get_10","text":"","title":"GET"},{"location":"swagger/#parameters_19","text":"Name Located in Description Required Schema namespace path Yes string name path Yes string getOptions.resourceVersion query When specified: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. No string","title":"Parameters"},{"location":"swagger/#responses_21","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowTemplate","title":"Responses"},{"location":"swagger/#put_2","text":"","title":"PUT"},{"location":"swagger/#parameters_20","text":"Name Located in Description Required Schema namespace path Yes string name path DEPRECATED: This field is ignored. Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowTemplateUpdateRequest","title":"Parameters"},{"location":"swagger/#responses_22","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowTemplate","title":"Responses"},{"location":"swagger/#delete_3","text":"","title":"DELETE"},{"location":"swagger/#parameters_21","text":"Name Located in Description Required Schema namespace path Yes string name path Yes string deleteOptions.gracePeriodSeconds query The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. No string (int64) deleteOptions.preconditions.uid query Specifies the target UID. +optional. No string deleteOptions.preconditions.resourceVersion query Specifies the target ResourceVersion +optional. No string deleteOptions.orphanDependents query Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. No boolean (boolean) deleteOptions.propagationPolicy query Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. No string deleteOptions.dryRun query When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. No [ string ]","title":"Parameters"},{"location":"swagger/#responses_23","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowTemplateDeleteResponse","title":"Responses"},{"location":"swagger/#apiv1workflowsnamespace","text":"","title":"/api/v1/workflows/{namespace}"},{"location":"swagger/#get_11","text":"","title":"GET"},{"location":"swagger/#parameters_22","text":"Name Located in Description Required Schema namespace path Yes string listOptions.labelSelector query A selector to restrict the list of returned objects by their labels. Defaults to everything. +optional. No string listOptions.fieldSelector query A selector to restrict the list of returned objects by their fields. Defaults to everything. +optional. No string listOptions.watch query Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. +optional. No boolean (boolean) listOptions.allowWatchBookmarks query allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. If the feature gate WatchBookmarks is not enabled in apiserver, this field is ignored. +optional. No boolean (boolean) listOptions.resourceVersion query When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. +optional. No string listOptions.timeoutSeconds query Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity. +optional. No string (int64) listOptions.limit query limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. No string (int64) listOptions.continue query The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\". This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. No string fields query Fields to be included or excluded in the response. e.g. \"items.spec,items.status.phase\", \"-items.status.nodes\". No string","title":"Parameters"},{"location":"swagger/#responses_24","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowList","title":"Responses"},{"location":"swagger/#post_6","text":"","title":"POST"},{"location":"swagger/#parameters_23","text":"Name Located in Description Required Schema namespace path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowCreateRequest","title":"Parameters"},{"location":"swagger/#responses_25","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow","title":"Responses"},{"location":"swagger/#apiv1workflowsnamespacelint","text":"","title":"/api/v1/workflows/{namespace}/lint"},{"location":"swagger/#post_7","text":"","title":"POST"},{"location":"swagger/#parameters_24","text":"Name Located in Description Required Schema namespace path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowLintRequest","title":"Parameters"},{"location":"swagger/#responses_26","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow","title":"Responses"},{"location":"swagger/#apiv1workflowsnamespacesubmit","text":"","title":"/api/v1/workflows/{namespace}/submit"},{"location":"swagger/#post_8","text":"","title":"POST"},{"location":"swagger/#parameters_25","text":"Name Located in Description Required Schema namespace path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowSubmitRequest","title":"Parameters"},{"location":"swagger/#responses_27","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow","title":"Responses"},{"location":"swagger/#apiv1workflowsnamespacename","text":"","title":"/api/v1/workflows/{namespace}/{name}"},{"location":"swagger/#get_12","text":"","title":"GET"},{"location":"swagger/#parameters_26","text":"Name Located in Description Required Schema namespace path Yes string name path Yes string getOptions.resourceVersion query When specified: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv. No string fields query Fields to be included or excluded in the response. e.g. \"spec,status.phase\", \"-status.nodes\". No string","title":"Parameters"},{"location":"swagger/#responses_28","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow","title":"Responses"},{"location":"swagger/#delete_4","text":"","title":"DELETE"},{"location":"swagger/#parameters_27","text":"Name Located in Description Required Schema namespace path Yes string name path Yes string deleteOptions.gracePeriodSeconds query The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. +optional. No string (int64) deleteOptions.preconditions.uid query Specifies the target UID. +optional. No string deleteOptions.preconditions.resourceVersion query Specifies the target ResourceVersion +optional. No string deleteOptions.orphanDependents query Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. +optional. No boolean (boolean) deleteOptions.propagationPolicy query Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. +optional. No string deleteOptions.dryRun query When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed +optional. No [ string ]","title":"Parameters"},{"location":"swagger/#responses_29","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.WorkflowDeleteResponse","title":"Responses"},{"location":"swagger/#apiv1workflowsnamespacenameresubmit","text":"","title":"/api/v1/workflows/{namespace}/{name}/resubmit"},{"location":"swagger/#put_3","text":"","title":"PUT"},{"location":"swagger/#parameters_28","text":"Name Located in Description Required Schema namespace path Yes string name path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowResubmitRequest","title":"Parameters"},{"location":"swagger/#responses_30","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow","title":"Responses"},{"location":"swagger/#apiv1workflowsnamespacenameresume","text":"","title":"/api/v1/workflows/{namespace}/{name}/resume"},{"location":"swagger/#put_4","text":"","title":"PUT"},{"location":"swagger/#parameters_29","text":"Name Located in Description Required Schema namespace path Yes string name path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowResumeRequest","title":"Parameters"},{"location":"swagger/#responses_31","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow","title":"Responses"},{"location":"swagger/#apiv1workflowsnamespacenameretry","text":"","title":"/api/v1/workflows/{namespace}/{name}/retry"},{"location":"swagger/#put_5","text":"","title":"PUT"},{"location":"swagger/#parameters_30","text":"Name Located in Description Required Schema namespace path Yes string name path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowRetryRequest","title":"Parameters"},{"location":"swagger/#responses_32","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow","title":"Responses"},{"location":"swagger/#apiv1workflowsnamespacenamestop","text":"","title":"/api/v1/workflows/{namespace}/{name}/stop"},{"location":"swagger/#put_6","text":"","title":"PUT"},{"location":"swagger/#parameters_31","text":"Name Located in Description Required Schema namespace path Yes string name path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowStopRequest","title":"Parameters"},{"location":"swagger/#responses_33","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow","title":"Responses"},{"location":"swagger/#apiv1workflowsnamespacenamesuspend","text":"","title":"/api/v1/workflows/{namespace}/{name}/suspend"},{"location":"swagger/#put_7","text":"","title":"PUT"},{"location":"swagger/#parameters_32","text":"Name Located in Description Required Schema namespace path Yes string name path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowSuspendRequest","title":"Parameters"},{"location":"swagger/#responses_34","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow","title":"Responses"},{"location":"swagger/#apiv1workflowsnamespacenameterminate","text":"","title":"/api/v1/workflows/{namespace}/{name}/terminate"},{"location":"swagger/#put_8","text":"","title":"PUT"},{"location":"swagger/#parameters_33","text":"Name Located in Description Required Schema namespace path Yes string name path Yes string body body Yes io.argoproj.workflow.v1alpha1.WorkflowTerminateRequest","title":"Parameters"},{"location":"swagger/#responses_35","text":"Code Description Schema 200 A successful response. io.argoproj.workflow.v1alpha1.Workflow","title":"Responses"},{"location":"swagger/#apiv1workflowsnamespacenamepodnamelog","text":"","title":"/api/v1/workflows/{namespace}/{name}/{podName}/log"},{"location":"swagger/#get_13","text":"","title":"GET"},{"location":"swagger/#parameters_34","text":"Name Located in Description Required Schema namespace path Yes string name path Yes string podName path Yes string logOptions.container query The container for which to stream logs. Defaults to only container if there is one container in the pod. +optional. No string logOptions.follow query Follow the log stream of the pod. Defaults to false. +optional. No boolean (boolean) logOptions.previous query Return previous terminated container logs. Defaults to false. +optional. No boolean (boolean) logOptions.sinceSeconds query A relative time in seconds before the current time from which to show logs. If this value precedes the time a pod was started, only logs since the pod start will be returned. If this value is in the future, no logs will be returned. Only one of sinceSeconds or sinceTime may be specified. +optional. No string (int64) logOptions.sinceTime.seconds query Represents seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to 9999-12-31T23:59:59Z inclusive. No string (int64) logOptions.sinceTime.nanos query Non-negative fractions of a second at nanosecond resolution. Negative second values with fractions must still have non-negative nanos values that count forward in time. Must be from 0 to 999,999,999 inclusive. This field may be limited in precision depending on context. No integer logOptions.timestamps query If true, add an RFC3339 or RFC3339Nano timestamp at the beginning of every line of log output. Defaults to false. +optional. No boolean (boolean) logOptions.tailLines query If set, the number of lines from the end of the logs to show. If not specified, logs are shown from the creation of the container or sinceSeconds or sinceTime +optional. No string (int64) logOptions.limitBytes query If set, the number of bytes to read from the server before terminating the log output. This may not display a complete final line of logging, and may return slightly more or slightly less than the specified limit. +optional. No string (int64) logOptions.insecureSkipTLSVerifyBackend query insecureSkipTLSVerifyBackend indicates that the apiserver should not confirm the validity of the serving certificate of the backend it is connecting to. This will make the HTTPS connection between the apiserver and the backend insecure. This means the apiserver cannot verify the log data it is receiving came from the real kubelet. If the kubelet is configured to verify the apiserver's TLS credentials, it does not mean the connection to the real kubelet is vulnerable to a man in the middle attack (e.g. an attacker could not intercept the actual log data coming from the real kubelet). +optional. No boolean (boolean)","title":"Parameters"},{"location":"swagger/#responses_36","text":"Code Description Schema 200 A successful response.(streaming responses) object","title":"Responses"},{"location":"swagger/#models","text":"","title":"Models"},{"location":"swagger/#googleprotobufany","text":"Name Type Description Required type_url string No value byte No","title":"google.protobuf.Any"},{"location":"swagger/#grpcgatewayruntimestreamerror","text":"Name Type Description Required details [ google.protobuf.Any ] No grpc_code integer No http_code integer No http_status string No message string No","title":"grpc.gateway.runtime.StreamError"},{"location":"swagger/#ioargoprojworkflowv1alpha1amount","text":"Amount represent a numeric amount. Name Type Description Required io.argoproj.workflow.v1alpha1.Amount number Amount represent a numeric amount.","title":"io.argoproj.workflow.v1alpha1.Amount"},{"location":"swagger/#ioargoprojworkflowv1alpha1archivestrategy","text":"ArchiveStrategy describes how to archive files/directory when saving artifacts Name Type Description Required none io.argoproj.workflow.v1alpha1.NoneStrategy No tar io.argoproj.workflow.v1alpha1.TarStrategy No","title":"io.argoproj.workflow.v1alpha1.ArchiveStrategy"},{"location":"swagger/#ioargoprojworkflowv1alpha1archivedworkflowdeletedresponse","text":"Name Type Description Required io.argoproj.workflow.v1alpha1.ArchivedWorkflowDeletedResponse object","title":"io.argoproj.workflow.v1alpha1.ArchivedWorkflowDeletedResponse"},{"location":"swagger/#ioargoprojworkflowv1alpha1arguments","text":"Arguments to a template Name Type Description Required artifacts [ io.argoproj.workflow.v1alpha1.Artifact ] Artifacts is the list of artifacts to pass to the template or workflow No parameters [ io.argoproj.workflow.v1alpha1.Parameter ] Parameters is the list of parameters to pass to the template or workflow No","title":"io.argoproj.workflow.v1alpha1.Arguments"},{"location":"swagger/#ioargoprojworkflowv1alpha1artifact","text":"Artifact indicates an artifact to place at a specified path Name Type Description Required archive io.argoproj.workflow.v1alpha1.ArchiveStrategy Archive controls how the artifact will be saved to the artifact repository. No archiveLogs boolean ArchiveLogs indicates if the container logs should be archived No artifactory io.argoproj.workflow.v1alpha1.ArtifactoryArtifact Artifactory contains artifactory artifact location details No from string From allows an artifact to reference an artifact from a previous step No gcs io.argoproj.workflow.v1alpha1.GCSArtifact GCS contains GCS artifact location details No git io.argoproj.workflow.v1alpha1.GitArtifact Git contains git artifact location details No globalName string GlobalName exports an output artifact to the global scope, making it available as '{{io.argoproj.workflow.v1alpha1.outputs.artifacts.XXXX}} and in workflow.status.outputs.artifacts No hdfs io.argoproj.workflow.v1alpha1.HDFSArtifact HDFS contains HDFS artifact location details No http io.argoproj.workflow.v1alpha1.HTTPArtifact HTTP contains HTTP artifact location details No mode integer mode bits to use on this file, must be a value between 0 and 0777 set when loading input artifacts. No name string name of the artifact. must be unique within a template's inputs/outputs. Yes optional boolean Make Artifacts optional, if Artifacts doesn't generate or exist No oss io.argoproj.workflow.v1alpha1.OSSArtifact OSS contains OSS artifact location details No path string Path is the container path to the artifact No raw io.argoproj.workflow.v1alpha1.RawArtifact Raw contains raw artifact location details No s3 io.argoproj.workflow.v1alpha1.S3Artifact S3 contains S3 artifact location details No","title":"io.argoproj.workflow.v1alpha1.Artifact"},{"location":"swagger/#ioargoprojworkflowv1alpha1artifactlocation","text":"ArtifactLocation describes a location for a single or multiple artifacts. It is used as single artifact in the context of inputs/outputs (e.g. outputs.artifacts.artname). It is also used to describe the location of multiple artifacts such as the archive location of a single workflow step, which the executor will use as a default location to store its files. Name Type Description Required archiveLogs boolean ArchiveLogs indicates if the container logs should be archived No artifactory io.argoproj.workflow.v1alpha1.ArtifactoryArtifact Artifactory contains artifactory artifact location details No gcs io.argoproj.workflow.v1alpha1.GCSArtifact GCS contains GCS artifact location details No git io.argoproj.workflow.v1alpha1.GitArtifact Git contains git artifact location details No hdfs io.argoproj.workflow.v1alpha1.HDFSArtifact HDFS contains HDFS artifact location details No http io.argoproj.workflow.v1alpha1.HTTPArtifact HTTP contains HTTP artifact location details No oss io.argoproj.workflow.v1alpha1.OSSArtifact OSS contains OSS artifact location details No raw io.argoproj.workflow.v1alpha1.RawArtifact Raw contains raw artifact location details No s3 io.argoproj.workflow.v1alpha1.S3Artifact S3 contains S3 artifact location details No","title":"io.argoproj.workflow.v1alpha1.ArtifactLocation"},{"location":"swagger/#ioargoprojworkflowv1alpha1artifactrepositoryref","text":"Name Type Description Required configMap string No key string No","title":"io.argoproj.workflow.v1alpha1.ArtifactRepositoryRef"},{"location":"swagger/#ioargoprojworkflowv1alpha1artifactoryartifact","text":"ArtifactoryArtifact is the location of an artifactory artifact Name Type Description Required passwordSecret io.k8s.api.core.v1.SecretKeySelector PasswordSecret is the secret selector to the repository password No url string URL of the artifact Yes usernameSecret io.k8s.api.core.v1.SecretKeySelector UsernameSecret is the secret selector to the repository username No","title":"io.argoproj.workflow.v1alpha1.ArtifactoryArtifact"},{"location":"swagger/#ioargoprojworkflowv1alpha1backoff","text":"Backoff is a backoff strategy to use within retryStrategy Name Type Description Required duration string Duration is the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\") No factor integer Factor is a factor to multiply the base duration after each failed retry No maxDuration string MaxDuration is the maximum amount of time allowed for the backoff strategy No","title":"io.argoproj.workflow.v1alpha1.Backoff"},{"location":"swagger/#ioargoprojworkflowv1alpha1clusterworkflowtemplate","text":"ClusterWorkflowTemplate is the definition of a workflow template resource in cluster scope Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta Yes spec io.argoproj.workflow.v1alpha1.WorkflowTemplateSpec Yes","title":"io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate"},{"location":"swagger/#ioargoprojworkflowv1alpha1clusterworkflowtemplatecreaterequest","text":"Name Type Description Required createOptions io.k8s.apimachinery.pkg.apis.meta.v1.CreateOptions No template io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate No","title":"io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateCreateRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1clusterworkflowtemplatedeleteresponse","text":"Name Type Description Required io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateDeleteResponse object","title":"io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateDeleteResponse"},{"location":"swagger/#ioargoprojworkflowv1alpha1clusterworkflowtemplatelintrequest","text":"Name Type Description Required createOptions io.k8s.apimachinery.pkg.apis.meta.v1.CreateOptions No template io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate No","title":"io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateLintRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1clusterworkflowtemplatelist","text":"ClusterWorkflowTemplateList is list of ClusterWorkflowTemplate resources Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No items [ io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate ] Yes kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta Yes","title":"io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateList"},{"location":"swagger/#ioargoprojworkflowv1alpha1clusterworkflowtemplateupdaterequest","text":"Name Type Description Required name string DEPRECATED: This field is ignored. No template io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplate No","title":"io.argoproj.workflow.v1alpha1.ClusterWorkflowTemplateUpdateRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1condition","text":"Name Type Description Required message string Message is the condition message No status string Status is the status of the condition No type string Type is the type of condition No","title":"io.argoproj.workflow.v1alpha1.Condition"},{"location":"swagger/#ioargoprojworkflowv1alpha1continueon","text":"ContinueOn defines if a workflow should continue even if a task or step fails/errors. It can be specified if the workflow should continue when the pod errors, fails or both. Name Type Description Required error boolean No failed boolean No","title":"io.argoproj.workflow.v1alpha1.ContinueOn"},{"location":"swagger/#ioargoprojworkflowv1alpha1counter","text":"Counter is a Counter prometheus metric Name Type Description Required value string Value is the value of the metric Yes","title":"io.argoproj.workflow.v1alpha1.Counter"},{"location":"swagger/#ioargoprojworkflowv1alpha1createcronworkflowrequest","text":"Name Type Description Required createOptions io.k8s.apimachinery.pkg.apis.meta.v1.CreateOptions No cronWorkflow io.argoproj.workflow.v1alpha1.CronWorkflow No namespace string No","title":"io.argoproj.workflow.v1alpha1.CreateCronWorkflowRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1cronworkflow","text":"CronWorkflow is the definition of a scheduled workflow resource Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta Yes spec io.argoproj.workflow.v1alpha1.CronWorkflowSpec Yes status io.argoproj.workflow.v1alpha1.CronWorkflowStatus No","title":"io.argoproj.workflow.v1alpha1.CronWorkflow"},{"location":"swagger/#ioargoprojworkflowv1alpha1cronworkflowdeletedresponse","text":"Name Type Description Required io.argoproj.workflow.v1alpha1.CronWorkflowDeletedResponse object","title":"io.argoproj.workflow.v1alpha1.CronWorkflowDeletedResponse"},{"location":"swagger/#ioargoprojworkflowv1alpha1cronworkflowlist","text":"CronWorkflowList is list of CronWorkflow resources Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No items [ io.argoproj.workflow.v1alpha1.CronWorkflow ] Yes kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta Yes","title":"io.argoproj.workflow.v1alpha1.CronWorkflowList"},{"location":"swagger/#ioargoprojworkflowv1alpha1cronworkflowspec","text":"CronWorkflowSpec is the specification of a CronWorkflow Name Type Description Required concurrencyPolicy string ConcurrencyPolicy is the K8s-style concurrency policy that will be used No failedJobsHistoryLimit integer FailedJobsHistoryLimit is the number of successful jobs to be kept at a time No schedule string Schedule is a schedule to run the Workflow in Cron format Yes startingDeadlineSeconds long StartingDeadlineSeconds is the K8s-style deadline that will limit the time a CronWorkflow will be run after its original scheduled time if it is missed. No successfulJobsHistoryLimit integer SuccessfulJobsHistoryLimit is the number of successful jobs to be kept at a time No suspend boolean Suspend is a flag that will stop new CronWorkflows from running if set to true No timezone string Timezone is the timezone against which the cron schedule will be calculated, e.g. \"Asia/Tokyo\". Default is machine's local time. No workflowMetadata io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta WorkflowMetadata contains some metadata of the workflow to be run No workflowSpec io.argoproj.workflow.v1alpha1.WorkflowSpec WorkflowSpec is the spec of the workflow to be run Yes","title":"io.argoproj.workflow.v1alpha1.CronWorkflowSpec"},{"location":"swagger/#ioargoprojworkflowv1alpha1cronworkflowstatus","text":"CronWorkflowStatus is the status of a CronWorkflow Name Type Description Required active [ io.k8s.api.core.v1.ObjectReference ] Active is a list of active workflows stemming from this CronWorkflow No conditions [ io.argoproj.workflow.v1alpha1.Condition ] Conditions is a list of conditions the CronWorkflow may have No lastScheduledTime io.k8s.apimachinery.pkg.apis.meta.v1.Time LastScheduleTime is the last time the CronWorkflow was scheduled No","title":"io.argoproj.workflow.v1alpha1.CronWorkflowStatus"},{"location":"swagger/#ioargoprojworkflowv1alpha1dagtask","text":"DAGTask represents a node in the graph during DAG execution Name Type Description Required arguments io.argoproj.workflow.v1alpha1.Arguments Arguments are the parameter and artifact arguments to the template No continueOn io.argoproj.workflow.v1alpha1.ContinueOn ContinueOn makes argo to proceed with the following step even if this step fails. Errors and Failed states can be specified No dependencies [ string ] Dependencies are name of other targets which this depends on No depends string Depends are name of other targets which this depends on No name string Name is the name of the target Yes onExit string OnExit is a template reference which is invoked at the end of the template, irrespective of the success, failure, or error of the primary template. No template string Name of template to execute Yes templateRef io.argoproj.workflow.v1alpha1.TemplateRef TemplateRef is the reference to the template resource to execute. No when string When is an expression in which the task should conditionally execute No withItems [ io.argoproj.workflow.v1alpha1.Item ] WithItems expands a task into multiple parallel tasks from the items in the list No withParam string WithParam expands a task into multiple parallel tasks from the value in the parameter, which is expected to be a JSON list. No withSequence io.argoproj.workflow.v1alpha1.Sequence WithSequence expands a task into a numeric sequence No","title":"io.argoproj.workflow.v1alpha1.DAGTask"},{"location":"swagger/#ioargoprojworkflowv1alpha1dagtemplate","text":"DAGTemplate is a template subtype for directed acyclic graph templates Name Type Description Required failFast boolean This flag is for DAG logic. The DAG logic has a built-in \"fail fast\" feature to stop scheduling new steps, as soon as it detects that one of the DAG nodes is failed. Then it waits until all DAG nodes are completed before failing the DAG itself. The FailFast flag default is true, if set to false, it will allow a DAG to run all branches of the DAG to completion (either success or failure), regardless of the failed outcomes of branches in the DAG. More info and example about this feature at https://github.com/argoproj/argo/issues/1442 No target string Target are one or more names of targets to execute in a DAG No tasks [ io.argoproj.workflow.v1alpha1.DAGTask ] Tasks are a list of DAG tasks Yes","title":"io.argoproj.workflow.v1alpha1.DAGTemplate"},{"location":"swagger/#ioargoprojworkflowv1alpha1executorconfig","text":"ExecutorConfig holds configurations of an executor container. Name Type Description Required serviceAccountName string ServiceAccountName specifies the service account name of the executor container. No","title":"io.argoproj.workflow.v1alpha1.ExecutorConfig"},{"location":"swagger/#ioargoprojworkflowv1alpha1gcsartifact","text":"GCSArtifact is the location of a GCS artifact Name Type Description Required bucket string Bucket is the name of the bucket Yes key string Key is the path in the bucket where the artifact resides Yes serviceAccountKeySecret io.k8s.api.core.v1.SecretKeySelector ServiceAccountKeySecret is the secret selector to the bucket's service account key No","title":"io.argoproj.workflow.v1alpha1.GCSArtifact"},{"location":"swagger/#ioargoprojworkflowv1alpha1gauge","text":"Gauge is a Gauge prometheus metric Name Type Description Required realtime boolean Realtime emits this metric in real time if applicable Yes value string Value is the value of the metric Yes","title":"io.argoproj.workflow.v1alpha1.Gauge"},{"location":"swagger/#ioargoprojworkflowv1alpha1gitartifact","text":"GitArtifact is the location of an git artifact Name Type Description Required depth long Depth specifies clones/fetches should be shallow and include the given number of commits from the branch tip No fetch [ string ] Fetch specifies a number of refs that should be fetched before checkout No insecureIgnoreHostKey boolean InsecureIgnoreHostKey disables SSH strict host key checking during git clone No passwordSecret io.k8s.api.core.v1.SecretKeySelector PasswordSecret is the secret selector to the repository password No repo string Repo is the git repository Yes revision string Revision is the git commit, tag, branch to checkout No sshPrivateKeySecret io.k8s.api.core.v1.SecretKeySelector SSHPrivateKeySecret is the secret selector to the repository ssh private key No usernameSecret io.k8s.api.core.v1.SecretKeySelector UsernameSecret is the secret selector to the repository username No","title":"io.argoproj.workflow.v1alpha1.GitArtifact"},{"location":"swagger/#ioargoprojworkflowv1alpha1hdfsartifact","text":"HDFSArtifact is the location of an HDFS artifact Name Type Description Required addresses [ string ] Addresses is accessible addresses of HDFS name nodes Yes force boolean Force copies a file forcibly even if it exists (default: false) No hdfsUser string HDFSUser is the user to access HDFS file system. It is ignored if either ccache or keytab is used. No krbCCacheSecret io.k8s.api.core.v1.SecretKeySelector KrbCCacheSecret is the secret selector for Kerberos ccache Either ccache or keytab can be set to use Kerberos. No krbConfigConfigMap io.k8s.api.core.v1.ConfigMapKeySelector KrbConfig is the configmap selector for Kerberos config as string It must be set if either ccache or keytab is used. No krbKeytabSecret io.k8s.api.core.v1.SecretKeySelector KrbKeytabSecret is the secret selector for Kerberos keytab Either ccache or keytab can be set to use Kerberos. No krbRealm string KrbRealm is the Kerberos realm used with Kerberos keytab It must be set if keytab is used. No krbServicePrincipalName string KrbServicePrincipalName is the principal name of Kerberos service It must be set if either ccache or keytab is used. No krbUsername string KrbUsername is the Kerberos username used with Kerberos keytab It must be set if keytab is used. No path string Path is a file path in HDFS Yes","title":"io.argoproj.workflow.v1alpha1.HDFSArtifact"},{"location":"swagger/#ioargoprojworkflowv1alpha1httpartifact","text":"HTTPArtifact allows an file served on HTTP to be placed as an input artifact in a container Name Type Description Required url string URL of the artifact Yes","title":"io.argoproj.workflow.v1alpha1.HTTPArtifact"},{"location":"swagger/#ioargoprojworkflowv1alpha1histogram","text":"Histogram is a Histogram prometheus metric Name Type Description Required buckets [ io.argoproj.workflow.v1alpha1.Amount ] Buckets is a list of bucket divisors for the histogram Yes value string Value is the value of the metric Yes","title":"io.argoproj.workflow.v1alpha1.Histogram"},{"location":"swagger/#ioargoprojworkflowv1alpha1inforesponse","text":"Name Type Description Required links [ io.argoproj.workflow.v1alpha1.Link ] No managedNamespace string No","title":"io.argoproj.workflow.v1alpha1.InfoResponse"},{"location":"swagger/#ioargoprojworkflowv1alpha1inputs","text":"Inputs are the mechanism for passing parameters, artifacts, volumes from one template to another Name Type Description Required artifacts [ io.argoproj.workflow.v1alpha1.Artifact ] Artifact are a list of artifacts passed as inputs No parameters [ io.argoproj.workflow.v1alpha1.Parameter ] Parameters are a list of parameters passed as inputs No","title":"io.argoproj.workflow.v1alpha1.Inputs"},{"location":"swagger/#ioargoprojworkflowv1alpha1item","text":"Item expands a single workflow step into multiple parallel steps The value of Item can be a map, string, bool, or number Name Type Description Required io.argoproj.workflow.v1alpha1.Item string,number,boolean,array,object Item expands a single workflow step into multiple parallel steps The value of Item can be a map, string, bool, or number","title":"io.argoproj.workflow.v1alpha1.Item"},{"location":"swagger/#ioargoprojworkflowv1alpha1link","text":"A link to another app. Name Type Description Required name string The name of the link, E.g. \"Workflow Logs\" or \"Pod Logs\" Yes scope string Either \"workflow\" or \"pod\" Yes url string The URL. May contain \"${metadata.namespace}\" and \"${metadata.name}\". Yes","title":"io.argoproj.workflow.v1alpha1.Link"},{"location":"swagger/#ioargoprojworkflowv1alpha1lintcronworkflowrequest","text":"Name Type Description Required cronWorkflow io.argoproj.workflow.v1alpha1.CronWorkflow No namespace string No","title":"io.argoproj.workflow.v1alpha1.LintCronWorkflowRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1logentry","text":"Name Type Description Required content string No podName string No","title":"io.argoproj.workflow.v1alpha1.LogEntry"},{"location":"swagger/#ioargoprojworkflowv1alpha1metadata","text":"Pod metdata Name Type Description Required annotations object No labels object No","title":"io.argoproj.workflow.v1alpha1.Metadata"},{"location":"swagger/#ioargoprojworkflowv1alpha1metriclabel","text":"MetricLabel is a single label for a prometheus metric Name Type Description Required key string Yes value string Yes","title":"io.argoproj.workflow.v1alpha1.MetricLabel"},{"location":"swagger/#ioargoprojworkflowv1alpha1metrics","text":"Metrics are a list of metrics emitted from a Workflow/Template Name Type Description Required prometheus [ io.argoproj.workflow.v1alpha1.Prometheus ] Prometheus is a list of prometheus metrics to be emitted Yes","title":"io.argoproj.workflow.v1alpha1.Metrics"},{"location":"swagger/#ioargoprojworkflowv1alpha1nodestatus","text":"NodeStatus contains status information about an individual node in the workflow Name Type Description Required boundaryID string BoundaryID indicates the node ID of the associated template root node in which this node belongs to No children [ string ] Children is a list of child node IDs No daemoned boolean Daemoned tracks whether or not this node was daemoned and need to be terminated No displayName string DisplayName is a human readable representation of the node. Unique within a template boundary No finishedAt io.k8s.apimachinery.pkg.apis.meta.v1.Time Time at which this node completed No hostNodeName string HostNodeName name of the Kubernetes node on which the Pod is running, if applicable No id string ID is a unique identifier of a node within the worklow It is implemented as a hash of the node name, which makes the ID deterministic Yes inputs io.argoproj.workflow.v1alpha1.Inputs Inputs captures input parameter values and artifact locations supplied to this template invocation No message string A human readable message indicating details about why the node is in this condition. No name string Name is unique name in the node tree used to generate the node ID Yes outboundNodes [ string ] OutboundNodes tracks the node IDs which are considered \"outbound\" nodes to a template invocation. For every invocation of a template, there are nodes which we considered as \"outbound\". Essentially, these are last nodes in the execution sequence to run, before the template is considered completed. These nodes are then connected as parents to a following step. In the case of single pod steps (i.e. container, script, resource templates), this list will be nil since the pod itself is already considered the \"outbound\" node. In the case of DAGs, outbound nodes are the \"target\" tasks (tasks with no children). In the case of steps, outbound nodes are all the containers involved in the last step group. NOTE: since templates are composable, the list of outbound nodes are carried upwards when a DAG/steps template invokes another DAG/steps template. In other words, the outbound nodes of a template, will be a superset of the outbound nodes of its last children. No outputs io.argoproj.workflow.v1alpha1.Outputs Outputs captures output parameter values and artifact locations produced by this template invocation No phase string Phase a simple, high-level summary of where the node is in its lifecycle. Can be used as a state machine. No podIP string PodIP captures the IP of the pod for daemoned steps No resourcesDuration object ResourcesDuration is indicative, but not accurate, resource duration. This is populated when the nodes completes. No startedAt io.k8s.apimachinery.pkg.apis.meta.v1.Time Time at which this node started No storedTemplateID string StoredTemplateID is the ID of stored template. DEPRECATED: This value is not used anymore. No templateName string TemplateName is the template name which this node corresponds to. Not applicable to virtual nodes (e.g. Retry, StepGroup) No templateRef io.argoproj.workflow.v1alpha1.TemplateRef TemplateRef is the reference to the template resource which this node corresponds to. Not applicable to virtual nodes (e.g. Retry, StepGroup) No templateScope string TemplateScope is the template scope in which the template of this node was retrieved. No type string Type indicates type of node Yes workflowTemplateName string WorkflowTemplateName is the WorkflowTemplate resource name on which the resolved template of this node is retrieved. DEPRECATED: This value is not used anymore. No","title":"io.argoproj.workflow.v1alpha1.NodeStatus"},{"location":"swagger/#ioargoprojworkflowv1alpha1nonestrategy","text":"NoneStrategy indicates to skip tar process and upload the files or directory tree as independent files. Note that if the artifact is a directory, the artifact driver must support the ability to save/load the directory appropriately. Name Type Description Required io.argoproj.workflow.v1alpha1.NoneStrategy object NoneStrategy indicates to skip tar process and upload the files or directory tree as independent files. Note that if the artifact is a directory, the artifact driver must support the ability to save/load the directory appropriately.","title":"io.argoproj.workflow.v1alpha1.NoneStrategy"},{"location":"swagger/#ioargoprojworkflowv1alpha1ossartifact","text":"OSSArtifact is the location of an Alibaba Cloud OSS artifact Name Type Description Required accessKeySecret io.k8s.api.core.v1.SecretKeySelector AccessKeySecret is the secret selector to the bucket's access key Yes bucket string Bucket is the name of the bucket Yes endpoint string Endpoint is the hostname of the bucket endpoint Yes key string Key is the path in the bucket where the artifact resides Yes secretKeySecret io.k8s.api.core.v1.SecretKeySelector SecretKeySecret is the secret selector to the bucket's secret key Yes","title":"io.argoproj.workflow.v1alpha1.OSSArtifact"},{"location":"swagger/#ioargoprojworkflowv1alpha1outputs","text":"Outputs hold parameters, artifacts, and results from a step Name Type Description Required artifacts [ io.argoproj.workflow.v1alpha1.Artifact ] Artifacts holds the list of output artifacts produced by a step No exitCode string ExitCode holds the exit code of a script template No parameters [ io.argoproj.workflow.v1alpha1.Parameter ] Parameters holds the list of output parameters produced by a step No result string Result holds the result (stdout) of a script template No","title":"io.argoproj.workflow.v1alpha1.Outputs"},{"location":"swagger/#ioargoprojworkflowv1alpha1parallelsteps","text":"Name Type Description Required io.argoproj.workflow.v1alpha1.ParallelSteps array","title":"io.argoproj.workflow.v1alpha1.ParallelSteps"},{"location":"swagger/#ioargoprojworkflowv1alpha1parameter","text":"Parameter indicate a passed string parameter to a service template with an optional default value Name Type Description Required default io.k8s.apimachinery.pkg.util.intstr.IntOrString Default is the default value to use for an input parameter if a value was not supplied No globalName string GlobalName exports an output parameter to the global scope, making it available as '{{io.argoproj.workflow.v1alpha1.outputs.parameters.XXXX}} and in workflow.status.outputs.parameters No name string Name is the parameter name Yes value io.k8s.apimachinery.pkg.util.intstr.IntOrString Value is the literal value to use for the parameter. If specified in the context of an input parameter, the value takes precedence over any passed values No valueFrom io.argoproj.workflow.v1alpha1.ValueFrom ValueFrom is the source for the output parameter's value No","title":"io.argoproj.workflow.v1alpha1.Parameter"},{"location":"swagger/#ioargoprojworkflowv1alpha1podgc","text":"PodGC describes how to delete completed pods as they complete Name Type Description Required strategy string Strategy is the strategy to use. One of \"OnPodCompletion\", \"OnPodSuccess\", \"OnWorkflowCompletion\", \"OnWorkflowSuccess\" No","title":"io.argoproj.workflow.v1alpha1.PodGC"},{"location":"swagger/#ioargoprojworkflowv1alpha1prometheus","text":"Prometheus is a prometheus metric to be emitted Name Type Description Required counter io.argoproj.workflow.v1alpha1.Counter Counter is a counter metric No gauge io.argoproj.workflow.v1alpha1.Gauge Gauge is a gauge metric No help string Help is a string that describes the metric Yes histogram io.argoproj.workflow.v1alpha1.Histogram Histogram is a histogram metric No labels [ io.argoproj.workflow.v1alpha1.MetricLabel ] Labels is a list of metric labels No name string Name is the name of the metric Yes when string When is a conditional statement that decides when to emit the metric No","title":"io.argoproj.workflow.v1alpha1.Prometheus"},{"location":"swagger/#ioargoprojworkflowv1alpha1rawartifact","text":"RawArtifact allows raw string content to be placed as an artifact in a container Name Type Description Required data string Data is the string contents of the artifact Yes","title":"io.argoproj.workflow.v1alpha1.RawArtifact"},{"location":"swagger/#ioargoprojworkflowv1alpha1resourcetemplate","text":"ResourceTemplate is a template subtype to manipulate kubernetes resources Name Type Description Required action string Action is the action to perform to the resource. Must be one of: get, create, apply, delete, replace, patch Yes failureCondition string FailureCondition is a label selector expression which describes the conditions of the k8s resource in which the step was considered failed No flags [ string ] Flags is a set of additional options passed to kubectl before submitting a resource I.e. to disable resource validation: flags: [ \"--validate=false\" # disable resource validation ] No manifest string Manifest contains the kubernetes manifest No mergeStrategy string MergeStrategy is the strategy used to merge a patch. It defaults to \"strategic\" Must be one of: strategic, merge, json No setOwnerReference boolean SetOwnerReference sets the reference to the workflow on the OwnerReference of generated resource. No successCondition string SuccessCondition is a label selector expression which describes the conditions of the k8s resource in which it is acceptable to proceed to the following step No","title":"io.argoproj.workflow.v1alpha1.ResourceTemplate"},{"location":"swagger/#ioargoprojworkflowv1alpha1retrystrategy","text":"RetryStrategy provides controls on how to retry a workflow step Name Type Description Required backoff io.argoproj.workflow.v1alpha1.Backoff Backoff is a backoff strategy No limit integer Limit is the maximum number of attempts when retrying a container No retryPolicy string RetryPolicy is a policy of NodePhase statuses that will be retried No","title":"io.argoproj.workflow.v1alpha1.RetryStrategy"},{"location":"swagger/#ioargoprojworkflowv1alpha1s3artifact","text":"S3Artifact is the location of an S3 artifact Name Type Description Required accessKeySecret io.k8s.api.core.v1.SecretKeySelector AccessKeySecret is the secret selector to the bucket's access key Yes bucket string Bucket is the name of the bucket Yes endpoint string Endpoint is the hostname of the bucket endpoint Yes insecure boolean Insecure will connect to the service with TLS No key string Key is the key in the bucket where the artifact resides Yes region string Region contains the optional bucket region No roleARN string RoleARN is the Amazon Resource Name (ARN) of the role to assume. No secretKeySecret io.k8s.api.core.v1.SecretKeySelector SecretKeySecret is the secret selector to the bucket's secret key Yes useSDKCreds boolean UseSDKCreds tells the driver to figure out credentials based on sdk defaults. No","title":"io.argoproj.workflow.v1alpha1.S3Artifact"},{"location":"swagger/#ioargoprojworkflowv1alpha1scripttemplate","text":"ScriptTemplate is a template subtype to enable scripting through code steps Name Type Description Required args [ string ] Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell No command [ string ] Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell No env [ io.k8s.api.core.v1.EnvVar ] List of environment variables to set in the container. Cannot be updated. No envFrom [ io.k8s.api.core.v1.EnvFromSource ] List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. No image string Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets. Yes imagePullPolicy string Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images No lifecycle io.k8s.api.core.v1.Lifecycle Actions that the management system should take in response to container lifecycle events. Cannot be updated. No livenessProbe io.k8s.api.core.v1.Probe Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No name string Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated. No ports [ io.k8s.api.core.v1.ContainerPort ] List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \"0.0.0.0\" address inside a container will be accessible from the network. Cannot be updated. No readinessProbe io.k8s.api.core.v1.Probe Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No resources io.k8s.api.core.v1.ResourceRequirements Compute Resources required by this container. Cannot be updated. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ No securityContext io.k8s.api.core.v1.SecurityContext Security options the pod should run with. More info: https://kubernetes.io/docs/concepts/policy/security-context/ More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ No source string Source contains the source code of the script to execute Yes startupProbe io.k8s.api.core.v1.Probe StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state operation. This cannot be updated. This is an alpha feature enabled by the StartupProbe feature flag. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No stdin boolean Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false. No stdinOnce boolean Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false No terminationMessagePath string Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated. No terminationMessagePolicy string Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated. No tty boolean Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false. No volumeDevices [ io.k8s.api.core.v1.VolumeDevice ] volumeDevices is the list of block devices to be used by the container. This is a beta feature. No volumeMounts [ io.k8s.api.core.v1.VolumeMount ] Pod volumes to mount into the container's filesystem. Cannot be updated. No workingDir string Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated. No","title":"io.argoproj.workflow.v1alpha1.ScriptTemplate"},{"location":"swagger/#ioargoprojworkflowv1alpha1sequence","text":"Sequence expands a workflow step into numeric range Name Type Description Required count string Count is number of elements in the sequence (default: 0). Not to be used with end No end string Number at which to end the sequence (default: 0). Not to be used with Count No format string Format is a printf format string to format the value in the sequence No start string Number at which to start the sequence (default: 0) No","title":"io.argoproj.workflow.v1alpha1.Sequence"},{"location":"swagger/#ioargoprojworkflowv1alpha1submitopts","text":"SubmitOpts are workflow submission options Name Type Description Required dryRun boolean DryRun validates the workflow on the client-side without creating it. This option is not supported in API No entryPoint string Entrypoint overrides spec.entrypoint No generateName string GenerateName overrides metadata.generateName No labels string Labels adds to metadata.labels No name string Name overrides metadata.name No ownerReference io.k8s.apimachinery.pkg.apis.meta.v1.OwnerReference OwnerReference creates a metadata.ownerReference No parameterFile string ParameterFile holds a reference to a parameter file. This option is not supported in API No parameters [ string ] Parameters passes input parameters to workflow No serverDryRun boolean ServerDryRun validates the workflow on the server-side without creating it No serviceAccount string ServiceAccount runs all pods in the workflow using specified ServiceAccount. No","title":"io.argoproj.workflow.v1alpha1.SubmitOpts"},{"location":"swagger/#ioargoprojworkflowv1alpha1suspendtemplate","text":"SuspendTemplate is a template subtype to suspend a workflow at a predetermined point in time Name Type Description Required duration string Duration is the seconds to wait before automatically resuming a template No","title":"io.argoproj.workflow.v1alpha1.SuspendTemplate"},{"location":"swagger/#ioargoprojworkflowv1alpha1ttlstrategy","text":"TTLStrategy is the strategy for the time to live depending on if the workflow succeeded or failed Name Type Description Required secondsAfterCompletion integer SecondsAfterCompletion is the number of seconds to live after completion No secondsAfterFailure integer SecondsAfterFailure is the number of seconds to live after failure No secondsAfterSuccess integer SecondsAfterSuccess is the number of seconds to live after success No","title":"io.argoproj.workflow.v1alpha1.TTLStrategy"},{"location":"swagger/#ioargoprojworkflowv1alpha1tarstrategy","text":"TarStrategy will tar and gzip the file or directory when saving Name Type Description Required compressionLevel integer CompressionLevel specifies the gzip compression level to use for the artifact. Defaults to gzip.DefaultCompression. No","title":"io.argoproj.workflow.v1alpha1.TarStrategy"},{"location":"swagger/#ioargoprojworkflowv1alpha1template","text":"Template is a reusable and composable unit of execution in a workflow Name Type Description Required activeDeadlineSeconds long Optional duration in seconds relative to the StartTime that the pod may be active on a node before the system actively tries to terminate the pod; value must be positive integer This field is only applicable to container and script templates. No affinity io.k8s.api.core.v1.Affinity Affinity sets the pod's scheduling constraints Overrides the affinity set at the workflow level (if any) No archiveLocation io.argoproj.workflow.v1alpha1.ArtifactLocation Location in which all files related to the step will be stored (logs, artifacts, etc...). Can be overridden by individual items in Outputs. If omitted, will use the default artifact repository location configured in the controller, appended with the / in the key. No arguments io.argoproj.workflow.v1alpha1.Arguments Arguments hold arguments to the template. DEPRECATED: This field is not used. No automountServiceAccountToken boolean AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false. No container io.k8s.api.core.v1.Container Container is the main container image to run in the pod No daemon boolean Deamon will allow a workflow to proceed to the next step so long as the container reaches readiness No dag io.argoproj.workflow.v1alpha1.DAGTemplate DAG template subtype which runs a DAG No executor io.argoproj.workflow.v1alpha1.ExecutorConfig Executor holds configurations of the executor container. No hostAliases [ io.k8s.api.core.v1.HostAlias ] HostAliases is an optional list of hosts and IPs that will be injected into the pod spec No initContainers [ io.argoproj.workflow.v1alpha1.UserContainer ] InitContainers is a list of containers which run before the main container. No inputs io.argoproj.workflow.v1alpha1.Inputs Inputs describe what inputs parameters and artifacts are supplied to this template No metadata io.argoproj.workflow.v1alpha1.Metadata Metdata sets the pods's metadata, i.e. annotations and labels No metrics io.argoproj.workflow.v1alpha1.Metrics Metrics are a list of metrics emitted from this template No name string Name is the name of the template Yes nodeSelector object NodeSelector is a selector to schedule this step of the workflow to be run on the selected node(s). Overrides the selector set at the workflow level. No outputs io.argoproj.workflow.v1alpha1.Outputs Outputs describe the parameters and artifacts that this template produces No parallelism long Parallelism limits the max total parallel pods that can execute at the same time within the boundaries of this template invocation. If additional steps/dag templates are invoked, the pods created by those templates will not be counted towards this total. No podSpecPatch string PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits). No priority integer Priority to apply to workflow pods. No priorityClassName string PriorityClassName to apply to workflow pods. No resource io.argoproj.workflow.v1alpha1.ResourceTemplate Resource template subtype which can run k8s resources No resubmitPendingPods boolean ResubmitPendingPods is a flag to enable resubmitting pods that remain Pending after initial submission No retryStrategy io.argoproj.workflow.v1alpha1.RetryStrategy RetryStrategy describes how to retry a template when it fails No schedulerName string If specified, the pod will be dispatched by specified scheduler. Or it will be dispatched by workflow scope scheduler if specified. If neither specified, the pod will be dispatched by default scheduler. No script io.argoproj.workflow.v1alpha1.ScriptTemplate Script runs a portion of code against an interpreter No securityContext io.k8s.api.core.v1.PodSecurityContext SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to empty. See type description for default values of each field. No serviceAccountName string ServiceAccountName to apply to workflow pods No sidecars [ io.argoproj.workflow.v1alpha1.UserContainer ] Sidecars is a list of containers which run alongside the main container Sidecars are automatically killed when the main container completes No steps [ io.argoproj.workflow.v1alpha1.ParallelSteps ] Steps define a series of sequential/parallel workflow steps No suspend io.argoproj.workflow.v1alpha1.SuspendTemplate Suspend template subtype which can suspend a workflow when reaching the step No template string Template is the name of the template which is used as the base of this template. DEPRECATED: This field is not used. No templateRef io.argoproj.workflow.v1alpha1.TemplateRef TemplateRef is the reference to the template resource which is used as the base of this template. DEPRECATED: This field is not used. No tolerations [ io.k8s.api.core.v1.Toleration ] Tolerations to apply to workflow pods. No volumes [ io.k8s.api.core.v1.Volume ] Volumes is a list of volumes that can be mounted by containers in a template. No","title":"io.argoproj.workflow.v1alpha1.Template"},{"location":"swagger/#ioargoprojworkflowv1alpha1templateref","text":"TemplateRef is a reference of template resource. Name Type Description Required clusterScope boolean ClusterScope indicates the referred template is cluster scoped (i.e. a ClusterWorkflowTemplate). No name string Name is the resource name of the template. No runtimeResolution boolean RuntimeResolution skips validation at creation time. By enabling this option, you can create the referred workflow template before the actual runtime. No template string Template is the name of referred template in the resource. No","title":"io.argoproj.workflow.v1alpha1.TemplateRef"},{"location":"swagger/#ioargoprojworkflowv1alpha1updatecronworkflowrequest","text":"Name Type Description Required cronWorkflow io.argoproj.workflow.v1alpha1.CronWorkflow No name string DEPRECATED: This field is ignored. No namespace string No","title":"io.argoproj.workflow.v1alpha1.UpdateCronWorkflowRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1usercontainer","text":"UserContainer is a container specified by a user. Name Type Description Required args [ string ] Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell No command [ string ] Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell No env [ io.k8s.api.core.v1.EnvVar ] List of environment variables to set in the container. Cannot be updated. No envFrom [ io.k8s.api.core.v1.EnvFromSource ] List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. No image string Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets. No imagePullPolicy string Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images No lifecycle io.k8s.api.core.v1.Lifecycle Actions that the management system should take in response to container lifecycle events. Cannot be updated. No livenessProbe io.k8s.api.core.v1.Probe Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No mirrorVolumeMounts boolean MirrorVolumeMounts will mount the same volumes specified in the main container to the container (including artifacts), at the same mountPaths. This enables dind daemon to partially see the same filesystem as the main container in order to use features such as docker volume binding No name string Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated. Yes ports [ io.k8s.api.core.v1.ContainerPort ] List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \"0.0.0.0\" address inside a container will be accessible from the network. Cannot be updated. No readinessProbe io.k8s.api.core.v1.Probe Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No resources io.k8s.api.core.v1.ResourceRequirements Compute Resources required by this container. Cannot be updated. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ No securityContext io.k8s.api.core.v1.SecurityContext Security options the pod should run with. More info: https://kubernetes.io/docs/concepts/policy/security-context/ More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ No startupProbe io.k8s.api.core.v1.Probe StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state operation. This cannot be updated. This is an alpha feature enabled by the StartupProbe feature flag. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No stdin boolean Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false. No stdinOnce boolean Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false No terminationMessagePath string Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated. No terminationMessagePolicy string Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated. No tty boolean Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false. No volumeDevices [ io.k8s.api.core.v1.VolumeDevice ] volumeDevices is the list of block devices to be used by the container. This is a beta feature. No volumeMounts [ io.k8s.api.core.v1.VolumeMount ] Pod volumes to mount into the container's filesystem. Cannot be updated. No workingDir string Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated. No","title":"io.argoproj.workflow.v1alpha1.UserContainer"},{"location":"swagger/#ioargoprojworkflowv1alpha1valuefrom","text":"ValueFrom describes a location in which to obtain the value to a parameter Name Type Description Required default io.k8s.apimachinery.pkg.util.intstr.IntOrString Default specifies a value to be used if retrieving the value from the specified source fails No jqFilter string JQFilter expression against the resource object in resource templates No jsonPath string JSONPath of a resource to retrieve an output parameter value from in resource templates No parameter string Parameter reference to a step or dag task in which to retrieve an output parameter value from (e.g. '{{steps.mystep.outputs.myparam}}') No path string Path in the container to retrieve an output parameter value from in container templates No","title":"io.argoproj.workflow.v1alpha1.ValueFrom"},{"location":"swagger/#ioargoprojworkflowv1alpha1version","text":"Name Type Description Required buildDate string Yes compiler string Yes gitCommit string Yes gitTag string Yes gitTreeState string Yes goVersion string Yes platform string Yes version string Yes","title":"io.argoproj.workflow.v1alpha1.Version"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflow","text":"Workflow is the definition of a workflow resource Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta Yes spec io.argoproj.workflow.v1alpha1.WorkflowSpec Yes status io.argoproj.workflow.v1alpha1.WorkflowStatus No","title":"io.argoproj.workflow.v1alpha1.Workflow"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowcreaterequest","text":"Name Type Description Required createOptions io.k8s.apimachinery.pkg.apis.meta.v1.CreateOptions No instanceID string This field is no longer used. No namespace string No serverDryRun boolean (boolean) No workflow io.argoproj.workflow.v1alpha1.Workflow No","title":"io.argoproj.workflow.v1alpha1.WorkflowCreateRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowdeleteresponse","text":"Name Type Description Required io.argoproj.workflow.v1alpha1.WorkflowDeleteResponse object","title":"io.argoproj.workflow.v1alpha1.WorkflowDeleteResponse"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowlintrequest","text":"Name Type Description Required namespace string No workflow io.argoproj.workflow.v1alpha1.Workflow No","title":"io.argoproj.workflow.v1alpha1.WorkflowLintRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowlist","text":"WorkflowList is list of Workflow resources Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No items [ io.argoproj.workflow.v1alpha1.Workflow ] Yes kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta Yes","title":"io.argoproj.workflow.v1alpha1.WorkflowList"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowresubmitrequest","text":"Name Type Description Required memoized boolean (boolean) No name string No namespace string No","title":"io.argoproj.workflow.v1alpha1.WorkflowResubmitRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowresumerequest","text":"Name Type Description Required name string No namespace string No nodeFieldSelector string No","title":"io.argoproj.workflow.v1alpha1.WorkflowResumeRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowretryrequest","text":"Name Type Description Required name string No namespace string No nodeFieldSelector string No restartSuccessful boolean (boolean) No","title":"io.argoproj.workflow.v1alpha1.WorkflowRetryRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowspec","text":"WorkflowSpec is the specification of a Workflow. Name Type Description Required activeDeadlineSeconds long Optional duration in seconds relative to the workflow start time which the workflow is allowed to run before the controller terminates the io.argoproj.workflow.v1alpha1. A value of zero is used to terminate a Running workflow No affinity io.k8s.api.core.v1.Affinity Affinity sets the scheduling constraints for all pods in the io.argoproj.workflow.v1alpha1. Can be overridden by an affinity specified in the template No arguments io.argoproj.workflow.v1alpha1.Arguments Arguments contain the parameters and artifacts sent to the workflow entrypoint Parameters are referencable globally using the 'workflow' variable prefix. e.g. {{io.argoproj.workflow.v1alpha1.parameters.myparam}} No artifactRepositoryRef io.argoproj.workflow.v1alpha1.ArtifactRepositoryRef ArtifactRepositoryRef specifies the configMap name and key containing the artifact repository config. No automountServiceAccountToken boolean AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false. No dnsConfig io.k8s.api.core.v1.PodDNSConfig PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. No dnsPolicy string Set DNS policy for the pod. Defaults to \"ClusterFirst\". Valid values are 'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to 'ClusterFirstWithHostNet'. No entrypoint string Entrypoint is a template reference to the starting point of the io.argoproj.workflow.v1alpha1. No executor io.argoproj.workflow.v1alpha1.ExecutorConfig Executor holds configurations of executor containers of the io.argoproj.workflow.v1alpha1. No hostAliases [ io.k8s.api.core.v1.HostAlias ] No hostNetwork boolean Host networking requested for this workflow pod. Default to false. No imagePullSecrets [ io.k8s.api.core.v1.LocalObjectReference ] ImagePullSecrets is a list of references to secrets in the same namespace to use for pulling any images in pods that reference this ServiceAccount. ImagePullSecrets are distinct from Secrets because Secrets can be mounted in the pod, but ImagePullSecrets are only accessed by the kubelet. More info: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod No metrics io.argoproj.workflow.v1alpha1.Metrics Metrics are a list of metrics emitted from this Workflow No nodeSelector object NodeSelector is a selector which will result in all pods of the workflow to be scheduled on the selected node(s). This is able to be overridden by a nodeSelector specified in the template. No onExit string OnExit is a template reference which is invoked at the end of the workflow, irrespective of the success, failure, or error of the primary io.argoproj.workflow.v1alpha1. No parallelism long Parallelism limits the max total parallel pods that can execute at the same time in a workflow No podDisruptionBudget io.k8s.api.policy.v1beta1.PodDisruptionBudgetSpec PodDisruptionBudget holds the number of concurrent disruptions that you allow for Workflow's Pods. Controller will automatically add the selector with workflow name, if selector is empty. Optional: Defaults to empty. No podGC io.argoproj.workflow.v1alpha1.PodGC PodGC describes the strategy to use when to deleting completed pods No podPriority integer Priority to apply to workflow pods. No podPriorityClassName string PriorityClassName to apply to workflow pods. No podSpecPatch string PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits). No priority integer Priority is used if controller is configured to process limited number of workflows in parallel. Workflows with higher priority are processed first. No schedulerName string Set scheduler name for all pods. Will be overridden if container/script template's scheduler name is set. Default scheduler will be used if neither specified. No securityContext io.k8s.api.core.v1.PodSecurityContext SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to empty. See type description for default values of each field. No serviceAccountName string ServiceAccountName is the name of the ServiceAccount to run all pods of the workflow as. No shutdown string Shutdown will shutdown the workflow according to its ShutdownStrategy No suspend boolean Suspend will suspend the workflow and prevent execution of any future steps in the workflow No templates [ io.argoproj.workflow.v1alpha1.Template ] Templates is a list of workflow templates used in a workflow No tolerations [ io.k8s.api.core.v1.Toleration ] Tolerations to apply to workflow pods. No ttlSecondsAfterFinished integer TTLSecondsAfterFinished limits the lifetime of a Workflow that has finished execution (Succeeded, Failed, Error). If this field is set, once the Workflow finishes, it will be deleted after ttlSecondsAfterFinished expires. If this field is unset, ttlSecondsAfterFinished will not expire. If this field is set to zero, ttlSecondsAfterFinished expires immediately after the Workflow finishes. DEPRECATED: Use TTLStrategy.SecondsAfterCompletion instead. No ttlStrategy io.argoproj.workflow.v1alpha1.TTLStrategy TTLStrategy limits the lifetime of a Workflow that has finished execution depending on if it Succeeded or Failed. If this struct is set, once the Workflow finishes, it will be deleted after the time to live expires. If this field is unset, the controller config map will hold the default values. No volumeClaimTemplates [ io.k8s.api.core.v1.PersistentVolumeClaim ] VolumeClaimTemplates is a list of claims that containers are allowed to reference. The Workflow controller will create the claims at the beginning of the workflow and delete the claims upon completion of the workflow No volumes [ io.k8s.api.core.v1.Volume ] Volumes is a list of volumes that can be mounted by containers in a io.argoproj.workflow.v1alpha1. No workflowTemplateRef io.argoproj.workflow.v1alpha1.WorkflowTemplateRef WorkflowTemplateRef holds a reference to a WorkflowTemplate for execution No","title":"io.argoproj.workflow.v1alpha1.WorkflowSpec"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowstatus","text":"WorkflowStatus contains overall status information about a workflow Name Type Description Required compressedNodes string Compressed and base64 decoded Nodes map No conditions [ io.argoproj.workflow.v1alpha1.Condition ] Conditions is a list of conditions the Workflow may have No finishedAt io.k8s.apimachinery.pkg.apis.meta.v1.Time Time at which this workflow completed No message string A human readable message indicating details about why the workflow is in this condition. No nodes object Nodes is a mapping between a node ID and the node's status. No offloadNodeStatusVersion string Whether on not node status has been offloaded to a database. If exists, then Nodes and CompressedNodes will be empty. This will actually be populated with a hash of the offloaded data. No outputs io.argoproj.workflow.v1alpha1.Outputs Outputs captures output values and artifact locations produced by the workflow via global outputs No persistentVolumeClaims [ io.k8s.api.core.v1.Volume ] PersistentVolumeClaims tracks all PVCs that were created as part of the io.argoproj.workflow.v1alpha1. The contents of this list are drained at the end of the workflow. No phase string Phase a simple, high-level summary of where the workflow is in its lifecycle. No resourcesDuration object ResourcesDuration is the total for the workflow No startedAt io.k8s.apimachinery.pkg.apis.meta.v1.Time Time at which this workflow started No storedTemplates object StoredTemplates is a mapping between a template ref and the node's status. No storedWorkflowTemplateSpec io.argoproj.workflow.v1alpha1.WorkflowSpec StoredWorkflowSpec stores the WorkflowTemplate spec for future execution. No","title":"io.argoproj.workflow.v1alpha1.WorkflowStatus"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowstep","text":"WorkflowStep is a reference to a template to execute in a series of step Name Type Description Required arguments io.argoproj.workflow.v1alpha1.Arguments Arguments hold arguments to the template No continueOn io.argoproj.workflow.v1alpha1.ContinueOn ContinueOn makes argo to proceed with the following step even if this step fails. Errors and Failed states can be specified No name string Name of the step No onExit string OnExit is a template reference which is invoked at the end of the template, irrespective of the success, failure, or error of the primary template. No template string Template is the name of the template to execute as the step No templateRef io.argoproj.workflow.v1alpha1.TemplateRef TemplateRef is the reference to the template resource to execute as the step. No when string When is an expression in which the step should conditionally execute No withItems [ io.argoproj.workflow.v1alpha1.Item ] WithItems expands a step into multiple parallel steps from the items in the list No withParam string WithParam expands a step into multiple parallel steps from the value in the parameter, which is expected to be a JSON list. No withSequence io.argoproj.workflow.v1alpha1.Sequence WithSequence expands a step into a numeric sequence No","title":"io.argoproj.workflow.v1alpha1.WorkflowStep"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowstoprequest","text":"Name Type Description Required message string No name string No namespace string No nodeFieldSelector string No","title":"io.argoproj.workflow.v1alpha1.WorkflowStopRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowsubmitrequest","text":"Name Type Description Required namespace string No resourceKind string No resourceName string No submitOptions io.argoproj.workflow.v1alpha1.SubmitOpts No","title":"io.argoproj.workflow.v1alpha1.WorkflowSubmitRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowsuspendrequest","text":"Name Type Description Required name string No namespace string No","title":"io.argoproj.workflow.v1alpha1.WorkflowSuspendRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowtemplate","text":"WorkflowTemplate is the definition of a workflow template resource Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta Yes spec io.argoproj.workflow.v1alpha1.WorkflowTemplateSpec Yes","title":"io.argoproj.workflow.v1alpha1.WorkflowTemplate"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowtemplatecreaterequest","text":"Name Type Description Required createOptions io.k8s.apimachinery.pkg.apis.meta.v1.CreateOptions No namespace string No template io.argoproj.workflow.v1alpha1.WorkflowTemplate No","title":"io.argoproj.workflow.v1alpha1.WorkflowTemplateCreateRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowtemplatedeleteresponse","text":"Name Type Description Required io.argoproj.workflow.v1alpha1.WorkflowTemplateDeleteResponse object","title":"io.argoproj.workflow.v1alpha1.WorkflowTemplateDeleteResponse"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowtemplatelintrequest","text":"Name Type Description Required createOptions io.k8s.apimachinery.pkg.apis.meta.v1.CreateOptions No namespace string No template io.argoproj.workflow.v1alpha1.WorkflowTemplate No","title":"io.argoproj.workflow.v1alpha1.WorkflowTemplateLintRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowtemplatelist","text":"WorkflowTemplateList is list of WorkflowTemplate resources Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources No items [ io.argoproj.workflow.v1alpha1.WorkflowTemplate ] Yes kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta Yes","title":"io.argoproj.workflow.v1alpha1.WorkflowTemplateList"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowtemplateref","text":"WorkflowTemplateRef is a reference to a WorkflowTemplate resource. Name Type Description Required clusterScope boolean ClusterScope indicates the referred template is cluster scoped (i.e. a ClusterWorkflowTemplate). No name string Name is the resource name of the workflow template. No","title":"io.argoproj.workflow.v1alpha1.WorkflowTemplateRef"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowtemplatespec","text":"WorkflowTemplateSpec is a spec of WorkflowTemplate. Name Type Description Required activeDeadlineSeconds long Optional duration in seconds relative to the workflow start time which the workflow is allowed to run before the controller terminates the io.argoproj.workflow.v1alpha1. A value of zero is used to terminate a Running workflow No affinity io.k8s.api.core.v1.Affinity Affinity sets the scheduling constraints for all pods in the io.argoproj.workflow.v1alpha1. Can be overridden by an affinity specified in the template No arguments io.argoproj.workflow.v1alpha1.Arguments Arguments contain the parameters and artifacts sent to the workflow entrypoint Parameters are referencable globally using the 'workflow' variable prefix. e.g. {{io.argoproj.workflow.v1alpha1.parameters.myparam}} No artifactRepositoryRef io.argoproj.workflow.v1alpha1.ArtifactRepositoryRef ArtifactRepositoryRef specifies the configMap name and key containing the artifact repository config. No automountServiceAccountToken boolean AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false. No dnsConfig io.k8s.api.core.v1.PodDNSConfig PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. No dnsPolicy string Set DNS policy for the pod. Defaults to \"ClusterFirst\". Valid values are 'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to 'ClusterFirstWithHostNet'. No entrypoint string Entrypoint is a template reference to the starting point of the io.argoproj.workflow.v1alpha1. No executor io.argoproj.workflow.v1alpha1.ExecutorConfig Executor holds configurations of executor containers of the io.argoproj.workflow.v1alpha1. No hostAliases [ io.k8s.api.core.v1.HostAlias ] No hostNetwork boolean Host networking requested for this workflow pod. Default to false. No imagePullSecrets [ io.k8s.api.core.v1.LocalObjectReference ] ImagePullSecrets is a list of references to secrets in the same namespace to use for pulling any images in pods that reference this ServiceAccount. ImagePullSecrets are distinct from Secrets because Secrets can be mounted in the pod, but ImagePullSecrets are only accessed by the kubelet. More info: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod No metrics io.argoproj.workflow.v1alpha1.Metrics Metrics are a list of metrics emitted from this Workflow No nodeSelector object NodeSelector is a selector which will result in all pods of the workflow to be scheduled on the selected node(s). This is able to be overridden by a nodeSelector specified in the template. No onExit string OnExit is a template reference which is invoked at the end of the workflow, irrespective of the success, failure, or error of the primary io.argoproj.workflow.v1alpha1. No parallelism long Parallelism limits the max total parallel pods that can execute at the same time in a workflow No podDisruptionBudget io.k8s.api.policy.v1beta1.PodDisruptionBudgetSpec PodDisruptionBudget holds the number of concurrent disruptions that you allow for Workflow's Pods. Controller will automatically add the selector with workflow name, if selector is empty. Optional: Defaults to empty. No podGC io.argoproj.workflow.v1alpha1.PodGC PodGC describes the strategy to use when to deleting completed pods No podPriority integer Priority to apply to workflow pods. No podPriorityClassName string PriorityClassName to apply to workflow pods. No podSpecPatch string PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits). No priority integer Priority is used if controller is configured to process limited number of workflows in parallel. Workflows with higher priority are processed first. No schedulerName string Set scheduler name for all pods. Will be overridden if container/script template's scheduler name is set. Default scheduler will be used if neither specified. No securityContext io.k8s.api.core.v1.PodSecurityContext SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to empty. See type description for default values of each field. No serviceAccountName string ServiceAccountName is the name of the ServiceAccount to run all pods of the workflow as. No shutdown string Shutdown will shutdown the workflow according to its ShutdownStrategy No suspend boolean Suspend will suspend the workflow and prevent execution of any future steps in the workflow No templates [ io.argoproj.workflow.v1alpha1.Template ] Templates is a list of workflow templates used in a workflow No tolerations [ io.k8s.api.core.v1.Toleration ] Tolerations to apply to workflow pods. No ttlSecondsAfterFinished integer TTLSecondsAfterFinished limits the lifetime of a Workflow that has finished execution (Succeeded, Failed, Error). If this field is set, once the Workflow finishes, it will be deleted after ttlSecondsAfterFinished expires. If this field is unset, ttlSecondsAfterFinished will not expire. If this field is set to zero, ttlSecondsAfterFinished expires immediately after the Workflow finishes. DEPRECATED: Use TTLStrategy.SecondsAfterCompletion instead. No ttlStrategy io.argoproj.workflow.v1alpha1.TTLStrategy TTLStrategy limits the lifetime of a Workflow that has finished execution depending on if it Succeeded or Failed. If this struct is set, once the Workflow finishes, it will be deleted after the time to live expires. If this field is unset, the controller config map will hold the default values. No volumeClaimTemplates [ io.k8s.api.core.v1.PersistentVolumeClaim ] VolumeClaimTemplates is a list of claims that containers are allowed to reference. The Workflow controller will create the claims at the beginning of the workflow and delete the claims upon completion of the workflow No volumes [ io.k8s.api.core.v1.Volume ] Volumes is a list of volumes that can be mounted by containers in a io.argoproj.workflow.v1alpha1. No workflowTemplateRef io.argoproj.workflow.v1alpha1.WorkflowTemplateRef WorkflowTemplateRef holds a reference to a WorkflowTemplate for execution No","title":"io.argoproj.workflow.v1alpha1.WorkflowTemplateSpec"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowtemplateupdaterequest","text":"Name Type Description Required name string DEPRECATED: This field is ignored. No namespace string No template io.argoproj.workflow.v1alpha1.WorkflowTemplate No","title":"io.argoproj.workflow.v1alpha1.WorkflowTemplateUpdateRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowterminaterequest","text":"Name Type Description Required name string No namespace string No","title":"io.argoproj.workflow.v1alpha1.WorkflowTerminateRequest"},{"location":"swagger/#ioargoprojworkflowv1alpha1workflowwatchevent","text":"Name Type Description Required object io.argoproj.workflow.v1alpha1.Workflow No type string No","title":"io.argoproj.workflow.v1alpha1.WorkflowWatchEvent"},{"location":"swagger/#iok8sapicorev1awselasticblockstorevolumesource","text":"Represents a Persistent Disk resource in AWS. An AWS EBS disk must exist before mounting to a container. The disk must also be in the same AWS zone as the kubelet. An AWS EBS disk can only be mounted as read/write once. AWS EBS volumes support ownership management and SELinux relabeling. Name Type Description Required fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore No partition integer The partition in the volume that you want to mount. If omitted, the default is to mount by volume name. Examples: For volume /dev/sda1, you specify the partition as \"1\". Similarly, the volume partition for /dev/sda is \"0\" (or you can leave the property empty). No readOnly boolean Specify \"true\" to force and set the ReadOnly property in VolumeMounts to \"true\". If omitted, the default is \"false\". More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore No volumeID string Unique ID of the persistent disk resource in AWS (Amazon EBS volume). More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore Yes","title":"io.k8s.api.core.v1.AWSElasticBlockStoreVolumeSource"},{"location":"swagger/#iok8sapicorev1affinity","text":"Affinity is a group of affinity scheduling rules. Name Type Description Required nodeAffinity io.k8s.api.core.v1.NodeAffinity Describes node affinity scheduling rules for the pod. No podAffinity io.k8s.api.core.v1.PodAffinity Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). No podAntiAffinity io.k8s.api.core.v1.PodAntiAffinity Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). No","title":"io.k8s.api.core.v1.Affinity"},{"location":"swagger/#iok8sapicorev1azurediskvolumesource","text":"AzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod. Name Type Description Required cachingMode string Host Caching mode: None, Read Only, Read Write. No diskName string The Name of the data disk in the blob storage Yes diskURI string The URI the data disk in the blob storage Yes fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. No kind string Expected values Shared: multiple blob disks per storage account Dedicated: single blob disk per storage account Managed: azure managed data disk (only in managed availability set). defaults to shared No readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. No","title":"io.k8s.api.core.v1.AzureDiskVolumeSource"},{"location":"swagger/#iok8sapicorev1azurefilevolumesource","text":"AzureFile represents an Azure File Service mount on the host and bind mount to the pod. Name Type Description Required readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. No secretName string the name of secret that contains Azure Storage Account Name and Key Yes shareName string Share Name Yes","title":"io.k8s.api.core.v1.AzureFileVolumeSource"},{"location":"swagger/#iok8sapicorev1csivolumesource","text":"Represents a source location of a volume to mount, managed by an external CSI driver Name Type Description Required driver string Driver is the name of the CSI driver that handles this volume. Consult with your admin for the correct name as registered in the cluster. Yes fsType string Filesystem type to mount. Ex. \"ext4\", \"xfs\", \"ntfs\". If not provided, the empty value is passed to the associated CSI driver which will determine the default filesystem to apply. No nodePublishSecretRef io.k8s.api.core.v1.LocalObjectReference NodePublishSecretRef is a reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI NodePublishVolume and NodeUnpublishVolume calls. This field is optional, and may be empty if no secret is required. If the secret object contains more than one secret, all secret references are passed. No readOnly boolean Specifies a read-only configuration for the volume. Defaults to false (read/write). No volumeAttributes object VolumeAttributes stores driver-specific properties that are passed to the CSI driver. Consult your driver's documentation for supported values. No","title":"io.k8s.api.core.v1.CSIVolumeSource"},{"location":"swagger/#iok8sapicorev1capabilities","text":"Adds and removes POSIX capabilities from running containers. Name Type Description Required add [ string ] Added capabilities No drop [ string ] Removed capabilities No","title":"io.k8s.api.core.v1.Capabilities"},{"location":"swagger/#iok8sapicorev1cephfsvolumesource","text":"Represents a Ceph Filesystem mount that lasts the lifetime of a pod Cephfs volumes do not support ownership management or SELinux relabeling. Name Type Description Required monitors [ string ] Required: Monitors is a collection of Ceph monitors More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it Yes path string Optional: Used as the mounted root, rather than the full Ceph tree, default is / No readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it No secretFile string Optional: SecretFile is the path to key ring for User, default is /etc/ceph/user.secret More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it No secretRef io.k8s.api.core.v1.LocalObjectReference Optional: SecretRef is reference to the authentication secret for User, default is empty. More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it No user string Optional: User is the rados user name, default is admin More info: https://releases.k8s.io/HEAD/examples/volumes/cephfs/README.md#how-to-use-it No","title":"io.k8s.api.core.v1.CephFSVolumeSource"},{"location":"swagger/#iok8sapicorev1cindervolumesource","text":"Represents a cinder volume resource in Openstack. A Cinder volume must exist before mounting to a container. The volume must also be in the same region as the kubelet. Cinder volumes support ownership management and SELinux relabeling. Name Type Description Required fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md No readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md No secretRef io.k8s.api.core.v1.LocalObjectReference Optional: points to a secret object containing parameters used to connect to OpenStack. No volumeID string volume id used to identify the volume in cinder More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md Yes","title":"io.k8s.api.core.v1.CinderVolumeSource"},{"location":"swagger/#iok8sapicorev1configmapenvsource","text":"ConfigMapEnvSource selects a ConfigMap to populate the environment variables with. The contents of the target ConfigMap's Data field will represent the key-value pairs as environment variables. Name Type Description Required name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No optional boolean Specify whether the ConfigMap must be defined No","title":"io.k8s.api.core.v1.ConfigMapEnvSource"},{"location":"swagger/#iok8sapicorev1configmapkeyselector","text":"Selects a key from a ConfigMap. Name Type Description Required key string The key to select. Yes name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No optional boolean Specify whether the ConfigMap or its key must be defined No","title":"io.k8s.api.core.v1.ConfigMapKeySelector"},{"location":"swagger/#iok8sapicorev1configmapprojection","text":"Adapts a ConfigMap into a projected volume. The contents of the target ConfigMap's Data field will be presented in a projected volume as files using the keys in the Data field as the file names, unless the items element is populated with specific mappings of keys to paths. Note that this is identical to a configmap volume source without the default mode. Name Type Description Required items [ io.k8s.api.core.v1.KeyToPath ] If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. No name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No optional boolean Specify whether the ConfigMap or its keys must be defined No","title":"io.k8s.api.core.v1.ConfigMapProjection"},{"location":"swagger/#iok8sapicorev1configmapvolumesource","text":"Adapts a ConfigMap into a volume. The contents of the target ConfigMap's Data field will be presented in a volume as files using the keys in the Data field as the file names, unless the items element is populated with specific mappings of keys to paths. ConfigMap volumes support ownership management and SELinux relabeling. Name Type Description Required defaultMode integer Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. No items [ io.k8s.api.core.v1.KeyToPath ] If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. No name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No optional boolean Specify whether the ConfigMap or its keys must be defined No","title":"io.k8s.api.core.v1.ConfigMapVolumeSource"},{"location":"swagger/#iok8sapicorev1container","text":"A single application container that you want to run within a pod. Name Type Description Required args [ string ] Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell No command [ string ] Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell No env [ io.k8s.api.core.v1.EnvVar ] List of environment variables to set in the container. Cannot be updated. No envFrom [ io.k8s.api.core.v1.EnvFromSource ] List of sources to populate environment variables in the container. The keys defined within a source must be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a key exists in multiple sources, the value associated with the last source will take precedence. Values defined by an Env with a duplicate key will take precedence. Cannot be updated. No image string Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets. Yes imagePullPolicy string Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise. Cannot be updated. More info: https://kubernetes.io/docs/concepts/containers/images#updating-images No lifecycle io.k8s.api.core.v1.Lifecycle Actions that the management system should take in response to container lifecycle events. Cannot be updated. No livenessProbe io.k8s.api.core.v1.Probe Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No name string Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated. No ports [ io.k8s.api.core.v1.ContainerPort ] List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \"0.0.0.0\" address inside a container will be accessible from the network. Cannot be updated. No readinessProbe io.k8s.api.core.v1.Probe Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No resources io.k8s.api.core.v1.ResourceRequirements Compute Resources required by this container. Cannot be updated. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ No securityContext io.k8s.api.core.v1.SecurityContext Security options the pod should run with. More info: https://kubernetes.io/docs/concepts/policy/security-context/ More info: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ No stdin boolean Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF. Default is false. No stdinOnce boolean Whether the container runtime should close the stdin channel after it has been opened by a single attach. When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and then remains open and accepts data until the client disconnects, at which time stdin is closed and remains closed until the container is restarted. If this flag is false, a container processes that reads from stdin will never receive an EOF. Default is false No terminationMessagePath string Optional: Path at which the file to which the container's termination message will be written is mounted into the container's filesystem. Message written is intended to be brief final status, such as an assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be updated. No terminationMessagePolicy string Indicate how the termination message should be populated. File will use the contents of terminationMessagePath to populate the container status message on both success and failure. FallbackToLogsOnError will use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller. Defaults to File. Cannot be updated. No tty boolean Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is false. No volumeDevices [ io.k8s.api.core.v1.VolumeDevice ] volumeDevices is the list of block devices to be used by the container. This is a beta feature. No volumeMounts [ io.k8s.api.core.v1.VolumeMount ] Pod volumes to mount into the container's filesystem. Cannot be updated. No workingDir string Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image. Cannot be updated. No","title":"io.k8s.api.core.v1.Container"},{"location":"swagger/#iok8sapicorev1containerport","text":"ContainerPort represents a network port in a single container. Name Type Description Required containerPort integer Number of port to expose on the pod's IP address. This must be a valid port number, 0 < x < 65536. Yes hostIP string What host IP to bind the external port to. No hostPort integer Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. If HostNetwork is specified, this must match ContainerPort. Most containers do not need this. No name string If specified, this must be an IANA_SVC_NAME and unique within the pod. Each named port in a pod must have a unique name. Name for the port that can be referred to by services. No protocol string Protocol for port. Must be UDP, TCP, or SCTP. Defaults to \"TCP\". No","title":"io.k8s.api.core.v1.ContainerPort"},{"location":"swagger/#iok8sapicorev1downwardapiprojection","text":"Represents downward API info for projecting into a projected volume. Note that this is identical to a downwardAPI volume source without the default mode. Name Type Description Required items [ io.k8s.api.core.v1.DownwardAPIVolumeFile ] Items is a list of DownwardAPIVolume file No","title":"io.k8s.api.core.v1.DownwardAPIProjection"},{"location":"swagger/#iok8sapicorev1downwardapivolumefile","text":"DownwardAPIVolumeFile represents information to create the file containing the pod field Name Type Description Required fieldRef io.k8s.api.core.v1.ObjectFieldSelector Required: Selects a field of the pod: only annotations, labels, name and namespace are supported. No mode integer Optional: mode bits to use on this file, must be a value between 0 and 0777. If not specified, the volume defaultMode will be used. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. No path string Required: Path is the relative path name of the file to be created. Must not be absolute or contain the '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..' Yes resourceFieldRef io.k8s.api.core.v1.ResourceFieldSelector Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported. No","title":"io.k8s.api.core.v1.DownwardAPIVolumeFile"},{"location":"swagger/#iok8sapicorev1downwardapivolumesource","text":"DownwardAPIVolumeSource represents a volume containing downward API info. Downward API volumes support ownership management and SELinux relabeling. Name Type Description Required defaultMode integer Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. No items [ io.k8s.api.core.v1.DownwardAPIVolumeFile ] Items is a list of downward API volume file No","title":"io.k8s.api.core.v1.DownwardAPIVolumeSource"},{"location":"swagger/#iok8sapicorev1emptydirvolumesource","text":"Represents an empty directory for a pod. Empty directory volumes support ownership management and SELinux relabeling. Name Type Description Required medium string What type of storage medium should back this directory. The default is \"\" which means to use the node's default medium. Must be an empty string (default) or Memory. More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir No sizeLimit io.k8s.apimachinery.pkg.api.resource.Quantity Total amount of local storage required for this EmptyDir volume. The size limit is also applicable for memory medium. The maximum usage on memory medium EmptyDir would be the minimum value between the SizeLimit specified here and the sum of memory limits of all containers in a pod. The default is nil which means that the limit is undefined. More info: http://kubernetes.io/docs/user-guide/volumes#emptydir No","title":"io.k8s.api.core.v1.EmptyDirVolumeSource"},{"location":"swagger/#iok8sapicorev1envfromsource","text":"EnvFromSource represents the source of a set of ConfigMaps Name Type Description Required configMapRef io.k8s.api.core.v1.ConfigMapEnvSource The ConfigMap to select from No prefix string An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER. No secretRef io.k8s.api.core.v1.SecretEnvSource The Secret to select from No","title":"io.k8s.api.core.v1.EnvFromSource"},{"location":"swagger/#iok8sapicorev1envvar","text":"EnvVar represents an environment variable present in a Container. Name Type Description Required name string Name of the environment variable. Must be a C_IDENTIFIER. Yes value string Variable references $(VAR_NAME) are expanded using the previous defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to \"\". No valueFrom io.k8s.api.core.v1.EnvVarSource Source for the environment variable's value. Cannot be used if value is not empty. No","title":"io.k8s.api.core.v1.EnvVar"},{"location":"swagger/#iok8sapicorev1envvarsource","text":"EnvVarSource represents a source for the value of an EnvVar. Name Type Description Required configMapKeyRef io.k8s.api.core.v1.ConfigMapKeySelector Selects a key of a ConfigMap. No fieldRef io.k8s.api.core.v1.ObjectFieldSelector Selects a field of the pod: supports metadata.name, metadata.namespace, metadata.labels, metadata.annotations, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP. No resourceFieldRef io.k8s.api.core.v1.ResourceFieldSelector Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. No secretKeyRef io.k8s.api.core.v1.SecretKeySelector Selects a key of a secret in the pod's namespace No","title":"io.k8s.api.core.v1.EnvVarSource"},{"location":"swagger/#iok8sapicorev1execaction","text":"ExecAction describes a \"run in container\" action. Name Type Description Required command [ string ] Command is the command line to execute inside the container, the working directory for the command is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions (' ', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.","title":"io.k8s.api.core.v1.ExecAction"},{"location":"swagger/#iok8sapicorev1fcvolumesource","text":"Represents a Fibre Channel volume. Fibre Channel volumes can only be mounted as read/write once. Fibre Channel volumes support ownership management and SELinux relabeling. Name Type Description Required fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. No lun integer Optional: FC target lun number No readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. No targetWWNs [ string ] Optional: FC target worldwide names (WWNs) No wwids [ string ] Optional: FC volume world wide identifiers (wwids) Either wwids or combination of targetWWNs and lun must be set, but not both simultaneously. No","title":"io.k8s.api.core.v1.FCVolumeSource"},{"location":"swagger/#iok8sapicorev1flexvolumesource","text":"FlexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin. Name Type Description Required driver string Driver is the name of the driver to use for this volume. Yes fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". The default filesystem depends on FlexVolume script. No options object Optional: Extra command options if any. No readOnly boolean Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. No secretRef io.k8s.api.core.v1.LocalObjectReference Optional: SecretRef is reference to the secret object containing sensitive information to pass to the plugin scripts. This may be empty if no secret object is specified. If the secret object contains more than one secret, all secrets are passed to the plugin scripts. No","title":"io.k8s.api.core.v1.FlexVolumeSource"},{"location":"swagger/#iok8sapicorev1flockervolumesource","text":"Represents a Flocker volume mounted by the Flocker agent. One and only one of datasetName and datasetUUID should be set. Flocker volumes do not support ownership management or SELinux relabeling. Name Type Description Required datasetName string Name of the dataset stored as metadata -> name on the dataset for Flocker should be considered as deprecated No datasetUUID string UUID of the dataset. This is unique identifier of a Flocker dataset No","title":"io.k8s.api.core.v1.FlockerVolumeSource"},{"location":"swagger/#iok8sapicorev1gcepersistentdiskvolumesource","text":"Represents a Persistent Disk resource in Google Compute Engine. A GCE PD must exist before mounting to a container. The disk must also be in the same GCE project and zone as the kubelet. A GCE PD can only be mounted as read/write once or read-only many times. GCE PDs support ownership management and SELinux relabeling. Name Type Description Required fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk No partition integer The partition in the volume that you want to mount. If omitted, the default is to mount by volume name. Examples: For volume /dev/sda1, you specify the partition as \"1\". Similarly, the volume partition for /dev/sda is \"0\" (or you can leave the property empty). More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk No pdName string Unique name of the PD resource in GCE. Used to identify the disk in GCE. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk Yes readOnly boolean ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk No","title":"io.k8s.api.core.v1.GCEPersistentDiskVolumeSource"},{"location":"swagger/#iok8sapicorev1gitrepovolumesource","text":"Represents a volume that is populated with the contents of a git repository. Git repo volumes do not support ownership management. Git repo volumes support SELinux relabeling. DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir into the Pod's container. Name Type Description Required directory string Target directory name. Must not contain or start with '..'. If '.' is supplied, the volume directory will be the git repository. Otherwise, if specified, the volume will contain the git repository in the subdirectory with the given name. No repository string Repository URL Yes revision string Commit hash for the specified revision. No","title":"io.k8s.api.core.v1.GitRepoVolumeSource"},{"location":"swagger/#iok8sapicorev1glusterfsvolumesource","text":"Represents a Glusterfs mount that lasts the lifetime of a pod. Glusterfs volumes do not support ownership management or SELinux relabeling. Name Type Description Required endpoints string EndpointsName is the endpoint name that details Glusterfs topology. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md#create-a-pod Yes path string Path is the Glusterfs volume path. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md#create-a-pod Yes readOnly boolean ReadOnly here will force the Glusterfs volume to be mounted with read-only permissions. Defaults to false. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md#create-a-pod No","title":"io.k8s.api.core.v1.GlusterfsVolumeSource"},{"location":"swagger/#iok8sapicorev1httpgetaction","text":"HTTPGetAction describes an action based on HTTP Get requests. Name Type Description Required host string Host name to connect to, defaults to the pod IP. You probably want to set \"Host\" in httpHeaders instead. No httpHeaders [ io.k8s.api.core.v1.HTTPHeader ] Custom headers to set in the request. HTTP allows repeated headers. No path string Path to access on the HTTP server. No port io.k8s.apimachinery.pkg.util.intstr.IntOrString Name or number of the port to access on the container. Number must be in the range 1 to 65535. Name must be an IANA_SVC_NAME. Yes scheme string Scheme to use for connecting to the host. Defaults to HTTP. No","title":"io.k8s.api.core.v1.HTTPGetAction"},{"location":"swagger/#iok8sapicorev1httpheader","text":"HTTPHeader describes a custom header to be used in HTTP probes Name Type Description Required name string The header field name Yes value string The header field value Yes","title":"io.k8s.api.core.v1.HTTPHeader"},{"location":"swagger/#iok8sapicorev1handler","text":"Handler defines a specific action that should be taken Name Type Description Required exec io.k8s.api.core.v1.ExecAction One and only one of the following should be specified. Exec specifies the action to take. No httpGet io.k8s.api.core.v1.HTTPGetAction HTTPGet specifies the http request to perform. No tcpSocket io.k8s.api.core.v1.TCPSocketAction TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported No","title":"io.k8s.api.core.v1.Handler"},{"location":"swagger/#iok8sapicorev1hostalias","text":"HostAlias holds the mapping between IP and hostnames that will be injected as an entry in the pod's hosts file. Name Type Description Required hostnames [ string ] Hostnames for the above IP address. No ip string IP address of the host file entry. No","title":"io.k8s.api.core.v1.HostAlias"},{"location":"swagger/#iok8sapicorev1hostpathvolumesource","text":"Represents a host path mapped into a pod. Host path volumes do not support ownership management or SELinux relabeling. Name Type Description Required path string Path of the directory on the host. If the path is a symlink, it will follow the link to the real path. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath Yes type string Type for HostPath Volume Defaults to \"\" More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath No","title":"io.k8s.api.core.v1.HostPathVolumeSource"},{"location":"swagger/#iok8sapicorev1iscsivolumesource","text":"Represents an ISCSI disk. ISCSI volumes can only be mounted as read/write once. ISCSI volumes support ownership management and SELinux relabeling. Name Type Description Required chapAuthDiscovery boolean whether support iSCSI Discovery CHAP authentication No chapAuthSession boolean whether support iSCSI Session CHAP authentication No fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#iscsi No initiatorName string Custom iSCSI Initiator Name. If initiatorName is specified with iscsiInterface simultaneously, new iSCSI interface : will be created for the connection. No iqn string Target iSCSI Qualified Name. Yes iscsiInterface string iSCSI Interface Name that uses an iSCSI transport. Defaults to 'default' (tcp). No lun integer iSCSI Target Lun number. Yes portals [ string ] iSCSI Target Portal List. The portal is either an IP or ip_addr:port if the port is other than default (typically TCP ports 860 and 3260). No readOnly boolean ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. No secretRef io.k8s.api.core.v1.LocalObjectReference CHAP Secret for iSCSI target and initiator authentication No targetPortal string iSCSI Target Portal. The Portal is either an IP or ip_addr:port if the port is other than default (typically TCP ports 860 and 3260). Yes","title":"io.k8s.api.core.v1.ISCSIVolumeSource"},{"location":"swagger/#iok8sapicorev1keytopath","text":"Maps a string key to a path within a volume. Name Type Description Required key string The key to project. Yes mode integer Optional: mode bits to use on this file, must be a value between 0 and 0777. If not specified, the volume defaultMode will be used. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. No path string The relative path of the file to map the key to. May not be an absolute path. May not contain the path element '..'. May not start with the string '..'. Yes","title":"io.k8s.api.core.v1.KeyToPath"},{"location":"swagger/#iok8sapicorev1lifecycle","text":"Lifecycle describes actions that the management system should take in response to container lifecycle events. For the PostStart and PreStop lifecycle handlers, management of the container blocks until the action is complete, unless the container process fails, in which case the handler is aborted. Name Type Description Required postStart io.k8s.api.core.v1.Handler PostStart is called immediately after a container is created. If the handler fails, the container is terminated and restarted according to its restart policy. Other management of the container blocks until the hook completes. More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks No preStop io.k8s.api.core.v1.Handler PreStop is called immediately before a container is terminated due to an API request or management event such as liveness probe failure, preemption, resource contention, etc. The handler is not called if the container crashes or exits. The reason for termination is passed to the handler. The Pod's termination grace period countdown begins before the PreStop hooked is executed. Regardless of the outcome of the handler, the container will eventually terminate within the Pod's termination grace period. Other management of the container blocks until the hook completes or until the termination grace period is reached. More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks No","title":"io.k8s.api.core.v1.Lifecycle"},{"location":"swagger/#iok8sapicorev1localobjectreference","text":"LocalObjectReference contains enough information to let you locate the referenced object inside the same namespace. Name Type Description Required name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No","title":"io.k8s.api.core.v1.LocalObjectReference"},{"location":"swagger/#iok8sapicorev1nfsvolumesource","text":"Represents an NFS mount that lasts the lifetime of a pod. NFS volumes do not support ownership management or SELinux relabeling. Name Type Description Required path string Path that is exported by the NFS server. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs Yes readOnly boolean ReadOnly here will force the NFS export to be mounted with read-only permissions. Defaults to false. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs No server string Server is the hostname or IP address of the NFS server. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs Yes","title":"io.k8s.api.core.v1.NFSVolumeSource"},{"location":"swagger/#iok8sapicorev1nodeaffinity","text":"Node affinity is a group of node affinity scheduling rules. Name Type Description Required preferredDuringSchedulingIgnoredDuringExecution [ io.k8s.api.core.v1.PreferredSchedulingTerm ] The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node matches the corresponding matchExpressions; the node(s) with the highest sum are the most preferred. No requiredDuringSchedulingIgnoredDuringExecution io.k8s.api.core.v1.NodeSelector If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to an update), the system may or may not try to eventually evict the pod from its node. No","title":"io.k8s.api.core.v1.NodeAffinity"},{"location":"swagger/#iok8sapicorev1nodeselector","text":"A node selector represents the union of the results of one or more label queries over a set of nodes; that is, it represents the OR of the selectors represented by the node selector terms. Name Type Description Required nodeSelectorTerms [ io.k8s.api.core.v1.NodeSelectorTerm ] Required. A list of node selector terms. The terms are ORed. Yes","title":"io.k8s.api.core.v1.NodeSelector"},{"location":"swagger/#iok8sapicorev1nodeselectorrequirement","text":"A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string The label key that the selector applies to. Yes operator string Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt. Yes values [ string ] An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the values array must have a single element, which will be interpreted as an integer. This array is replaced during a strategic merge patch. No","title":"io.k8s.api.core.v1.NodeSelectorRequirement"},{"location":"swagger/#iok8sapicorev1nodeselectorterm","text":"A null or empty node selector term matches no objects. The requirements of them are ANDed. The TopologySelectorTerm type implements a subset of the NodeSelectorTerm. Name Type Description Required matchExpressions [ io.k8s.api.core.v1.NodeSelectorRequirement ] A list of node selector requirements by node's labels. No matchFields [ io.k8s.api.core.v1.NodeSelectorRequirement ] A list of node selector requirements by node's fields. No","title":"io.k8s.api.core.v1.NodeSelectorTerm"},{"location":"swagger/#iok8sapicorev1objectfieldselector","text":"ObjectFieldSelector selects an APIVersioned field of an object. Name Type Description Required apiVersion string Version of the schema the FieldPath is written in terms of, defaults to \"v1\". No fieldPath string Path of the field to select in the specified API version. Yes","title":"io.k8s.api.core.v1.ObjectFieldSelector"},{"location":"swagger/#iok8sapicorev1objectreference","text":"ObjectReference contains enough information to let you inspect or modify the referred object. Name Type Description Required apiVersion string API version of the referent. No fieldPath string If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. No kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds No name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No namespace string Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ No resourceVersion string Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency No uid string UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids No","title":"io.k8s.api.core.v1.ObjectReference"},{"location":"swagger/#iok8sapicorev1persistentvolumeclaim","text":"PersistentVolumeClaim is a user's request for and claim to a persistent volume Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources No kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta Standard object's metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata No spec io.k8s.api.core.v1.PersistentVolumeClaimSpec Spec defines the desired characteristics of a volume requested by a pod author. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims No status io.k8s.api.core.v1.PersistentVolumeClaimStatus Status represents the current information/status of a persistent volume claim. Read-only. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims No","title":"io.k8s.api.core.v1.PersistentVolumeClaim"},{"location":"swagger/#iok8sapicorev1persistentvolumeclaimcondition","text":"PersistentVolumeClaimCondition contails details about state of pvc Name Type Description Required lastProbeTime io.k8s.apimachinery.pkg.apis.meta.v1.Time Last time we probed the condition. No lastTransitionTime io.k8s.apimachinery.pkg.apis.meta.v1.Time Last time the condition transitioned from one status to another. No message string Human-readable message indicating details about last transition. No reason string Unique, this should be a short, machine understandable string that gives the reason for condition's last transition. If it reports \"ResizeStarted\" that means the underlying persistent volume is being resized. No status string Yes type string Yes","title":"io.k8s.api.core.v1.PersistentVolumeClaimCondition"},{"location":"swagger/#iok8sapicorev1persistentvolumeclaimspec","text":"PersistentVolumeClaimSpec describes the common attributes of storage devices and allows a Source for provider-specific attributes Name Type Description Required accessModes [ string ] AccessModes contains the desired access modes the volume should have. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1 No dataSource io.k8s.api.core.v1.TypedLocalObjectReference This field requires the VolumeSnapshotDataSource alpha feature gate to be enabled and currently VolumeSnapshot is the only supported data source. If the provisioner can support VolumeSnapshot data source, it will create a new volume and data will be restored to the volume at the same time. If the provisioner does not support VolumeSnapshot data source, volume will not be created and the failure will be reported as an event. In the future, we plan to support more data source types and the behavior of the provisioner may change. No resources io.k8s.api.core.v1.ResourceRequirements Resources represents the minimum resources the volume should have. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources No selector io.k8s.apimachinery.pkg.apis.meta.v1.LabelSelector A label query over volumes to consider for binding. No storageClassName string Name of the StorageClass required by the claim. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1 No volumeMode string volumeMode defines what type of volume is required by the claim. Value of Filesystem is implied when not included in claim spec. This is a beta feature. No volumeName string VolumeName is the binding reference to the PersistentVolume backing this claim. No","title":"io.k8s.api.core.v1.PersistentVolumeClaimSpec"},{"location":"swagger/#iok8sapicorev1persistentvolumeclaimstatus","text":"PersistentVolumeClaimStatus is the current status of a persistent volume claim. Name Type Description Required accessModes [ string ] AccessModes contains the actual access modes the volume backing the PVC has. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1 No capacity object Represents the actual resources of the underlying volume. No conditions [ io.k8s.api.core.v1.PersistentVolumeClaimCondition ] Current Condition of persistent volume claim. If underlying persistent volume is being resized then the Condition will be set to 'ResizeStarted'. No phase string Phase represents the current phase of PersistentVolumeClaim. No","title":"io.k8s.api.core.v1.PersistentVolumeClaimStatus"},{"location":"swagger/#iok8sapicorev1persistentvolumeclaimvolumesource","text":"PersistentVolumeClaimVolumeSource references the user's PVC in the same namespace. This volume finds the bound PV and mounts that volume for the pod. A PersistentVolumeClaimVolumeSource is, essentially, a wrapper around another type of volume that is owned by someone else (the system). Name Type Description Required claimName string ClaimName is the name of a PersistentVolumeClaim in the same namespace as the pod using this volume. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims Yes readOnly boolean Will force the ReadOnly setting in VolumeMounts. Default false. No","title":"io.k8s.api.core.v1.PersistentVolumeClaimVolumeSource"},{"location":"swagger/#iok8sapicorev1photonpersistentdiskvolumesource","text":"Represents a Photon Controller persistent disk resource. Name Type Description Required fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. No pdID string ID that identifies Photon Controller persistent disk Yes","title":"io.k8s.api.core.v1.PhotonPersistentDiskVolumeSource"},{"location":"swagger/#iok8sapicorev1podaffinity","text":"Pod affinity is a group of inter pod affinity scheduling rules. Name Type Description Required preferredDuringSchedulingIgnoredDuringExecution [ io.k8s.api.core.v1.WeightedPodAffinityTerm ] The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred. No requiredDuringSchedulingIgnoredDuringExecution [ io.k8s.api.core.v1.PodAffinityTerm ] If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied. No","title":"io.k8s.api.core.v1.PodAffinity"},{"location":"swagger/#iok8sapicorev1podaffinityterm","text":"Defines a set of pods (namely those matching the labelSelector relative to the given namespace(s)) that this pod should be co-located (affinity) or not co-located (anti-affinity) with, where co-located is defined as running on a node whose value of the label with key matches that of any node on which a pod of the set of pods is running Name Type Description Required labelSelector io.k8s.apimachinery.pkg.apis.meta.v1.LabelSelector A label query over a set of resources, in this case pods. No namespaces [ string ] namespaces specifies which namespaces the labelSelector applies to (matches against); null or empty list means \"this pod's namespace\" No topologyKey string This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching the labelSelector in the specified namespaces, where co-located is defined as running on a node whose value of the label with key topologyKey matches that of any node on which any of the selected pods is running. Empty topologyKey is not allowed. Yes","title":"io.k8s.api.core.v1.PodAffinityTerm"},{"location":"swagger/#iok8sapicorev1podantiaffinity","text":"Pod anti affinity is a group of inter pod anti affinity scheduling rules. Name Type Description Required preferredDuringSchedulingIgnoredDuringExecution [ io.k8s.api.core.v1.WeightedPodAffinityTerm ] The scheduler will prefer to schedule pods to nodes that satisfy the anti-affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred. No requiredDuringSchedulingIgnoredDuringExecution [ io.k8s.api.core.v1.PodAffinityTerm ] If the anti-affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the anti-affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied. No","title":"io.k8s.api.core.v1.PodAntiAffinity"},{"location":"swagger/#iok8sapicorev1poddnsconfig","text":"PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy. Name Type Description Required nameservers [ string ] A list of DNS name server IP addresses. This will be appended to the base nameservers generated from DNSPolicy. Duplicated nameservers will be removed. No options [ io.k8s.api.core.v1.PodDNSConfigOption ] A list of DNS resolver options. This will be merged with the base options generated from DNSPolicy. Duplicated entries will be removed. Resolution options given in Options will override those that appear in the base DNSPolicy. No searches [ string ] A list of DNS search domains for host-name lookup. This will be appended to the base search paths generated from DNSPolicy. Duplicated search paths will be removed. No","title":"io.k8s.api.core.v1.PodDNSConfig"},{"location":"swagger/#iok8sapicorev1poddnsconfigoption","text":"PodDNSConfigOption defines DNS resolver options of a pod. Name Type Description Required name string Required. No value string No","title":"io.k8s.api.core.v1.PodDNSConfigOption"},{"location":"swagger/#iok8sapicorev1podsecuritycontext","text":"PodSecurityContext holds pod-level security attributes and common container settings. Some fields are also present in container.securityContext. Field values of container.securityContext take precedence over field values of PodSecurityContext. Name Type Description Required fsGroup long A special supplemental group that applies to all containers in a pod. Some volume types allow the Kubelet to change the ownership of that volume to be owned by the pod: 1. The owning GID will be the FSGroup 2. The setgid bit is set (new files created in the volume will be owned by FSGroup) 3. The permission bits are OR'd with rw-rw---- If unset, the Kubelet will not modify the ownership and permissions of any volume. No runAsGroup long The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. No runAsNonRoot boolean Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. No runAsUser long The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. No seLinuxOptions io.k8s.api.core.v1.SELinuxOptions The SELinux context to be applied to all containers. If unspecified, the container runtime will allocate a random SELinux context for each container. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. No supplementalGroups [ long ] A list of groups applied to the first process run in each container, in addition to the container's primary GID. If unspecified, no groups will be added to any container. No sysctls [ io.k8s.api.core.v1.Sysctl ] Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported sysctls (by the container runtime) might fail to launch. No windowsOptions io.k8s.api.core.v1.WindowsSecurityContextOptions Windows security options. No","title":"io.k8s.api.core.v1.PodSecurityContext"},{"location":"swagger/#iok8sapicorev1portworxvolumesource","text":"PortworxVolumeSource represents a Portworx volume resource. Name Type Description Required fsType string FSType represents the filesystem type to mount Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\". Implicitly inferred to be \"ext4\" if unspecified. No readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. No volumeID string VolumeID uniquely identifies a Portworx volume Yes","title":"io.k8s.api.core.v1.PortworxVolumeSource"},{"location":"swagger/#iok8sapicorev1preferredschedulingterm","text":"An empty preferred scheduling term matches all objects with implicit weight 0 (i.e. it's a no-op). A null preferred scheduling term matches no objects (i.e. is also a no-op). Name Type Description Required preference io.k8s.api.core.v1.NodeSelectorTerm A node selector term, associated with the corresponding weight. Yes weight integer Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100. Yes","title":"io.k8s.api.core.v1.PreferredSchedulingTerm"},{"location":"swagger/#iok8sapicorev1probe","text":"Probe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic. Name Type Description Required exec io.k8s.api.core.v1.ExecAction One and only one of the following should be specified. Exec specifies the action to take. No failureThreshold integer Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1. No httpGet io.k8s.api.core.v1.HTTPGetAction HTTPGet specifies the http request to perform. No initialDelaySeconds integer Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No periodSeconds integer How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. No successThreshold integer Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness. Minimum value is 1. No tcpSocket io.k8s.api.core.v1.TCPSocketAction TCPSocket specifies an action involving a TCP port. TCP hooks not yet supported No timeoutSeconds integer Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes No","title":"io.k8s.api.core.v1.Probe"},{"location":"swagger/#iok8sapicorev1projectedvolumesource","text":"Represents a projected volume source Name Type Description Required defaultMode integer Mode bits to use on created files by default. Must be a value between 0 and 0777. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. No sources [ io.k8s.api.core.v1.VolumeProjection ] list of volume projections Yes","title":"io.k8s.api.core.v1.ProjectedVolumeSource"},{"location":"swagger/#iok8sapicorev1quobytevolumesource","text":"Represents a Quobyte mount that lasts the lifetime of a pod. Quobyte volumes do not support ownership management or SELinux relabeling. Name Type Description Required group string Group to map volume access to Default is no group No readOnly boolean ReadOnly here will force the Quobyte volume to be mounted with read-only permissions. Defaults to false. No registry string Registry represents a single or multiple Quobyte Registry services specified as a string as host:port pair (multiple entries are separated with commas) which acts as the central registry for volumes Yes tenant string Tenant owning the given Quobyte volume in the Backend Used with dynamically provisioned Quobyte volumes, value is set by the plugin No user string User to map volume access to Defaults to serivceaccount user No volume string Volume is a string that references an already created Quobyte volume by name. Yes","title":"io.k8s.api.core.v1.QuobyteVolumeSource"},{"location":"swagger/#iok8sapicorev1rbdvolumesource","text":"Represents a Rados Block Device mount that lasts the lifetime of a pod. RBD volumes support ownership management and SELinux relabeling. Name Type Description Required fsType string Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#rbd No image string The rados image name. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it Yes keyring string Keyring is the path to key ring for RBDUser. Default is /etc/ceph/keyring. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it No monitors [ string ] A collection of Ceph monitors. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it Yes pool string The rados pool name. Default is rbd. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it No readOnly boolean ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it No secretRef io.k8s.api.core.v1.LocalObjectReference SecretRef is name of the authentication secret for RBDUser. If provided overrides keyring. Default is nil. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it No user string The rados user name. Default is admin. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md#how-to-use-it No","title":"io.k8s.api.core.v1.RBDVolumeSource"},{"location":"swagger/#iok8sapicorev1resourcefieldselector","text":"ResourceFieldSelector represents container resources (cpu, memory) and their output format Name Type Description Required containerName string Container name: required for volumes, optional for env vars No divisor io.k8s.apimachinery.pkg.api.resource.Quantity Specifies the output format of the exposed resources, defaults to \"1\" No resource string Required: resource to select Yes","title":"io.k8s.api.core.v1.ResourceFieldSelector"},{"location":"swagger/#iok8sapicorev1resourcerequirements","text":"ResourceRequirements describes the compute resource requirements. Name Type Description Required limits object Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ No requests object Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ No","title":"io.k8s.api.core.v1.ResourceRequirements"},{"location":"swagger/#iok8sapicorev1selinuxoptions","text":"SELinuxOptions are the labels to be applied to the container Name Type Description Required level string Level is SELinux level label that applies to the container. No role string Role is a SELinux role label that applies to the container. No type string Type is a SELinux type label that applies to the container. No user string User is a SELinux user label that applies to the container. No","title":"io.k8s.api.core.v1.SELinuxOptions"},{"location":"swagger/#iok8sapicorev1scaleiovolumesource","text":"ScaleIOVolumeSource represents a persistent ScaleIO volume Name Type Description Required fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Default is \"xfs\". No gateway string The host address of the ScaleIO API Gateway. Yes protectionDomain string The name of the ScaleIO Protection Domain for the configured storage. No readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. No secretRef io.k8s.api.core.v1.LocalObjectReference SecretRef references to the secret for ScaleIO user and other sensitive information. If this is not provided, Login operation will fail. Yes sslEnabled boolean Flag to enable/disable SSL communication with Gateway, default false No storageMode string Indicates whether the storage for a volume should be ThickProvisioned or ThinProvisioned. Default is ThinProvisioned. No storagePool string The ScaleIO Storage Pool associated with the protection domain. No system string The name of the storage system as configured in ScaleIO. Yes volumeName string The name of a volume already created in the ScaleIO system that is associated with this volume source. No","title":"io.k8s.api.core.v1.ScaleIOVolumeSource"},{"location":"swagger/#iok8sapicorev1secretenvsource","text":"SecretEnvSource selects a Secret to populate the environment variables with. The contents of the target Secret's Data field will represent the key-value pairs as environment variables. Name Type Description Required name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No optional boolean Specify whether the Secret must be defined No","title":"io.k8s.api.core.v1.SecretEnvSource"},{"location":"swagger/#iok8sapicorev1secretkeyselector","text":"SecretKeySelector selects a key of a Secret. Name Type Description Required key string The key of the secret to select from. Must be a valid secret key. Yes name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No optional boolean Specify whether the Secret or its key must be defined No","title":"io.k8s.api.core.v1.SecretKeySelector"},{"location":"swagger/#iok8sapicorev1secretprojection","text":"Adapts a secret into a projected volume. The contents of the target Secret's Data field will be presented in a projected volume as files using the keys in the Data field as the file names. Note that this is identical to a secret volume source without the default mode. Name Type Description Required items [ io.k8s.api.core.v1.KeyToPath ] If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. No name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names No optional boolean Specify whether the Secret or its key must be defined No","title":"io.k8s.api.core.v1.SecretProjection"},{"location":"swagger/#iok8sapicorev1secretvolumesource","text":"Adapts a Secret into a volume. The contents of the target Secret's Data field will be presented in a volume as files using the keys in the Data field as the file names. Secret volumes support ownership management and SELinux relabeling. Name Type Description Required defaultMode integer Optional: mode bits to use on created files by default. Must be a value between 0 and 0777. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. No items [ io.k8s.api.core.v1.KeyToPath ] If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. No optional boolean Specify whether the Secret or its keys must be defined No secretName string Name of the secret in the pod's namespace to use. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret No","title":"io.k8s.api.core.v1.SecretVolumeSource"},{"location":"swagger/#iok8sapicorev1securitycontext","text":"SecurityContext holds security configuration that will be applied to a container. Some fields are present in both SecurityContext and PodSecurityContext. When both are set, the values in SecurityContext take precedence. Name Type Description Required allowPrivilegeEscalation boolean AllowPrivilegeEscalation controls whether a process can gain more privileges than its parent process. This bool directly controls if the no_new_privs flag will be set on the container process. AllowPrivilegeEscalation is true always when the container is: 1) run as Privileged 2) has CAP_SYS_ADMIN No capabilities io.k8s.api.core.v1.Capabilities The capabilities to add/drop when running containers. Defaults to the default set of capabilities granted by the container runtime. No privileged boolean Run container in privileged mode. Processes in privileged containers are essentially equivalent to root on the host. Defaults to false. No procMount string procMount denotes the type of proc mount to use for the containers. The default is DefaultProcMount which uses the container runtime defaults for readonly paths and masked paths. This requires the ProcMountType feature flag to be enabled. No readOnlyRootFilesystem boolean Whether this container has a read-only root filesystem. Default is false. No runAsGroup long The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. No runAsNonRoot boolean Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. No runAsUser long The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. No seLinuxOptions io.k8s.api.core.v1.SELinuxOptions The SELinux context to be applied to the container. If unspecified, the container runtime will allocate a random SELinux context for each container. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. No windowsOptions io.k8s.api.core.v1.WindowsSecurityContextOptions Windows security options. No","title":"io.k8s.api.core.v1.SecurityContext"},{"location":"swagger/#iok8sapicorev1serviceaccounttokenprojection","text":"ServiceAccountTokenProjection represents a projected service account token volume. This projection can be used to insert a service account token into the pods runtime filesystem for use against APIs (Kubernetes API Server or otherwise). Name Type Description Required audience string Audience is the intended audience of the token. A recipient of a token must identify itself with an identifier specified in the audience of the token, and otherwise should reject the token. The audience defaults to the identifier of the apiserver. No expirationSeconds long ExpirationSeconds is the requested duration of validity of the service account token. As the token approaches expiration, the kubelet volume plugin will proactively rotate the service account token. The kubelet will start trying to rotate the token if the token is older than 80 percent of its time to live or if the token is older than 24 hours.Defaults to 1 hour and must be at least 10 minutes. No path string Path is the path relative to the mount point of the file to project the token into. Yes","title":"io.k8s.api.core.v1.ServiceAccountTokenProjection"},{"location":"swagger/#iok8sapicorev1storageosvolumesource","text":"Represents a StorageOS persistent volume resource. Name Type Description Required fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. No readOnly boolean Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. No secretRef io.k8s.api.core.v1.LocalObjectReference SecretRef specifies the secret to use for obtaining the StorageOS API credentials. If not specified, default values will be attempted. No volumeName string VolumeName is the human-readable name of the StorageOS volume. Volume names are only unique within a namespace. No volumeNamespace string VolumeNamespace specifies the scope of the volume within StorageOS. If no namespace is specified then the Pod's namespace will be used. This allows the Kubernetes name scoping to be mirrored within StorageOS for tighter integration. Set VolumeName to any name to override the default behaviour. Set to \"default\" if you are not using namespaces within StorageOS. Namespaces that do not pre-exist within StorageOS will be created. No","title":"io.k8s.api.core.v1.StorageOSVolumeSource"},{"location":"swagger/#iok8sapicorev1sysctl","text":"Sysctl defines a kernel parameter to be set Name Type Description Required name string Name of a property to set Yes value string Value of a property to set Yes","title":"io.k8s.api.core.v1.Sysctl"},{"location":"swagger/#iok8sapicorev1tcpsocketaction","text":"TCPSocketAction describes an action based on opening a socket Name Type Description Required host string Optional: Host name to connect to, defaults to the pod IP. No port io.k8s.apimachinery.pkg.util.intstr.IntOrString Number or name of the port to access on the container. Number must be in the range 1 to 65535. Name must be an IANA_SVC_NAME. Yes","title":"io.k8s.api.core.v1.TCPSocketAction"},{"location":"swagger/#iok8sapicorev1toleration","text":"The pod this Toleration is attached to tolerates any taint that matches the triple using the matching operator . Name Type Description Required effect string Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. No key string Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. No operator string Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. No tolerationSeconds long TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. No value string Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. No","title":"io.k8s.api.core.v1.Toleration"},{"location":"swagger/#iok8sapicorev1typedlocalobjectreference","text":"TypedLocalObjectReference contains enough information to let you locate the typed referenced object inside the same namespace. Name Type Description Required apiGroup string APIGroup is the group for the resource being referenced. If APIGroup is not specified, the specified Kind must be in the core API group. For any other third-party types, APIGroup is required. No kind string Kind is the type of resource being referenced Yes name string Name is the name of resource being referenced Yes","title":"io.k8s.api.core.v1.TypedLocalObjectReference"},{"location":"swagger/#iok8sapicorev1volume","text":"Volume represents a named volume in a pod that may be accessed by any container in the pod. Name Type Description Required awsElasticBlockStore io.k8s.api.core.v1.AWSElasticBlockStoreVolumeSource AWSElasticBlockStore represents an AWS Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore No azureDisk io.k8s.api.core.v1.AzureDiskVolumeSource AzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod. No azureFile io.k8s.api.core.v1.AzureFileVolumeSource AzureFile represents an Azure File Service mount on the host and bind mount to the pod. No cephfs io.k8s.api.core.v1.CephFSVolumeSource CephFS represents a Ceph FS mount on the host that shares a pod's lifetime No cinder io.k8s.api.core.v1.CinderVolumeSource Cinder represents a cinder volume attached and mounted on kubelets host machine More info: https://releases.k8s.io/HEAD/examples/mysql-cinder-pd/README.md No configMap io.k8s.api.core.v1.ConfigMapVolumeSource ConfigMap represents a configMap that should populate this volume No csi io.k8s.api.core.v1.CSIVolumeSource CSI (Container Storage Interface) represents storage that is handled by an external CSI driver (Alpha feature). No downwardAPI io.k8s.api.core.v1.DownwardAPIVolumeSource DownwardAPI represents downward API about the pod that should populate this volume No emptyDir io.k8s.api.core.v1.EmptyDirVolumeSource EmptyDir represents a temporary directory that shares a pod's lifetime. More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir No fc io.k8s.api.core.v1.FCVolumeSource FC represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod. No flexVolume io.k8s.api.core.v1.FlexVolumeSource FlexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin. No flocker io.k8s.api.core.v1.FlockerVolumeSource Flocker represents a Flocker volume attached to a kubelet's host machine. This depends on the Flocker control service being running No gcePersistentDisk io.k8s.api.core.v1.GCEPersistentDiskVolumeSource GCEPersistentDisk represents a GCE Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk No gitRepo io.k8s.api.core.v1.GitRepoVolumeSource GitRepo represents a git repository at a particular revision. DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir into the Pod's container. No glusterfs io.k8s.api.core.v1.GlusterfsVolumeSource Glusterfs represents a Glusterfs mount on the host that shares a pod's lifetime. More info: https://releases.k8s.io/HEAD/examples/volumes/glusterfs/README.md No hostPath io.k8s.api.core.v1.HostPathVolumeSource HostPath represents a pre-existing file or directory on the host machine that is directly exposed to the container. This is generally used for system agents or other privileged things that are allowed to see the host machine. Most containers will NOT need this. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath No iscsi io.k8s.api.core.v1.ISCSIVolumeSource ISCSI represents an ISCSI Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://releases.k8s.io/HEAD/examples/volumes/iscsi/README.md No name string Volume's name. Must be a DNS_LABEL and unique within the pod. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names Yes nfs io.k8s.api.core.v1.NFSVolumeSource NFS represents an NFS mount on the host that shares a pod's lifetime More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs No persistentVolumeClaim io.k8s.api.core.v1.PersistentVolumeClaimVolumeSource PersistentVolumeClaimVolumeSource represents a reference to a PersistentVolumeClaim in the same namespace. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims No photonPersistentDisk io.k8s.api.core.v1.PhotonPersistentDiskVolumeSource PhotonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine No portworxVolume io.k8s.api.core.v1.PortworxVolumeSource PortworxVolume represents a portworx volume attached and mounted on kubelets host machine No projected io.k8s.api.core.v1.ProjectedVolumeSource Items for all in one resources secrets, configmaps, and downward API No quobyte io.k8s.api.core.v1.QuobyteVolumeSource Quobyte represents a Quobyte mount on the host that shares a pod's lifetime No rbd io.k8s.api.core.v1.RBDVolumeSource RBD represents a Rados Block Device mount on the host that shares a pod's lifetime. More info: https://releases.k8s.io/HEAD/examples/volumes/rbd/README.md No scaleIO io.k8s.api.core.v1.ScaleIOVolumeSource ScaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes. No secret io.k8s.api.core.v1.SecretVolumeSource Secret represents a secret that should populate this volume. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret No storageos io.k8s.api.core.v1.StorageOSVolumeSource StorageOS represents a StorageOS volume attached and mounted on Kubernetes nodes. No vsphereVolume io.k8s.api.core.v1.VsphereVirtualDiskVolumeSource VsphereVolume represents a vSphere volume attached and mounted on kubelets host machine No","title":"io.k8s.api.core.v1.Volume"},{"location":"swagger/#iok8sapicorev1volumedevice","text":"volumeDevice describes a mapping of a raw block device within a container. Name Type Description Required devicePath string devicePath is the path inside of the container that the device will be mapped to. Yes name string name must match the name of a persistentVolumeClaim in the pod Yes","title":"io.k8s.api.core.v1.VolumeDevice"},{"location":"swagger/#iok8sapicorev1volumemount","text":"VolumeMount describes a mounting of a Volume within a container. Name Type Description Required mountPath string Path within the container at which the volume should be mounted. Must not contain ':'. Yes mountPropagation string mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. This field is beta in 1.10. No name string This must match the Name of a Volume. Yes readOnly boolean Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. No subPath string Path within the volume from which the container's volume should be mounted. Defaults to \"\" (volume's root). No subPathExpr string Expanded path within the volume from which the container's volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment. Defaults to \"\" (volume's root). SubPathExpr and SubPath are mutually exclusive. This field is beta in 1.15. No","title":"io.k8s.api.core.v1.VolumeMount"},{"location":"swagger/#iok8sapicorev1volumeprojection","text":"Projection that may be projected along with other supported volume types Name Type Description Required configMap io.k8s.api.core.v1.ConfigMapProjection information about the configMap data to project No downwardAPI io.k8s.api.core.v1.DownwardAPIProjection information about the downwardAPI data to project No secret io.k8s.api.core.v1.SecretProjection information about the secret data to project No serviceAccountToken io.k8s.api.core.v1.ServiceAccountTokenProjection information about the serviceAccountToken data to project No","title":"io.k8s.api.core.v1.VolumeProjection"},{"location":"swagger/#iok8sapicorev1vspherevirtualdiskvolumesource","text":"Represents a vSphere volume resource. Name Type Description Required fsType string Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. No storagePolicyID string Storage Policy Based Management (SPBM) profile ID associated with the StoragePolicyName. No storagePolicyName string Storage Policy Based Management (SPBM) profile name. No volumePath string Path that identifies vSphere volume vmdk Yes","title":"io.k8s.api.core.v1.VsphereVirtualDiskVolumeSource"},{"location":"swagger/#iok8sapicorev1weightedpodaffinityterm","text":"The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s) Name Type Description Required podAffinityTerm io.k8s.api.core.v1.PodAffinityTerm Required. A pod affinity term, associated with the corresponding weight. Yes weight integer weight associated with matching the corresponding podAffinityTerm, in the range 1-100. Yes","title":"io.k8s.api.core.v1.WeightedPodAffinityTerm"},{"location":"swagger/#iok8sapicorev1windowssecuritycontextoptions","text":"WindowsSecurityContextOptions contain Windows-specific options and credentials. Name Type Description Required gmsaCredentialSpec string GMSACredentialSpec is where the GMSA admission webhook (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the GMSA credential spec named by the GMSACredentialSpecName field. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag. No gmsaCredentialSpecName string GMSACredentialSpecName is the name of the GMSA credential spec to use. This field is alpha-level and is only honored by servers that enable the WindowsGMSA feature flag. No","title":"io.k8s.api.core.v1.WindowsSecurityContextOptions"},{"location":"swagger/#iok8sapipolicyv1beta1poddisruptionbudgetspec","text":"PodDisruptionBudgetSpec is a description of a PodDisruptionBudget. Name Type Description Required maxUnavailable io.k8s.apimachinery.pkg.util.intstr.IntOrString An eviction is allowed if at most \"maxUnavailable\" pods selected by \"selector\" are unavailable after the eviction, i.e. even in absence of the evicted pod. For example, one can prevent all voluntary evictions by specifying 0. This is a mutually exclusive setting with \"minAvailable\". No minAvailable io.k8s.apimachinery.pkg.util.intstr.IntOrString An eviction is allowed if at least \"minAvailable\" pods selected by \"selector\" will still be available after the eviction, i.e. even in the absence of the evicted pod. So for example you can prevent all voluntary evictions by specifying \"100%\". No selector io.k8s.apimachinery.pkg.apis.meta.v1.LabelSelector Label query over pods whose evictions are managed by the disruption budget. No","title":"io.k8s.api.policy.v1beta1.PodDisruptionBudgetSpec"},{"location":"swagger/#iok8sapimachinerypkgapiresourcequantity","text":"Quantity is a fixed-point representation of a number. It provides convenient marshaling/unmarshaling in JSON and YAML, in addition to String() and Int64() accessors. The serialization format is: ::= (Note that may be empty, from the \"\" case in .) ::= 0 | 1 | ... | 9 ::= | ::= | . | . | . ::= \"+\" | \"-\" ::= | ::= | | ::= Ki | Mi | Gi | Ti | Pi | Ei (International System of units; See: http://physics.nist.gov/cuu/Units/binary.html) ::= m | \"\" | k | M | G | T | P | E (Note that 1024 = 1Ki but 1000 = 1k; I didn't choose the capitalization.) ::= \"e\" | \"E\" No matter which of the three exponent forms is used, no quantity may represent a number greater than 2^63-1 in magnitude, nor may it have more than 3 decimal places. Numbers larger or more precise will be capped or rounded up. (E.g.: 0.1m will rounded up to 1m.) This may be extended in the future if we require larger or smaller quantities. When a Quantity is parsed from a string, it will remember the type of suffix it had, and will use the same type again when it is serialized. Before serializing, Quantity will be put in \"canonical form\". This means that Exponent/suffix will be adjusted up or down (with a corresponding increase or decrease in Mantissa) such that: a. No precision is lost b. No fractional digits will be emitted c. The exponent (or suffix) is as large as possible. The sign will be omitted unless the number is negative. Examples: 1.5 will be serialized as \"1500m\" 1.5Gi will be serialized as \"1536Mi\" Note that the quantity will NEVER be internally represented by a floating point number. That is the whole point of this exercise. Non-canonical values will still parse as long as they are well formed, but will be re-emitted in their canonical form. (So always use canonical form, or don't diff.) This format is intended to make it difficult to use these numbers without writing some sort of special handling code in the hopes that that will cause implementors to also use a fixed point implementation. Name Type Description Required io.k8s.apimachinery.pkg.api.resource.Quantity string Quantity is a fixed-point representation of a number. It provides convenient marshaling/unmarshaling in JSON and YAML, in addition to String() and Int64() accessors. The serialization format is: ::= (Note that may be empty, from the \"\" case in .) ::= 0 1","title":"io.k8s.apimachinery.pkg.api.resource.Quantity"},{"location":"swagger/#iok8sapimachinerypkgapismetav1createoptions","text":"CreateOptions may be provided when creating an API object. Name Type Description Required dryRun [ string ] No fieldManager string No","title":"io.k8s.apimachinery.pkg.apis.meta.v1.CreateOptions"},{"location":"swagger/#iok8sapimachinerypkgapismetav1fields","text":"Fields stores a set of fields in a data structure like a Trie. To understand how this is used, see: https://github.com/kubernetes-sigs/structured-merge-diff Name Type Description Required io.k8s.apimachinery.pkg.apis.meta.v1.Fields object Fields stores a set of fields in a data structure like a Trie. To understand how this is used, see: https://github.com/kubernetes-sigs/structured-merge-diff","title":"io.k8s.apimachinery.pkg.apis.meta.v1.Fields"},{"location":"swagger/#iok8sapimachinerypkgapismetav1initializer","text":"Initializer is information about an initializer that has not yet completed. Name Type Description Required name string name of the process that is responsible for initializing this object. Yes","title":"io.k8s.apimachinery.pkg.apis.meta.v1.Initializer"},{"location":"swagger/#iok8sapimachinerypkgapismetav1initializers","text":"Initializers tracks the progress of initialization. Name Type Description Required pending [ io.k8s.apimachinery.pkg.apis.meta.v1.Initializer ] Pending is a list of initializers that must execute in order before this object is visible. When the last pending initializer is removed, and no failing result is set, the initializers struct will be set to nil and the object is considered as initialized and visible to all clients. Yes result io.k8s.apimachinery.pkg.apis.meta.v1.Status If result is set with the Failure field, the object will be persisted to storage and then deleted, ensuring that other clients can observe the deletion. No","title":"io.k8s.apimachinery.pkg.apis.meta.v1.Initializers"},{"location":"swagger/#iok8sapimachinerypkgapismetav1labelselector","text":"A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects. Name Type Description Required matchExpressions [ io.k8s.apimachinery.pkg.apis.meta.v1.LabelSelectorRequirement ] matchExpressions is a list of label selector requirements. The requirements are ANDed. No matchLabels object matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. No","title":"io.k8s.apimachinery.pkg.apis.meta.v1.LabelSelector"},{"location":"swagger/#iok8sapimachinerypkgapismetav1labelselectorrequirement","text":"A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. Yes operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. Yes values [ string ] values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. No","title":"io.k8s.apimachinery.pkg.apis.meta.v1.LabelSelectorRequirement"},{"location":"swagger/#iok8sapimachinerypkgapismetav1listmeta","text":"ListMeta describes metadata that synthetic resources must have, including lists and various status objects. A resource may have only one of {ObjectMeta, ListMeta}. Name Type Description Required continue string continue may be set if the user set a limit on the number of items returned, and indicates that the server has more data available. The value is opaque and may be used to issue another request to the endpoint that served this list to retrieve the next set of available objects. Continuing a consistent list may not be possible if the server configuration has changed or more than a few minutes have passed. The resourceVersion field returned when using this continue value will be identical to the value in the first response, unless you have received this token from an error message. No remainingItemCount long remainingItemCount is the number of subsequent items in the list which are not included in this list response. If the list request contained label or field selectors, then the number of remaining items is unknown and the field will be left unset and omitted during serialization. If the list is complete (either because it is not chunking or because this is the last chunk), then there are no more remaining items and this field will be left unset and omitted during serialization. Servers older than v1.15 do not set this field. The intended use of the remainingItemCount is estimating the size of a collection. Clients should not rely on the remainingItemCount to be set or to be exact. This field is alpha and can be changed or removed without notice. No resourceVersion string String that identifies the server's internal version of this object that can be used by clients to determine when objects have changed. Value must be treated as opaque by clients and passed unmodified back to the server. Populated by the system. Read-only. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency No selfLink string selfLink is a URL representing this object. Populated by the system. Read-only. No","title":"io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta"},{"location":"swagger/#iok8sapimachinerypkgapismetav1managedfieldsentry","text":"ManagedFieldsEntry is a workflow-id, a FieldSet and the group version of the resource that the fieldset applies to. Name Type Description Required apiVersion string APIVersion defines the version of this resource that this field set applies to. The format is \"group/version\" just like the top-level APIVersion field. It is necessary to track the version of a field set because it cannot be automatically converted. No fields io.k8s.apimachinery.pkg.apis.meta.v1.Fields Fields identifies a set of fields. No manager string Manager is an identifier of the workflow managing these fields. No operation string Operation is the type of operation which lead to this ManagedFieldsEntry being created. The only valid values for this field are 'Apply' and 'Update'. No time io.k8s.apimachinery.pkg.apis.meta.v1.Time Time is timestamp of when these fields were set. It should always be empty if Operation is 'Apply' No","title":"io.k8s.apimachinery.pkg.apis.meta.v1.ManagedFieldsEntry"},{"location":"swagger/#iok8sapimachinerypkgapismetav1objectmeta","text":"ObjectMeta is metadata that all persisted resources must have, which includes all objects users must create. Name Type Description Required annotations object Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations No clusterName string The name of the cluster which the object belongs to. This is used to distinguish resources with same name and namespace in different clusters. This field is not set anywhere right now and apiserver is going to ignore it if set in create or update request. No creationTimestamp io.k8s.apimachinery.pkg.apis.meta.v1.Time CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC. Populated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata No deletionGracePeriodSeconds long Number of seconds allowed for this object to gracefully terminate before it will be removed from the system. Only set when deletionTimestamp is also set. May only be shortened. Read-only. No deletionTimestamp io.k8s.apimachinery.pkg.apis.meta.v1.Time DeletionTimestamp is RFC 3339 date and time at which this resource will be deleted. This field is set by the server when a graceful deletion is requested by the user, and is not directly settable by a client. The resource is expected to be deleted (no longer visible from resource lists, and not reachable by name) after the time in this field, once the finalizers list is empty. As long as the finalizers list contains items, deletion is blocked. Once the deletionTimestamp is set, this value may not be unset or be set further into the future, although it may be shortened or the resource may be deleted prior to this time. For example, a user may request that a pod is deleted in 30 seconds. The Kubelet will react by sending a graceful termination signal to the containers in the pod. After that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL) to the container and after cleanup, remove the pod from the API. In the presence of network partitions, this object may still exist after this timestamp, until an administrator or automated process can determine the resource is fully terminated. If not set, graceful deletion of the object has not been requested. Populated by the system when a graceful deletion is requested. Read-only. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata No finalizers [ string ] Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. No generateName string GenerateName is an optional prefix, used by the server, to generate a unique name ONLY IF the Name field has not been provided. If this field is used, the name returned to the client will be different than the name passed. This value will also be combined with a unique suffix. The provided value has the same validation rules as the Name field, and may be truncated by the length of the suffix required to make the value unique on the server. If this field is specified and the generated name exists, the server will NOT return a 409 - instead, it will either return 201 Created or 500 with Reason ServerTimeout indicating a unique name could not be found in the time allotted, and the client should retry (optionally after the time indicated in the Retry-After header). Applied only if Name is not specified. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#idempotency No generation long A sequence number representing a specific generation of the desired state. Populated by the system. Read-only. No initializers io.k8s.apimachinery.pkg.apis.meta.v1.Initializers An initializer is a controller which enforces some system invariant at object creation time. This field is a list of initializers that have not yet acted on this object. If nil or empty, this object has been completely initialized. Otherwise, the object is considered uninitialized and is hidden (in list/watch and get calls) from clients that haven't explicitly asked to observe uninitialized objects. When an object is created, the system will populate this list with the current set of initializers. Only privileged users may set or modify this list. Once it is empty, it may not be modified further by any user. DEPRECATED - initializers are an alpha field and will be removed in v1.15. No labels object Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels No managedFields [ io.k8s.apimachinery.pkg.apis.meta.v1.ManagedFieldsEntry ] ManagedFields maps workflow-id and version to the set of fields that are managed by that workflow. This is mostly for internal housekeeping, and users typically shouldn't need to set or understand this field. A workflow can be the user's name, a controller's name, or the name of a specific apply path like \"ci-cd\". The set of fields is always in the version that the workflow used when modifying the object. This field is alpha and can be changed or removed without notice. No name string Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/identifiers#names No namespace string Namespace defines the space within each name must be unique. An empty namespace is equivalent to the \"default\" namespace, but \"default\" is the canonical representation. Not all objects are required to be scoped to a namespace - the value of this field for those objects will be empty. Must be a DNS_LABEL. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/namespaces No ownerReferences [ io.k8s.apimachinery.pkg.apis.meta.v1.OwnerReference ] List of objects depended by this object. If ALL objects in the list have been deleted, this object will be garbage collected. If this object is managed by a controller, then an entry in this list will point to this controller, with the controller field set to true. There cannot be more than one managing controller. No resourceVersion string An opaque value that represents the internal version of this object that can be used by clients to determine when objects have changed. May be used for optimistic concurrency, change detection, and the watch operation on a resource or set of resources. Clients must treat these values as opaque and passed unmodified back to the server. They may only be valid for a particular resource or set of resources. Populated by the system. Read-only. Value must be treated as opaque by clients and . More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency No selfLink string SelfLink is a URL representing this object. Populated by the system. Read-only. No uid string UID is the unique in time and space value for this object. It is typically generated by the server on successful creation of a resource and is not allowed to change on PUT operations. Populated by the system. Read-only. More info: http://kubernetes.io/docs/user-guide/identifiers#uids No","title":"io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta"},{"location":"swagger/#iok8sapimachinerypkgapismetav1ownerreference","text":"OwnerReference contains enough information to let you identify an owning object. An owning object must be in the same namespace as the dependent, or be cluster-scoped, so there is no namespace field. Name Type Description Required apiVersion string API version of the referent. Yes blockOwnerDeletion boolean If true, AND if the owner has the \"foregroundDeletion\" finalizer, then the owner cannot be deleted from the key-value store until this reference is removed. Defaults to false. To set this field, a user needs \"delete\" permission of the owner, otherwise 422 (Unprocessable Entity) will be returned. No controller boolean If true, this reference points to the managing controller. No kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds Yes name string Name of the referent. More info: http://kubernetes.io/docs/user-guide/identifiers#names Yes uid string UID of the referent. More info: http://kubernetes.io/docs/user-guide/identifiers#uids Yes","title":"io.k8s.apimachinery.pkg.apis.meta.v1.OwnerReference"},{"location":"swagger/#iok8sapimachinerypkgapismetav1status","text":"Status is a return value for calls that don't return other objects. Name Type Description Required apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources No code integer Suggested HTTP return code for this status, 0 if not set. No details io.k8s.apimachinery.pkg.apis.meta.v1.StatusDetails Extended data associated with the reason. Each reason may define its own extended details. This field is optional and the data returned is not guaranteed to conform to any schema except that defined by the reason type. No kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds No message string A human-readable description of the status of this operation. No metadata io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta Standard list metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds No reason string A machine-readable description of why this operation is in the \"Failure\" status. If this value is empty there is no information available. A Reason clarifies an HTTP status code but does not override it. No status string Status of the operation. One of: \"Success\" or \"Failure\". More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status No","title":"io.k8s.apimachinery.pkg.apis.meta.v1.Status"},{"location":"swagger/#iok8sapimachinerypkgapismetav1statuscause","text":"StatusCause provides more information about an api.Status failure, including cases when multiple errors are encountered. Name Type Description Required field string The field of the resource that has caused this error, as named by its JSON serialization. May include dot and postfix notation for nested attributes. Arrays are zero-indexed. Fields may appear more than once in an array of causes due to fields having multiple errors. Optional. Examples: \"name\" - the field \"name\" on the current resource \"items[0].name\" - the field \"name\" on the first array entry in \"items\" No message string A human-readable description of the cause of the error. This field may be presented as-is to a reader. No reason string A machine-readable description of the cause of the error. If this value is empty there is no information available. No","title":"io.k8s.apimachinery.pkg.apis.meta.v1.StatusCause"},{"location":"swagger/#iok8sapimachinerypkgapismetav1statusdetails","text":"StatusDetails is a set of additional properties that MAY be set by the server to provide additional information about a response. The Reason field of a Status object defines what attributes will be set. Clients must ignore fields that do not match the defined type of each attribute, and should assume that any attribute may be empty, invalid, or under defined. Name Type Description Required causes [ io.k8s.apimachinery.pkg.apis.meta.v1.StatusCause ] The Causes array includes more details associated with the StatusReason failure. Not all StatusReasons may provide detailed causes. No group string The group attribute of the resource associated with the status StatusReason. No kind string The kind attribute of the resource associated with the status StatusReason. On some operations may differ from the requested resource Kind. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds No name string The name attribute of the resource associated with the status StatusReason (when there is a single name which can be described). No retryAfterSeconds integer If specified, the time in seconds before the operation should be retried. Some errors may indicate the client must take an alternate action - for those errors this field may indicate how long to wait before taking the alternate action. No uid string UID of the resource. (when there is a single resource which can be described). More info: http://kubernetes.io/docs/user-guide/identifiers#uids No","title":"io.k8s.apimachinery.pkg.apis.meta.v1.StatusDetails"},{"location":"swagger/#iok8sapimachinerypkgapismetav1time","text":"Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON. Wrappers are provided for many of the factory methods that the time package offers. Name Type Description Required io.k8s.apimachinery.pkg.apis.meta.v1.Time string Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON. Wrappers are provided for many of the factory methods that the time package offers.","title":"io.k8s.apimachinery.pkg.apis.meta.v1.Time"},{"location":"swagger/#iok8sapimachinerypkgutilintstrintorstring","text":"IntOrString is a type that can hold an int32 or a string. When used in JSON or YAML marshalling and unmarshalling, it produces or consumes the inner type. This allows you to have, for example, a JSON field that can accept a name or number. Name Type Description Required io.k8s.apimachinery.pkg.util.intstr.IntOrString string IntOrString is a type that can hold an int32 or a string. When used in JSON or YAML marshalling and unmarshalling, it produces or consumes the inner type. This allows you to have, for example, a JSON field that can accept a name or number.","title":"io.k8s.apimachinery.pkg.util.intstr.IntOrString"},{"location":"tls/","text":"Transport Layer Security \u00b6 v2.8 and after If you're running Argo Server you have three options with increasing transport security (note - you should also be running authentication ): Plain Text \u00b6 Recommended for: dev This is the default setting: everything is sent in plain text. To secure the UI you may front it with a HTTPS proxy. Encrypted \u00b6 Recommended for: development and test environments You can encrypt connections without any real effort. Start Argo Server with the --secure flag, e.g.: argo server --secure It will start with a self-signed certificate that expires after 365 days. Run the CLI with --secure (or ARGO_SECURE=true ) and --insecure-skip-verify (or ARGO_INSECURE_SKIP_VERIFY=true ). argo --secure --insecure-skip-verify list export ARGO_SECURE = true export ARGO_INSECURE_SKIP_VERIFY = true argo --secure --insecure-skip-verify list Tip: Don't forget to update your readiness probe to use HTTPS, example . Encrypted and Verified \u00b6 Recommended for: production environments Run your HTTPS proxy in front of the Argo Server. You'll need to set-up your certificates and this out of scope of this documentation. Start Argo Server with the --secure flag, e.g.: argo server --secure As before, it will start with a self-signed certificate that expires after 365 days. Run the CLI with --secure (or ARGO_SECURE=true ) only. argo --secure list export ARGO_SECURE = true argo list","title":"Transport Layer Security"},{"location":"tls/#transport-layer-security","text":"v2.8 and after If you're running Argo Server you have three options with increasing transport security (note - you should also be running authentication ):","title":"Transport Layer Security"},{"location":"tls/#plain-text","text":"Recommended for: dev This is the default setting: everything is sent in plain text. To secure the UI you may front it with a HTTPS proxy.","title":"Plain Text"},{"location":"tls/#encrypted","text":"Recommended for: development and test environments You can encrypt connections without any real effort. Start Argo Server with the --secure flag, e.g.: argo server --secure It will start with a self-signed certificate that expires after 365 days. Run the CLI with --secure (or ARGO_SECURE=true ) and --insecure-skip-verify (or ARGO_INSECURE_SKIP_VERIFY=true ). argo --secure --insecure-skip-verify list export ARGO_SECURE = true export ARGO_INSECURE_SKIP_VERIFY = true argo --secure --insecure-skip-verify list Tip: Don't forget to update your readiness probe to use HTTPS, example .","title":"Encrypted"},{"location":"tls/#encrypted-and-verified","text":"Recommended for: production environments Run your HTTPS proxy in front of the Argo Server. You'll need to set-up your certificates and this out of scope of this documentation. Start Argo Server with the --secure flag, e.g.: argo server --secure As before, it will start with a self-signed certificate that expires after 365 days. Run the CLI with --secure (or ARGO_SECURE=true ) only. argo --secure list export ARGO_SECURE = true argo list","title":"Encrypted and Verified"},{"location":"variables/","text":"Workflow Variables \u00b6 The following variables are made available to reference various metadata of a workflow: All Templates \u00b6 Variable Description inputs.parameters.<NAME> Input parameter to a template inputs.parameters All input parameters to a template as a JSON string inputs.artifacts.<NAME> Input artifact to a template Steps Templates \u00b6 Variable Description steps.<STEPNAME>.ip IP address of a previous daemon container step steps.<STEPNAME>.status Phase status of any previous step steps.<STEPNAME>.exitCode Exit code of any previous script or container step steps.<STEPNAME>.outputs.result Output result of any previous container or script step steps.<STEPNAME>.outputs.parameters When the previous step uses 'withItems', this contains a JSON array of the output parameters of each invocation steps.<STEPNAME>.outputs.parameters.<NAME> Output parameter of any previous step steps.<STEPNAME>.outputs.artifacts.<NAME> Output artifact of any previous step DAG Templates \u00b6 Variable Description tasks.<TASKNAME>.ip IP address of a previous daemon container task tasks.<TASKNAME>.status Phase status of any previous task tasks.<TASKNAME>.exitCode Exit code of any previous script or container task tasks.<TASKNAME>.outputs.result Output result of any previous container or script task tasks.<STEPNAME>.outputs.parameters When the previous task uses 'withItems', this contains a JSON array of the output parameters of each invocation tasks.<TASKNAME>.outputs.parameters.<NAME> Output parameter of any previous task tasks.<TASKNAME>.outputs.artifacts.<NAME> Output artifact of any previous task Container/Script Templates \u00b6 Variable Description pod.name Pod name of the container/script retries The retry number of the container/script if retryStrategy is specified inputs.artifacts.<NAME>.path Local path of the input artifact outputs.artifacts.<NAME>.path Local path of the output artifact outputs.parameters.<NAME>.path Local path of the output parameter Loops (withItems / withParam) \u00b6 Variable Description item Value of the item in a list item.<FIELDNAME> Field value of the item in a list of maps Metrics \u00b6 When emitting custom metrics in a template , special variables are available that allow self-reference to the current step. Variable Description status Phase status of the metric-emitting template duration Duration of the metric-emitting template in seconds (only applicable in Template -level metrics, for Workflow -level use workflow.duration ) inputs.parameters.<NAME> Input parameter of the metric-emitting template outputs.parameters.<NAME> Output parameter of the metric-emitting template outputs.result Output result of the metric-emitting template resourcesDuration Resources duration as a string. Can also be indexed for a selected resource, if available (may be one of resourcesDuration.cpu or resourcesDuration.memory . For more info, see the Resource Duration doc. Realtime Metrics \u00b6 Some variables can be emitted in realtime (as opposed to just when the step/task completes). To emit these variables in real time, set realtime: true under gauge (note: only Gauge metrics allow for real time variable emission). Metrics currently available for real time emission: For Workflow -level metrics: * workflow.duration For Template -level metrics: * duration Global \u00b6 Variable Description workflow.name Workflow name workflow.namespace Workflow namespace workflow.serviceAccountName Workflow service account name workflow.uid Workflow UID. Useful for setting ownership reference to a resource, or a unique artifact location workflow.parameters.<NAME> Input parameter to the workflow workflow.parameters All input parameters to the workflow as a JSON string workflow.outputs.parameters.<NAME> Global parameter in the workflow workflow.outputs.artifacts.<NAME> Global artifact in the workflow workflow.annotations.<NAME> Workflow annotations workflow.labels.<NAME> Workflow labels workflow.creationTimestamp Workflow creation timestamp formatted in RFC 3339 (e.g. 2018-08-23T05:42:49Z ) workflow.creationTimestamp.<STRFTIMECHAR> Creation timestamp formatted with a strftime format character workflow.priority Workflow priority workflow.duration Workflow duration estimate, may differ from actual duration by a couple of seconds Exit Handler \u00b6 Variable Description workflow.status Workflow status. One of: Succeeded , Failed , Error workflow.failures A list of JSON objects containing information about nodes that failed or errored during execution. Available fields: displayName , message , templateName , phase , podName , and finishedAt .","title":"Workflow Variables"},{"location":"variables/#workflow-variables","text":"The following variables are made available to reference various metadata of a workflow:","title":"Workflow Variables"},{"location":"variables/#all-templates","text":"Variable Description inputs.parameters.<NAME> Input parameter to a template inputs.parameters All input parameters to a template as a JSON string inputs.artifacts.<NAME> Input artifact to a template","title":"All Templates"},{"location":"variables/#steps-templates","text":"Variable Description steps.<STEPNAME>.ip IP address of a previous daemon container step steps.<STEPNAME>.status Phase status of any previous step steps.<STEPNAME>.exitCode Exit code of any previous script or container step steps.<STEPNAME>.outputs.result Output result of any previous container or script step steps.<STEPNAME>.outputs.parameters When the previous step uses 'withItems', this contains a JSON array of the output parameters of each invocation steps.<STEPNAME>.outputs.parameters.<NAME> Output parameter of any previous step steps.<STEPNAME>.outputs.artifacts.<NAME> Output artifact of any previous step","title":"Steps Templates"},{"location":"variables/#dag-templates","text":"Variable Description tasks.<TASKNAME>.ip IP address of a previous daemon container task tasks.<TASKNAME>.status Phase status of any previous task tasks.<TASKNAME>.exitCode Exit code of any previous script or container task tasks.<TASKNAME>.outputs.result Output result of any previous container or script task tasks.<STEPNAME>.outputs.parameters When the previous task uses 'withItems', this contains a JSON array of the output parameters of each invocation tasks.<TASKNAME>.outputs.parameters.<NAME> Output parameter of any previous task tasks.<TASKNAME>.outputs.artifacts.<NAME> Output artifact of any previous task","title":"DAG Templates"},{"location":"variables/#containerscript-templates","text":"Variable Description pod.name Pod name of the container/script retries The retry number of the container/script if retryStrategy is specified inputs.artifacts.<NAME>.path Local path of the input artifact outputs.artifacts.<NAME>.path Local path of the output artifact outputs.parameters.<NAME>.path Local path of the output parameter","title":"Container/Script Templates"},{"location":"variables/#loops-withitems-withparam","text":"Variable Description item Value of the item in a list item.<FIELDNAME> Field value of the item in a list of maps","title":"Loops (withItems / withParam)"},{"location":"variables/#metrics","text":"When emitting custom metrics in a template , special variables are available that allow self-reference to the current step. Variable Description status Phase status of the metric-emitting template duration Duration of the metric-emitting template in seconds (only applicable in Template -level metrics, for Workflow -level use workflow.duration ) inputs.parameters.<NAME> Input parameter of the metric-emitting template outputs.parameters.<NAME> Output parameter of the metric-emitting template outputs.result Output result of the metric-emitting template resourcesDuration Resources duration as a string. Can also be indexed for a selected resource, if available (may be one of resourcesDuration.cpu or resourcesDuration.memory . For more info, see the Resource Duration doc.","title":"Metrics"},{"location":"variables/#realtime-metrics","text":"Some variables can be emitted in realtime (as opposed to just when the step/task completes). To emit these variables in real time, set realtime: true under gauge (note: only Gauge metrics allow for real time variable emission). Metrics currently available for real time emission: For Workflow -level metrics: * workflow.duration For Template -level metrics: * duration","title":"Realtime Metrics"},{"location":"variables/#global","text":"Variable Description workflow.name Workflow name workflow.namespace Workflow namespace workflow.serviceAccountName Workflow service account name workflow.uid Workflow UID. Useful for setting ownership reference to a resource, or a unique artifact location workflow.parameters.<NAME> Input parameter to the workflow workflow.parameters All input parameters to the workflow as a JSON string workflow.outputs.parameters.<NAME> Global parameter in the workflow workflow.outputs.artifacts.<NAME> Global artifact in the workflow workflow.annotations.<NAME> Workflow annotations workflow.labels.<NAME> Workflow labels workflow.creationTimestamp Workflow creation timestamp formatted in RFC 3339 (e.g. 2018-08-23T05:42:49Z ) workflow.creationTimestamp.<STRFTIMECHAR> Creation timestamp formatted with a strftime format character workflow.priority Workflow priority workflow.duration Workflow duration estimate, may differ from actual duration by a couple of seconds","title":"Global"},{"location":"variables/#exit-handler","text":"Variable Description workflow.status Workflow status. One of: Succeeded , Failed , Error workflow.failures A list of JSON objects containing information about nodes that failed or errored during execution. Available fields: displayName , message , templateName , phase , podName , and finishedAt .","title":"Exit Handler"},{"location":"versioning/","text":"Versioning \u00b6 Argo Workflows does not use Semantic Versioning, even though we have not introduced any breaking changes since v2. Breaking changes will be communicated in the release notes. See: Public API","title":"Versioning"},{"location":"versioning/#versioning","text":"Argo Workflows does not use Semantic Versioning, even though we have not introduced any breaking changes since v2. Breaking changes will be communicated in the release notes. See: Public API","title":"Versioning"},{"location":"windows/","text":"Windows Container Support \u00b6 The Argo server and the workflow controller currently only run on Linux. The workflow executor however also runs on Windows nodes, meaning you can use Windows containers inside your workflows! Here are the steps to get started. Requirements \u00b6 Kubernetes 1.14 or later, supporting Windows nodes Hybrid cluster containing Linux and Windows nodes like described in the Kubernetes docs Argo configured and running like described here Setting up the workflow executor \u00b6 Currently the worflow controller configuration doesn't support different configurations for the dockerSockPath based on the host OS. This means that the workflow executor, running in a Windows container can't use Docker for now. You therefore need to use kubelet or k8sapi instead in your workflow controller configmap: containerRuntimeExecutor : kubelet kubeletInsecure : true # you can disable TLS verification of the kubelet executor for testing Schedule workflows with Windows containers \u00b6 If you're running workflows in your hybrid Kubernetes cluster, always make sure to include a nodeSelector to run the steps on the correct host OS: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : hello-windows- spec : entrypoint : hello-win templates : - name : hello-win nodeSelector : kubernetes.io/os : windows # specify the OS your step should run on container : image : mcr.microsoft.com/windows/nanoserver:1809 command : [ \"cmd\" , \"/c\" ] args : [ \"echo\" , \"Hello from Windows Container!\" ] You can run this example and get the logs: $ argo submit --watch https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-windows.yaml $ argo logs hello-windows-s9kk5 hello-windows-s9kk5: \"Hello from Windows Container!\" Bonus: Hybrid workflows \u00b6 You can also run different steps on different host OSs. This can for example be very helpful when you need to compile your application on Windows and Linux. An example workflow can look like the following: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : hello-hybrid- spec : entrypoint : mytemplate templates : - name : mytemplate steps : - - name : step1 template : hello-win - - name : step2 template : hello-linux - name : hello-win nodeSelector : kubernetes.io/os : windows container : image : mcr.microsoft.com/windows/nanoserver:1809 command : [ \"cmd\" , \"/c\" ] args : [ \"echo\" , \"Hello from Windows Container!\" ] - name : hello-linux nodeSelector : beta.kubernetes.io/os : linux container : image : alpine command : [ echo ] args : [ \"Hello from Linux Container!\" ] Again, you can run this example and get the logs: $ argo submit --watch https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-hybrid.yaml $ argo logs hello-hybrid-plqpp hello-hybrid-plqpp-1977432187: \"Hello from Windows Container!\" hello-hybrid-plqpp-764774907: Hello from Linux Container! Building the workflow executor image for Windows \u00b6 To build the workflow executor image for Windows you need a Windows machine running Windows Server 2019 with Docker installed like described in the docs . You then clone the project and run the Docker build with the Dockerfile for Windows and argoexec as a target: git clone https : // github . com / argoproj / argo . git cd argo docker build - t myargoexec - f . \\ Dockerfile . windows --target argoexec .","title":"Windows Container Support"},{"location":"windows/#windows-container-support","text":"The Argo server and the workflow controller currently only run on Linux. The workflow executor however also runs on Windows nodes, meaning you can use Windows containers inside your workflows! Here are the steps to get started.","title":"Windows Container Support"},{"location":"windows/#requirements","text":"Kubernetes 1.14 or later, supporting Windows nodes Hybrid cluster containing Linux and Windows nodes like described in the Kubernetes docs Argo configured and running like described here","title":"Requirements"},{"location":"windows/#setting-up-the-workflow-executor","text":"Currently the worflow controller configuration doesn't support different configurations for the dockerSockPath based on the host OS. This means that the workflow executor, running in a Windows container can't use Docker for now. You therefore need to use kubelet or k8sapi instead in your workflow controller configmap: containerRuntimeExecutor : kubelet kubeletInsecure : true # you can disable TLS verification of the kubelet executor for testing","title":"Setting up the workflow executor"},{"location":"windows/#schedule-workflows-with-windows-containers","text":"If you're running workflows in your hybrid Kubernetes cluster, always make sure to include a nodeSelector to run the steps on the correct host OS: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : hello-windows- spec : entrypoint : hello-win templates : - name : hello-win nodeSelector : kubernetes.io/os : windows # specify the OS your step should run on container : image : mcr.microsoft.com/windows/nanoserver:1809 command : [ \"cmd\" , \"/c\" ] args : [ \"echo\" , \"Hello from Windows Container!\" ] You can run this example and get the logs: $ argo submit --watch https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-windows.yaml $ argo logs hello-windows-s9kk5 hello-windows-s9kk5: \"Hello from Windows Container!\"","title":"Schedule workflows with Windows containers"},{"location":"windows/#bonus-hybrid-workflows","text":"You can also run different steps on different host OSs. This can for example be very helpful when you need to compile your application on Windows and Linux. An example workflow can look like the following: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : hello-hybrid- spec : entrypoint : mytemplate templates : - name : mytemplate steps : - - name : step1 template : hello-win - - name : step2 template : hello-linux - name : hello-win nodeSelector : kubernetes.io/os : windows container : image : mcr.microsoft.com/windows/nanoserver:1809 command : [ \"cmd\" , \"/c\" ] args : [ \"echo\" , \"Hello from Windows Container!\" ] - name : hello-linux nodeSelector : beta.kubernetes.io/os : linux container : image : alpine command : [ echo ] args : [ \"Hello from Linux Container!\" ] Again, you can run this example and get the logs: $ argo submit --watch https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-hybrid.yaml $ argo logs hello-hybrid-plqpp hello-hybrid-plqpp-1977432187: \"Hello from Windows Container!\" hello-hybrid-plqpp-764774907: Hello from Linux Container!","title":"Bonus: Hybrid workflows"},{"location":"windows/#building-the-workflow-executor-image-for-windows","text":"To build the workflow executor image for Windows you need a Windows machine running Windows Server 2019 with Docker installed like described in the docs . You then clone the project and run the Docker build with the Dockerfile for Windows and argoexec as a target: git clone https : // github . com / argoproj / argo . git cd argo docker build - t myargoexec - f . \\ Dockerfile . windows --target argoexec .","title":"Building the workflow executor image for Windows"},{"location":"work-avoidance/","text":"Work Avoidance \u00b6 v2.9 and after You can make workflows faster and more robust by employing work avoidance . A workflow that utilizes this is simply a workflow containing steps that do not run if the work has already been done. This simplest way to do this is to use marker files . Use cases: An expensive step appears across multiple workflows - you want to avoid repeating them. A workflow has unreliable tasks - you want to be able resubmit the workflow. A marker file is a file on that indicates the work has already been done, before doing the work you check to see if the marker has already been done: if [ -e /work/markers/name-of-task ] ; then echo \"work already done\" exit 0 fi echo \"working very hard\" touch /work/markers/name-of-task Choose a name for the file that is unique for the task, e.g. the template name and all the parameters: touch /work/markers/ $( date +%Y-%m-%d ) -echo- {{ inputs.parameters.num }} You need to store the marker files between workflows and this can be achieved using a PVC and optional input artifact . This complete work avoidance example has the following: A PVC to store the markers on. A load-markers step that loads the marker files from artifact storage. Multiple echo tasks that avoid work using marker files. A save-markers exit handler to save the marker files, even if they are not needed.","title":"Work Avoidance"},{"location":"work-avoidance/#work-avoidance","text":"v2.9 and after You can make workflows faster and more robust by employing work avoidance . A workflow that utilizes this is simply a workflow containing steps that do not run if the work has already been done. This simplest way to do this is to use marker files . Use cases: An expensive step appears across multiple workflows - you want to avoid repeating them. A workflow has unreliable tasks - you want to be able resubmit the workflow. A marker file is a file on that indicates the work has already been done, before doing the work you check to see if the marker has already been done: if [ -e /work/markers/name-of-task ] ; then echo \"work already done\" exit 0 fi echo \"working very hard\" touch /work/markers/name-of-task Choose a name for the file that is unique for the task, e.g. the template name and all the parameters: touch /work/markers/ $( date +%Y-%m-%d ) -echo- {{ inputs.parameters.num }} You need to store the marker files between workflows and this can be achieved using a PVC and optional input artifact . This complete work avoidance example has the following: A PVC to store the markers on. A load-markers step that loads the marker files from artifact storage. Multiple echo tasks that avoid work using marker files. A save-markers exit handler to save the marker files, even if they are not needed.","title":"Work Avoidance"},{"location":"workflow-archive/","text":"Workflow Archive \u00b6 v2.5 and after For many uses, you may wish to keep workflows for a long time. Argo can save completed workflows to an SQL database. To enable this feature, configure a Postgres or MySQL (>= 5.7.8) database under persistence in your configuration and set archive: true .","title":"Workflow Archive"},{"location":"workflow-archive/#workflow-archive","text":"v2.5 and after For many uses, you may wish to keep workflows for a long time. Argo can save completed workflows to an SQL database. To enable this feature, configure a Postgres or MySQL (>= 5.7.8) database under persistence in your configuration and set archive: true .","title":"Workflow Archive"},{"location":"workflow-concepts/","text":"Core Concepts \u00b6 This page serves as an introduction into the core concepts of Argo. The Workflow \u00b6 The Workflow is the most important resource in Argo and serves two important functions: It defines the workflow to be executed. It stores the state of the workflow. Because of these dual responsibilities, a Workflow should be treated as a \"live\" object. It is not only a static definition, but is also an \"instance\" of said definition. (If it isn't clear what this means, it will be explained below). Workflow Spec \u00b6 The workflow to be executed is defined in the Workflow.spec field. The core structure of a Workflow spec is a list of templates and an entrypoint . templates can be loosely thought of as \"functions\": they define instructions to be executed. The entrypoint field defines what the \"main\" function will be \u2013 that is, the template that will be executed first. Here is an example of a simple Workflow spec with a single template : apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : hello-world- # Name of this Workflow spec : entrypoint : whalesay # Defines \"whalesay\" as the \"main\" template templates : - name : whalesay # Defining the \"whalesay\" template container : image : docker/whalesay command : [ cowsay ] args : [ \"hello world\" ] # This template runs \"cowsay\" in the \"whalesay\" image with arguments \"hello world\" template Types \u00b6 There are 6 types of templates, divided into two different categories. Template Definitions \u00b6 These templates define work to be done, usually in a Container. Container \u00b6 Perhaps the most common template type, it will schedule a Container. The spec of the template is the same as the K8s container spec , so you can define a container here the same way you do anywhere else in K8s. Example: - name : whalesay container : image : docker/whalesay command : [ cowsay ] args : [ \"hello world\" ] Script \u00b6 A convenience wrapper around a container . The spec is the same as a container, but adds the source: field which allows you to define a script in-place. The script will be saved into a file and executed for you. The result of the script is automatically exported into an Argo variable either {{tasks.<NAME>.outputs.result}} or {{steps.<NAME>.outputs.result}} , depending how it was called. Example: - name : gen-random-int script : image : python:alpine3.6 command : [ python ] source : | import random i = random.randint(1, 100) print(i) Resource \u00b6 Performs operations on cluster Resources directly. It can be used to get, create, apply, delete, replace, or patch resources on your cluster. This example creates a ConfigMap resource on the cluster: - name : k8s-owner-reference resource : action : create manifest : | apiVersion: v1 kind: ConfigMap metadata: generateName: owned-eg- data: some: value Suspend \u00b6 A suspend template will suspend execution, either for a duration or until it is resumed manually. Suspend templates can be resumed from the CLI (with argo resume ), the API endpoint , or the UI. Example: - name : delay suspend : duration : \"20s\" Template Invocators \u00b6 These templates are used to invoke/call other templates and provide execution control. Steps \u00b6 A steps template allows you to define your tasks in a series of steps. The structure of the template is a \"list of lists\". Outer lists will run sequentially and inner lists will run in parallel. You can set a wide array of options to control execution, such as when: clauses to conditionally execute a step . In this example step1 runs first. Once it is completed, step2a and step2b will run in parallel: - name : hello-hello-hello steps : - - name : step1 template : prepare-data - - name : step2a template : run-data-first-half - name : step2b template : run-data-second-half DAG \u00b6 A dag template allows you to define your tasks as a graph of dependencies. In a DAG, you list all your tasks and set which other tasks must complete before a particular task can begin. Tasks without any dependencies will be run immediately. In this example A runs first. Once it is completed, B and C will run in parallel and once they both complete, D will run: - name : diamond dag : tasks : - name : A template : echo - name : B dependencies : [ A ] template : echo - name : C dependencies : [ A ] template : echo - name : D dependencies : [ B , C ] template : echo","title":"Core Concepts"},{"location":"workflow-concepts/#core-concepts","text":"This page serves as an introduction into the core concepts of Argo.","title":"Core Concepts"},{"location":"workflow-concepts/#the-workflow","text":"The Workflow is the most important resource in Argo and serves two important functions: It defines the workflow to be executed. It stores the state of the workflow. Because of these dual responsibilities, a Workflow should be treated as a \"live\" object. It is not only a static definition, but is also an \"instance\" of said definition. (If it isn't clear what this means, it will be explained below).","title":"The Workflow"},{"location":"workflow-concepts/#workflow-spec","text":"The workflow to be executed is defined in the Workflow.spec field. The core structure of a Workflow spec is a list of templates and an entrypoint . templates can be loosely thought of as \"functions\": they define instructions to be executed. The entrypoint field defines what the \"main\" function will be \u2013 that is, the template that will be executed first. Here is an example of a simple Workflow spec with a single template : apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : hello-world- # Name of this Workflow spec : entrypoint : whalesay # Defines \"whalesay\" as the \"main\" template templates : - name : whalesay # Defining the \"whalesay\" template container : image : docker/whalesay command : [ cowsay ] args : [ \"hello world\" ] # This template runs \"cowsay\" in the \"whalesay\" image with arguments \"hello world\"","title":"Workflow Spec"},{"location":"workflow-concepts/#template-types","text":"There are 6 types of templates, divided into two different categories.","title":"template Types"},{"location":"workflow-concepts/#template-definitions","text":"These templates define work to be done, usually in a Container.","title":"Template Definitions"},{"location":"workflow-concepts/#container","text":"Perhaps the most common template type, it will schedule a Container. The spec of the template is the same as the K8s container spec , so you can define a container here the same way you do anywhere else in K8s. Example: - name : whalesay container : image : docker/whalesay command : [ cowsay ] args : [ \"hello world\" ]","title":"Container"},{"location":"workflow-concepts/#script","text":"A convenience wrapper around a container . The spec is the same as a container, but adds the source: field which allows you to define a script in-place. The script will be saved into a file and executed for you. The result of the script is automatically exported into an Argo variable either {{tasks.<NAME>.outputs.result}} or {{steps.<NAME>.outputs.result}} , depending how it was called. Example: - name : gen-random-int script : image : python:alpine3.6 command : [ python ] source : | import random i = random.randint(1, 100) print(i)","title":"Script"},{"location":"workflow-concepts/#resource","text":"Performs operations on cluster Resources directly. It can be used to get, create, apply, delete, replace, or patch resources on your cluster. This example creates a ConfigMap resource on the cluster: - name : k8s-owner-reference resource : action : create manifest : | apiVersion: v1 kind: ConfigMap metadata: generateName: owned-eg- data: some: value","title":"Resource"},{"location":"workflow-concepts/#suspend","text":"A suspend template will suspend execution, either for a duration or until it is resumed manually. Suspend templates can be resumed from the CLI (with argo resume ), the API endpoint , or the UI. Example: - name : delay suspend : duration : \"20s\"","title":"Suspend"},{"location":"workflow-concepts/#template-invocators","text":"These templates are used to invoke/call other templates and provide execution control.","title":"Template Invocators"},{"location":"workflow-concepts/#steps","text":"A steps template allows you to define your tasks in a series of steps. The structure of the template is a \"list of lists\". Outer lists will run sequentially and inner lists will run in parallel. You can set a wide array of options to control execution, such as when: clauses to conditionally execute a step . In this example step1 runs first. Once it is completed, step2a and step2b will run in parallel: - name : hello-hello-hello steps : - - name : step1 template : prepare-data - - name : step2a template : run-data-first-half - name : step2b template : run-data-second-half","title":"Steps"},{"location":"workflow-concepts/#dag","text":"A dag template allows you to define your tasks as a graph of dependencies. In a DAG, you list all your tasks and set which other tasks must complete before a particular task can begin. Tasks without any dependencies will be run immediately. In this example A runs first. Once it is completed, B and C will run in parallel and once they both complete, D will run: - name : diamond dag : tasks : - name : A template : echo - name : B dependencies : [ A ] template : echo - name : C dependencies : [ A ] template : echo - name : D dependencies : [ B , C ] template : echo","title":"DAG"},{"location":"workflow-controller-configmap/","text":"Workflow Controller Configmap \u00b6 Introduction \u00b6 The Workflow Controller Configmap is used to set controller-wide settings. For a detailed example, please see workflow-controller-configmap.yaml . Setting the Configmap \u00b6 The configmap should be saved as a K8s Configmap on the cluster in the same namespace as the workflow-controller . It should then be referenced by the workflow-controller as an command argument: apiVersion : apps/v1 kind : Deployment metadata : name : workflow-controller spec : selector : matchLabels : app : workflow-controller template : metadata : labels : app : workflow-controller spec : containers : - args : - --configmap - workflow-controller-configmap # Set configmap name here - --executor-image - argoproj/argoexec:latest - --namespaced command : - workflow-controller image : argoproj/workflow-controller:latest name : workflow-controller serviceAccountName : argo nodeSelector : kubernetes.io/os : linux Alternate Structure \u00b6 In all versions, the configuration may be under a config: | key: # This file describes the config settings available in the workflow controller configmap apiVersion : v1 kind : ConfigMap metadata : name : workflow-controller-configmap data : config : | instanceID: my-ci-controller artifactRepository: archiveLogs: true s3: endpoint: s3.amazonaws.com bucket: my-bucket region: us-west-2 insecure: false accessKeySecret: name: my-s3-credentials key: accessKey secretKeySecret: name: my-s3-credentials key: secretKey In version 2.7+, the config: | key is optional. However, if the config: | key is not used, all nested maps under top level keys should be strings. This makes it easier to generate the map with some configuration management tools like Kustomize. # This file describes the config settings available in the workflow controller configmap apiVersion : v1 kind : ConfigMap metadata : name : workflow-controller-configmap data : # \"config: |\" key is optional in 2.7+! instanceID : my-ci-controller artifactRepository : | # However, all nested maps must be strings archiveLogs: true s3: endpoint: s3.amazonaws.com bucket: my-bucket region: us-west-2 insecure: false accessKeySecret: name: my-s3-credentials key: accessKey secretKeySecret: name: my-s3-credentials key: secretKey","title":"Workflow Controller Configmap"},{"location":"workflow-controller-configmap/#workflow-controller-configmap","text":"","title":"Workflow Controller Configmap"},{"location":"workflow-controller-configmap/#introduction","text":"The Workflow Controller Configmap is used to set controller-wide settings. For a detailed example, please see workflow-controller-configmap.yaml .","title":"Introduction"},{"location":"workflow-controller-configmap/#setting-the-configmap","text":"The configmap should be saved as a K8s Configmap on the cluster in the same namespace as the workflow-controller . It should then be referenced by the workflow-controller as an command argument: apiVersion : apps/v1 kind : Deployment metadata : name : workflow-controller spec : selector : matchLabels : app : workflow-controller template : metadata : labels : app : workflow-controller spec : containers : - args : - --configmap - workflow-controller-configmap # Set configmap name here - --executor-image - argoproj/argoexec:latest - --namespaced command : - workflow-controller image : argoproj/workflow-controller:latest name : workflow-controller serviceAccountName : argo nodeSelector : kubernetes.io/os : linux","title":"Setting the Configmap"},{"location":"workflow-controller-configmap/#alternate-structure","text":"In all versions, the configuration may be under a config: | key: # This file describes the config settings available in the workflow controller configmap apiVersion : v1 kind : ConfigMap metadata : name : workflow-controller-configmap data : config : | instanceID: my-ci-controller artifactRepository: archiveLogs: true s3: endpoint: s3.amazonaws.com bucket: my-bucket region: us-west-2 insecure: false accessKeySecret: name: my-s3-credentials key: accessKey secretKeySecret: name: my-s3-credentials key: secretKey In version 2.7+, the config: | key is optional. However, if the config: | key is not used, all nested maps under top level keys should be strings. This makes it easier to generate the map with some configuration management tools like Kustomize. # This file describes the config settings available in the workflow controller configmap apiVersion : v1 kind : ConfigMap metadata : name : workflow-controller-configmap data : # \"config: |\" key is optional in 2.7+! instanceID : my-ci-controller artifactRepository : | # However, all nested maps must be strings archiveLogs: true s3: endpoint: s3.amazonaws.com bucket: my-bucket region: us-west-2 insecure: false accessKeySecret: name: my-s3-credentials key: accessKey secretKeySecret: name: my-s3-credentials key: secretKey","title":"Alternate Structure"},{"location":"workflow-creator/","text":"Workflow Creator \u00b6 Today, is not possible for Argo Workflows to determine who created a workflow. The recommended approach is to add a label to your workflow, e.g. argo submit - l creator = alex","title":"Workflow Creator"},{"location":"workflow-creator/#workflow-creator","text":"Today, is not possible for Argo Workflows to determine who created a workflow. The recommended approach is to add a label to your workflow, e.g. argo submit - l creator = alex","title":"Workflow Creator"},{"location":"workflow-events/","text":"Workflow Events \u00b6 v2.7.2 We emit Kubernetes events on certain events. Workflow state change: WorkflowRunning WorkflowSucceeded WorkflowFailed WorkflowTimedOut Node completion WorkflowNodeSucceeded WorkflowNodeFailed WorkflowNodeError The involved object is the workflow in both cases. Additionally, for node completion events, annotations indicate the name and type of the involved node: metadata : name : my-wf.160434cb3af841f8 namespace : my-ns annotations : workflows.argoproj.io/node-name : my-node workflows.argoproj.io/node-type : Pod type : Normal reason : WorkflowNodeSucceeded message : 'Succeeded node my-node: my message' involvedObject : apiVersion : v1alpha1 kind : Workflow name : my-wf namespace : my-ns resourceVersion : \"1234\" uid : my-uid firstTimestamp : \"2020-04-09T16:50:16Z\" lastTimestamp : \"2020-04-09T16:50:16Z\" count : 1","title":"Workflow Events"},{"location":"workflow-events/#workflow-events","text":"v2.7.2 We emit Kubernetes events on certain events. Workflow state change: WorkflowRunning WorkflowSucceeded WorkflowFailed WorkflowTimedOut Node completion WorkflowNodeSucceeded WorkflowNodeFailed WorkflowNodeError The involved object is the workflow in both cases. Additionally, for node completion events, annotations indicate the name and type of the involved node: metadata : name : my-wf.160434cb3af841f8 namespace : my-ns annotations : workflows.argoproj.io/node-name : my-node workflows.argoproj.io/node-type : Pod type : Normal reason : WorkflowNodeSucceeded message : 'Succeeded node my-node: my message' involvedObject : apiVersion : v1alpha1 kind : Workflow name : my-wf namespace : my-ns resourceVersion : \"1234\" uid : my-uid firstTimestamp : \"2020-04-09T16:50:16Z\" lastTimestamp : \"2020-04-09T16:50:16Z\" count : 1","title":"Workflow Events"},{"location":"workflow-executors/","text":"Workflow Executors \u00b6 A workflow executor is a process that conforms to a specific interface that allows Argo to perform certain actions like monitoring pod logs, collecting artifacts, managing container lifecycles, etc.. The executor to be used in your workflows can be changed in the configmap under the containerRuntimeExecutor key. Docker (docker) \u00b6 default Pros \u00b6 Most reliable and well-tested executor Supports all workflow examples Highly scalable as it communicates directly with the docker daemon for heavy lifting Output artifacts can be located on the base layer (e.g. /tmp) Cons \u00b6 Least secure as it required docker.sock of the host to be mounted which is often rejected by OPA. Kubelet (kubelet) \u00b6 Pros \u00b6 Secure since you cannot escape the privileges of the pod's service account Moderately scalable Log retrieval and container operations are performed against the kubelet Cons \u00b6 Additional kubelet configuration may be required Output artifacts can only be saved on volumes (e.g. emptyDir) and not the base image layer (e.g. /tmp) Kubernetes API (k8sapi) \u00b6 Pros \u00b6 Secure since you cannot escape the privileges of the pod's service account No extra configuration is required Cons \u00b6 Least scalable since log retrieval and container operations are performed against the kubernetes api Output artifacts can only be saved on volumes (e.g. emptyDir) and not the base image layer (e.g. /tmp) Process Namespace Sharing (pns) \u00b6 Pros \u00b6 Secure since you cannot escape the privileges of the pod's service account Output artifacts can be located on the base layer (e.g. /tmp) Highly scalable. Process polling is done over procfs rather than the Kubernetes/Kubelet API Process will no longer run with PID 1 Cons \u00b6 Immature Cannot capture artifact directories from base image layer which has a volume mounted under it","title":"Workflow Executors"},{"location":"workflow-executors/#workflow-executors","text":"A workflow executor is a process that conforms to a specific interface that allows Argo to perform certain actions like monitoring pod logs, collecting artifacts, managing container lifecycles, etc.. The executor to be used in your workflows can be changed in the configmap under the containerRuntimeExecutor key.","title":"Workflow Executors"},{"location":"workflow-executors/#docker-docker","text":"default","title":"Docker (docker)"},{"location":"workflow-executors/#pros","text":"Most reliable and well-tested executor Supports all workflow examples Highly scalable as it communicates directly with the docker daemon for heavy lifting Output artifacts can be located on the base layer (e.g. /tmp)","title":"Pros"},{"location":"workflow-executors/#cons","text":"Least secure as it required docker.sock of the host to be mounted which is often rejected by OPA.","title":"Cons"},{"location":"workflow-executors/#kubelet-kubelet","text":"","title":"Kubelet (kubelet)"},{"location":"workflow-executors/#pros_1","text":"Secure since you cannot escape the privileges of the pod's service account Moderately scalable Log retrieval and container operations are performed against the kubelet","title":"Pros"},{"location":"workflow-executors/#cons_1","text":"Additional kubelet configuration may be required Output artifacts can only be saved on volumes (e.g. emptyDir) and not the base image layer (e.g. /tmp)","title":"Cons"},{"location":"workflow-executors/#kubernetes-api-k8sapi","text":"","title":"Kubernetes API (k8sapi)"},{"location":"workflow-executors/#pros_2","text":"Secure since you cannot escape the privileges of the pod's service account No extra configuration is required","title":"Pros"},{"location":"workflow-executors/#cons_2","text":"Least scalable since log retrieval and container operations are performed against the kubernetes api Output artifacts can only be saved on volumes (e.g. emptyDir) and not the base image layer (e.g. /tmp)","title":"Cons"},{"location":"workflow-executors/#process-namespace-sharing-pns","text":"","title":"Process Namespace Sharing (pns)"},{"location":"workflow-executors/#pros_3","text":"Secure since you cannot escape the privileges of the pod's service account Output artifacts can be located on the base layer (e.g. /tmp) Highly scalable. Process polling is done over procfs rather than the Kubernetes/Kubelet API Process will no longer run with PID 1","title":"Pros"},{"location":"workflow-executors/#cons_3","text":"Immature Cannot capture artifact directories from base image layer which has a volume mounted under it","title":"Cons"},{"location":"workflow-notifications/","text":"Workflow Notifications \u00b6 There are a number of use cases where you may wish to notify an external system when a workflow completes: Send an email. Send a Slack (or other instant message). Send a message to Kafka (or other message bus). You have options: For individual workflows, can add an exit handler to your workflow, for example . If you want the same for every workflow, you can add an exit handler to the default workflow spec . Use a service (e.g. Heptio Labs EventRouter ) to the Workflow events we emit.","title":"Workflow Notifications"},{"location":"workflow-notifications/#workflow-notifications","text":"There are a number of use cases where you may wish to notify an external system when a workflow completes: Send an email. Send a Slack (or other instant message). Send a message to Kafka (or other message bus). You have options: For individual workflows, can add an exit handler to your workflow, for example . If you want the same for every workflow, you can add an exit handler to the default workflow spec . Use a service (e.g. Heptio Labs EventRouter ) to the Workflow events we emit.","title":"Workflow Notifications"},{"location":"workflow-rbac/","text":"Workfow RBAC \u00b6 All pods in a workflow run with the service account specified in workflow.spec.serviceAccountName , or if omitted, the default service account of the workflow's namespace. The amount of access which a workflow needs is dependent on what the workflow needs to do. For example, if your workflow needs to deploy a resource, then the workflow's service account will require 'create' privileges on that resource. The bare minimum for a workflow to function is outlined below: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : workflow-role rules : # pod get/watch is used to identify the container IDs of the current pod # pod patch is used to annotate the step's outputs back to controller (e.g. artifact location) - apiGroups : - \"\" resources : - pods verbs : - get - watch - patch # logs get/watch are used to get the pods logs for script outputs, and for log archival - apiGroups : - \"\" resources : - pods/log verbs : - get - watch","title":"Workfow RBAC"},{"location":"workflow-rbac/#workfow-rbac","text":"All pods in a workflow run with the service account specified in workflow.spec.serviceAccountName , or if omitted, the default service account of the workflow's namespace. The amount of access which a workflow needs is dependent on what the workflow needs to do. For example, if your workflow needs to deploy a resource, then the workflow's service account will require 'create' privileges on that resource. The bare minimum for a workflow to function is outlined below: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : workflow-role rules : # pod get/watch is used to identify the container IDs of the current pod # pod patch is used to annotate the step's outputs back to controller (e.g. artifact location) - apiGroups : - \"\" resources : - pods verbs : - get - watch - patch # logs get/watch are used to get the pods logs for script outputs, and for log archival - apiGroups : - \"\" resources : - pods/log verbs : - get - watch","title":"Workfow RBAC"},{"location":"workflow-requirements/","text":"Workflow Restrictions \u00b6 v2.9 and after Introduction \u00b6 As the administrator of the controller, you may want to limit which types of Workflows your users can run. Setting workflow restrictions allows you to ensure that Workflows comply with certain requirements. Available Restrictions \u00b6 templateReferencing: Strict : Only Workflows using \"workflowTemplateRef\" will be processed. This allows the administrator of the controller to set a \"library\" of templates that may be run by its opeartor, limiting arbitrary Workflow execution. templateReferencing: Secure : Only Workflows using \"workflowTemplateRef\" will be processed and the controller will enforce that the WorkflowTemplate that is referenced hasn't changed between operations. If you want to make sure the operator of the Workflow cannot run an arbitrary Workflow, use this option. Setting Workflow Restrictions \u00b6 Workflow Restrictions can be specified by adding them under the workflowRestrictions key in the workflow-controller-configmap . For example, to specify that Workflows may only run with workflowTemplateRef # This file describes the config settings available in the workflow controller configmap apiVersion : v1 kind : ConfigMap metadata : name : workflow-controller-configmap data : | workflowRestrictions: templateReferencing: Secure","title":"Workflow Restrictions"},{"location":"workflow-requirements/#workflow-restrictions","text":"v2.9 and after","title":"Workflow Restrictions"},{"location":"workflow-requirements/#introduction","text":"As the administrator of the controller, you may want to limit which types of Workflows your users can run. Setting workflow restrictions allows you to ensure that Workflows comply with certain requirements.","title":"Introduction"},{"location":"workflow-requirements/#available-restrictions","text":"templateReferencing: Strict : Only Workflows using \"workflowTemplateRef\" will be processed. This allows the administrator of the controller to set a \"library\" of templates that may be run by its opeartor, limiting arbitrary Workflow execution. templateReferencing: Secure : Only Workflows using \"workflowTemplateRef\" will be processed and the controller will enforce that the WorkflowTemplate that is referenced hasn't changed between operations. If you want to make sure the operator of the Workflow cannot run an arbitrary Workflow, use this option.","title":"Available Restrictions"},{"location":"workflow-requirements/#setting-workflow-restrictions","text":"Workflow Restrictions can be specified by adding them under the workflowRestrictions key in the workflow-controller-configmap . For example, to specify that Workflows may only run with workflowTemplateRef # This file describes the config settings available in the workflow controller configmap apiVersion : v1 kind : ConfigMap metadata : name : workflow-controller-configmap data : | workflowRestrictions: templateReferencing: Secure","title":"Setting Workflow Restrictions"},{"location":"workflow-submitting-workflow/","text":"One Workflow Submitting Another \u00b6 v2.8 and after If you want one workflow to create another, you can do this using curl . You'll need an access token . Typically the best way is to submit from a workflow template: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : demo- spec : entrypoint : main templates : - name : main steps : - - name : a template : create-wf - name : create-wf script : image : curlimages/curl:latest command : - sh source : > curl http://argo-server:2746/api/v1/workflows/argo/submit \\ -fs \\ -H \"Authorization: Bearer eyJhbGci...\" \\ -d '{\"resourceKind\": \"WorkflowTemplate\", \"resourceName\": \"wait\", \"submitOptions\": {\"labels\": \"workflows.argoproj.io/workflow-template=wait\"}}' ``` See also: access token resuming a workflow via automation submitting a workflow via automation async pattern","title":"One Workflow Submitting Another"},{"location":"workflow-submitting-workflow/#one-workflow-submitting-another","text":"v2.8 and after If you want one workflow to create another, you can do this using curl . You'll need an access token . Typically the best way is to submit from a workflow template: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : demo- spec : entrypoint : main templates : - name : main steps : - - name : a template : create-wf - name : create-wf script : image : curlimages/curl:latest command : - sh source : > curl http://argo-server:2746/api/v1/workflows/argo/submit \\ -fs \\ -H \"Authorization: Bearer eyJhbGci...\" \\ -d '{\"resourceKind\": \"WorkflowTemplate\", \"resourceName\": \"wait\", \"submitOptions\": {\"labels\": \"workflows.argoproj.io/workflow-template=wait\"}}' ``` See also: access token resuming a workflow via automation submitting a workflow via automation async pattern","title":"One Workflow Submitting Another"},{"location":"workflow-templates/","text":"Workflow Templates \u00b6 v2.4 and after Introduction \u00b6 WorkflowTemplates are definitions of Workflows that live in your cluster. This allows you to create a library of frequently-used templates and reuse them either by submitting them directly (v2.7 and after) or by referencing them from your Workflows . WorkflowTemplate vs template \u00b6 The terms WorkflowTemplate and template have created an unfortunate naming collision and have created some confusion in the past. However, a quick description should clarify each and their differences. A template (lower-case) is a task within a Workflow or (confusingly) a WorkflowTemplate under the field templates . Whenever you define a Workflow , you must define at least one (but usually more than one) template to run. This template can be of type container , script , dag , steps , resource , or suspend and can be referenced by an entrypoint or by other dag , and step templates. Here is an example of a Workflow with two templates : ```yaml apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: steps- spec: entrypoint: hello # We reference our first \"template\" here templates: - name: hello # The first \"template\" in this Workflow, it is referenced by \"entrypoint\" steps: # The type of this \"template\" is \"steps\" - - name: hello template: whalesay # We reference our second \"template\" here arguments: parameters: [{name: message, value: \"hello1\"}] name: whalesay # The second \"template\" in this Workflow, it is referenced by \"hello\" inputs: parameters: name: message container: # The type of this \"template\" is \"container\" image: docker/whalesay command: [cowsay] args: [\"{{inputs.parameters.message}}\"]``` A WorkflowTemplate is a definition of a Workflow that lives in your cluster. Since it is a definition of a Workflow it also contains templates . These templates can be referenced from within the WorkflowTemplate and from other Workflows and WorkflowTemplates on your cluster. To see how, please see Referencing Other WorkflowTemplates . WorkflowTemplate Spec \u00b6 v2.7 and after WorkflowTemplates in v2.7 and after are full Workflow definitions. You can take any existing Workflow you may have and convert it to a WorkflowTemplate by substituting kind: Workflow to kind: WorkflowTemplate . v2.4 \u2013 2.6 WorkflowTemplates in v2.4 - v2.6 are only partial Workflow definitions and only support the templates and arguments field. This would not be a valid WorkflowTemplate in v2.4 - v2.6 (notice entrypoint field): apiVersion : argoproj.io/v1alpha1 kind : WorkflowTemplate metadata : name : workflow-template-submittable spec : entrypoint : whalesay-template # Fields other than \"arguments\" and \"templates\" not supported in v2.4 - v2.6 arguments : parameters : - name : message value : hello world templates : - name : whalesay-template inputs : parameters : - name : message container : image : docker/whalesay command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] However, this would be a valid WorkflowTemplate : apiVersion : argoproj.io/v1alpha1 kind : WorkflowTemplate metadata : name : workflow-template-submittable spec : arguments : parameters : - name : message value : hello world templates : - name : whalesay-template inputs : parameters : - name : message container : image : docker/whalesay command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] Referencing other WorkflowTemplates \u00b6 You can reference templates from another WorkflowTemplates (see the difference between the two ) using a templateRef field. Just as how you reference other templates within the same Workflow , you should do so from a steps or dag template. Here is an example from a steps template: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : workflow-template-hello-world- spec : entrypoint : whalesay templates : - name : whalesay steps : # You should only reference external \"templates\" in a \"steps\" or \"dag\" \"template\". - - name : call-whalesay-template templateRef : # You can reference a \"template\" from another \"WorkflowTemplate\" using this field name : workflow-template-1 # This is the name of the \"WorkflowTemplate\" CRD that contains the \"template\" you want template : whalesay-template # This is the name of the \"template\" you want to reference arguments : # You can pass in arguments as normal parameters : - name : message value : \"hello world\" You can also do so similarly with a dag template: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : workflow-template-hello-world- spec : entrypoint : whalesay templates : - name : whalesay dag : tasks : - name : call-whalesay-template templateRef : name : workflow-template-1 template : whalesay-template arguments : parameters : - name : message value : \"hello world\" You should never reference another template directly on a template object (outside of a steps or dag template). This includes both using template and templateRef . This behavior is deprecated, no longer supported, and will be removed in a future version. Here is an example of a deprecated reference that should not be used : apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : workflow-template-hello-world- spec : entrypoint : whalesay templates : - name : whalesay template : # You should NEVER use \"template\" here. Use it under a \"steps\" or \"dag\" template (see above). templateRef : # You should NEVER use \"templateRef\" here. Use it under a \"steps\" or \"dag\" template (see above). name : workflow-template-1 template : whalesay-template arguments : # Arguments here are ignored. Use them under a \"steps\" or \"dag\" template (see above). parameters : - name : message value : \"hello world\" The reasoning for deprecating this behavior is that a template is a \"definition\": it defines inputs and things to be done once instantiated. With this deprecated behavior, the same template object is allowed to be an \"instantiator\": to pass in \"live\" arguments and reference other templates (those other templates may be \"definitions\" or \"instantiators\"). This behavior has been problematic and dangerous. It causes confusion and has design inconsistencies. 2.9 and after Create Workflow from WorkflowTemplate Spec \u00b6 You can create Workflow from WorkflowTemplate spec using workflowTemplateRef . If you pass the arguments to created Workflow , it will be merged with WorkflowTemplate arguments Here is an example for referring WorkflowTemplate as Workflow with passing entrypoint and Workflow Arguments to WorkflowTemplate apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : workflow-template-hello-world- spec : entrypoint : whalesay-template arguments : parameters : - name : message value : \"from workflow\" workflowTemplateRef : name : workflow-template-submittable Here is an example of a referring WorkflowTemplate as Workflow and using WorkflowTemplates 's entrypoint and Workflow Arguments apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : workflow-template-hello-world- spec : workflowTemplateRef : name : workflow-template-submittable Managing WorkflowTemplates \u00b6 CLI \u00b6 You can create some example templates as follows: argo template create https : // raw . githubusercontent . com / argoproj / argo / master / examples / workflow - template / templates . yaml The submit a workflow using one of those templates: argo submit https : // raw . githubusercontent . com / argoproj / argo / master / examples / workflow - template / hello - world . yaml 2.7 and after The submit a WorkflowTemplate as a Workflow : argo submit --from workflowtemplate/workflow-template-submittable kubectl \u00b6 Using kubectl apply -f and kubectl get wftmpl GitOps via Argo CD \u00b6 WorkflowTemplate resources can be managed with GitOps by using Argo CD UI \u00b6 WorkflowTemplate resources can also be managed by the UI","title":"Workflow Templates"},{"location":"workflow-templates/#workflow-templates","text":"v2.4 and after","title":"Workflow Templates"},{"location":"workflow-templates/#introduction","text":"WorkflowTemplates are definitions of Workflows that live in your cluster. This allows you to create a library of frequently-used templates and reuse them either by submitting them directly (v2.7 and after) or by referencing them from your Workflows .","title":"Introduction"},{"location":"workflow-templates/#workflowtemplate-vs-template","text":"The terms WorkflowTemplate and template have created an unfortunate naming collision and have created some confusion in the past. However, a quick description should clarify each and their differences. A template (lower-case) is a task within a Workflow or (confusingly) a WorkflowTemplate under the field templates . Whenever you define a Workflow , you must define at least one (but usually more than one) template to run. This template can be of type container , script , dag , steps , resource , or suspend and can be referenced by an entrypoint or by other dag , and step templates. Here is an example of a Workflow with two templates : ```yaml apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: steps- spec: entrypoint: hello # We reference our first \"template\" here templates: - name: hello # The first \"template\" in this Workflow, it is referenced by \"entrypoint\" steps: # The type of this \"template\" is \"steps\" - - name: hello template: whalesay # We reference our second \"template\" here arguments: parameters: [{name: message, value: \"hello1\"}] name: whalesay # The second \"template\" in this Workflow, it is referenced by \"hello\" inputs: parameters: name: message container: # The type of this \"template\" is \"container\" image: docker/whalesay command: [cowsay] args: [\"{{inputs.parameters.message}}\"]``` A WorkflowTemplate is a definition of a Workflow that lives in your cluster. Since it is a definition of a Workflow it also contains templates . These templates can be referenced from within the WorkflowTemplate and from other Workflows and WorkflowTemplates on your cluster. To see how, please see Referencing Other WorkflowTemplates .","title":"WorkflowTemplate vs template"},{"location":"workflow-templates/#workflowtemplate-spec","text":"v2.7 and after WorkflowTemplates in v2.7 and after are full Workflow definitions. You can take any existing Workflow you may have and convert it to a WorkflowTemplate by substituting kind: Workflow to kind: WorkflowTemplate . v2.4 \u2013 2.6 WorkflowTemplates in v2.4 - v2.6 are only partial Workflow definitions and only support the templates and arguments field. This would not be a valid WorkflowTemplate in v2.4 - v2.6 (notice entrypoint field): apiVersion : argoproj.io/v1alpha1 kind : WorkflowTemplate metadata : name : workflow-template-submittable spec : entrypoint : whalesay-template # Fields other than \"arguments\" and \"templates\" not supported in v2.4 - v2.6 arguments : parameters : - name : message value : hello world templates : - name : whalesay-template inputs : parameters : - name : message container : image : docker/whalesay command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] However, this would be a valid WorkflowTemplate : apiVersion : argoproj.io/v1alpha1 kind : WorkflowTemplate metadata : name : workflow-template-submittable spec : arguments : parameters : - name : message value : hello world templates : - name : whalesay-template inputs : parameters : - name : message container : image : docker/whalesay command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ]","title":"WorkflowTemplate Spec"},{"location":"workflow-templates/#referencing-other-workflowtemplates","text":"You can reference templates from another WorkflowTemplates (see the difference between the two ) using a templateRef field. Just as how you reference other templates within the same Workflow , you should do so from a steps or dag template. Here is an example from a steps template: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : workflow-template-hello-world- spec : entrypoint : whalesay templates : - name : whalesay steps : # You should only reference external \"templates\" in a \"steps\" or \"dag\" \"template\". - - name : call-whalesay-template templateRef : # You can reference a \"template\" from another \"WorkflowTemplate\" using this field name : workflow-template-1 # This is the name of the \"WorkflowTemplate\" CRD that contains the \"template\" you want template : whalesay-template # This is the name of the \"template\" you want to reference arguments : # You can pass in arguments as normal parameters : - name : message value : \"hello world\" You can also do so similarly with a dag template: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : workflow-template-hello-world- spec : entrypoint : whalesay templates : - name : whalesay dag : tasks : - name : call-whalesay-template templateRef : name : workflow-template-1 template : whalesay-template arguments : parameters : - name : message value : \"hello world\" You should never reference another template directly on a template object (outside of a steps or dag template). This includes both using template and templateRef . This behavior is deprecated, no longer supported, and will be removed in a future version. Here is an example of a deprecated reference that should not be used : apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : workflow-template-hello-world- spec : entrypoint : whalesay templates : - name : whalesay template : # You should NEVER use \"template\" here. Use it under a \"steps\" or \"dag\" template (see above). templateRef : # You should NEVER use \"templateRef\" here. Use it under a \"steps\" or \"dag\" template (see above). name : workflow-template-1 template : whalesay-template arguments : # Arguments here are ignored. Use them under a \"steps\" or \"dag\" template (see above). parameters : - name : message value : \"hello world\" The reasoning for deprecating this behavior is that a template is a \"definition\": it defines inputs and things to be done once instantiated. With this deprecated behavior, the same template object is allowed to be an \"instantiator\": to pass in \"live\" arguments and reference other templates (those other templates may be \"definitions\" or \"instantiators\"). This behavior has been problematic and dangerous. It causes confusion and has design inconsistencies. 2.9 and after","title":"Referencing other WorkflowTemplates"},{"location":"workflow-templates/#create-workflow-from-workflowtemplate-spec","text":"You can create Workflow from WorkflowTemplate spec using workflowTemplateRef . If you pass the arguments to created Workflow , it will be merged with WorkflowTemplate arguments Here is an example for referring WorkflowTemplate as Workflow with passing entrypoint and Workflow Arguments to WorkflowTemplate apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : workflow-template-hello-world- spec : entrypoint : whalesay-template arguments : parameters : - name : message value : \"from workflow\" workflowTemplateRef : name : workflow-template-submittable Here is an example of a referring WorkflowTemplate as Workflow and using WorkflowTemplates 's entrypoint and Workflow Arguments apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : workflow-template-hello-world- spec : workflowTemplateRef : name : workflow-template-submittable","title":"Create Workflow from WorkflowTemplate Spec"},{"location":"workflow-templates/#managing-workflowtemplates","text":"","title":"Managing WorkflowTemplates"},{"location":"workflow-templates/#cli","text":"You can create some example templates as follows: argo template create https : // raw . githubusercontent . com / argoproj / argo / master / examples / workflow - template / templates . yaml The submit a workflow using one of those templates: argo submit https : // raw . githubusercontent . com / argoproj / argo / master / examples / workflow - template / hello - world . yaml 2.7 and after The submit a WorkflowTemplate as a Workflow : argo submit --from workflowtemplate/workflow-template-submittable","title":"CLI"},{"location":"workflow-templates/#kubectl","text":"Using kubectl apply -f and kubectl get wftmpl","title":"kubectl"},{"location":"workflow-templates/#gitops-via-argo-cd","text":"WorkflowTemplate resources can be managed with GitOps by using Argo CD","title":"GitOps via Argo CD"},{"location":"workflow-templates/#ui","text":"WorkflowTemplate resources can also be managed by the UI","title":"UI"},{"location":"cli/argo/","text":"argo \u00b6 argo is the command line interface to Argo Synopsis \u00b6 argo is the command line interface to Argo argo [ flags ] Examples \u00b6 If you ' re using the Argo Server (e.g. because you need large workflow support or workflow archive), please read https://github.com/argoproj/argo/blob/master/docs/cli.md. Options \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use - h , -- help help for argo -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo archive - argo auth - argo cluster-template - manipulate cluster workflow templates argo completion - output shell completion code for the specified shell (bash or zsh) argo cron - manage cron workflows argo delete - delete workflows argo get - display details about a workflow argo lint - validate files or directories of workflow manifests argo list - list workflows argo logs - view logs of a pod or workflow argo resubmit - resubmit one or more workflows argo resume - resume zero or more workflows argo retry - retry zero or more workflows argo server - Start the Argo Server argo stop - stop zero or more workflows argo submit - submit a workflow argo suspend - suspend zero or more workflow argo template - manipulate workflow templates argo terminate - terminate zero or more workflows argo version - Print version information argo wait - waits for workflows to complete argo watch - watch a workflow until it completes","title":"argo"},{"location":"cli/argo/#argo","text":"argo is the command line interface to Argo","title":"argo"},{"location":"cli/argo/#synopsis","text":"argo is the command line interface to Argo argo [ flags ]","title":"Synopsis"},{"location":"cli/argo/#examples","text":"If you ' re using the Argo Server (e.g. because you need large workflow support or workflow archive), please read https://github.com/argoproj/argo/blob/master/docs/cli.md.","title":"Examples"},{"location":"cli/argo/#options","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use - h , -- help help for argo -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options"},{"location":"cli/argo/#see-also","text":"argo archive - argo auth - argo cluster-template - manipulate cluster workflow templates argo completion - output shell completion code for the specified shell (bash or zsh) argo cron - manage cron workflows argo delete - delete workflows argo get - display details about a workflow argo lint - validate files or directories of workflow manifests argo list - list workflows argo logs - view logs of a pod or workflow argo resubmit - resubmit one or more workflows argo resume - resume zero or more workflows argo retry - retry zero or more workflows argo server - Start the Argo Server argo stop - stop zero or more workflows argo submit - submit a workflow argo suspend - suspend zero or more workflow argo template - manipulate workflow templates argo terminate - terminate zero or more workflows argo version - Print version information argo wait - waits for workflows to complete argo watch - watch a workflow until it completes","title":"SEE ALSO"},{"location":"cli/argo_archive/","text":"argo archive \u00b6 Synopsis \u00b6 argo archive [ flags ] Options \u00b6 - h , -- help help for archive Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo argo archive delete - argo archive get - argo archive list -","title":"argo archive"},{"location":"cli/argo_archive/#argo-archive","text":"","title":"argo archive"},{"location":"cli/argo_archive/#synopsis","text":"argo archive [ flags ]","title":"Synopsis"},{"location":"cli/argo_archive/#options","text":"- h , -- help help for archive","title":"Options"},{"location":"cli/argo_archive/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_archive/#see-also","text":"argo - argo is the command line interface to Argo argo archive delete - argo archive get - argo archive list -","title":"SEE ALSO"},{"location":"cli/argo_archive_delete/","text":"argo archive delete \u00b6 Synopsis \u00b6 argo archive delete UID ... [ flags ] Options \u00b6 - h , -- help help for delete Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo archive -","title":"argo archive delete"},{"location":"cli/argo_archive_delete/#argo-archive-delete","text":"","title":"argo archive delete"},{"location":"cli/argo_archive_delete/#synopsis","text":"argo archive delete UID ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_archive_delete/#options","text":"- h , -- help help for delete","title":"Options"},{"location":"cli/argo_archive_delete/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_archive_delete/#see-also","text":"argo archive -","title":"SEE ALSO"},{"location":"cli/argo_archive_get/","text":"argo archive get \u00b6 Synopsis \u00b6 argo archive get UID [ flags ] Options \u00b6 - h , -- help help for get - o , -- output string Output format . One of : json | yaml | wide ( default \" wide \" ) Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo archive -","title":"argo archive get"},{"location":"cli/argo_archive_get/#argo-archive-get","text":"","title":"argo archive get"},{"location":"cli/argo_archive_get/#synopsis","text":"argo archive get UID [ flags ]","title":"Synopsis"},{"location":"cli/argo_archive_get/#options","text":"- h , -- help help for get - o , -- output string Output format . One of : json | yaml | wide ( default \" wide \" )","title":"Options"},{"location":"cli/argo_archive_get/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_archive_get/#see-also","text":"argo archive -","title":"SEE ALSO"},{"location":"cli/argo_archive_list/","text":"argo archive list \u00b6 Synopsis \u00b6 argo archive list [ flags ] Options \u00b6 -- chunk - size int Return large lists in chunks rather than all at once . Pass 0 to disable . - h , -- help help for list - o , -- output string Output format . One of : json | yaml | wide ( default \" wide \" ) - l , -- selector string Selector ( label query ) to filter on , not including uninitialized ones Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo archive -","title":"argo archive list"},{"location":"cli/argo_archive_list/#argo-archive-list","text":"","title":"argo archive list"},{"location":"cli/argo_archive_list/#synopsis","text":"argo archive list [ flags ]","title":"Synopsis"},{"location":"cli/argo_archive_list/#options","text":"-- chunk - size int Return large lists in chunks rather than all at once . Pass 0 to disable . - h , -- help help for list - o , -- output string Output format . One of : json | yaml | wide ( default \" wide \" ) - l , -- selector string Selector ( label query ) to filter on , not including uninitialized ones","title":"Options"},{"location":"cli/argo_archive_list/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_archive_list/#see-also","text":"argo archive -","title":"SEE ALSO"},{"location":"cli/argo_auth/","text":"argo auth \u00b6 Synopsis \u00b6 argo auth [ flags ] Options \u00b6 - h , -- help help for auth Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo argo auth token - Print the auth token","title":"argo auth"},{"location":"cli/argo_auth/#argo-auth","text":"","title":"argo auth"},{"location":"cli/argo_auth/#synopsis","text":"argo auth [ flags ]","title":"Synopsis"},{"location":"cli/argo_auth/#options","text":"- h , -- help help for auth","title":"Options"},{"location":"cli/argo_auth/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_auth/#see-also","text":"argo - argo is the command line interface to Argo argo auth token - Print the auth token","title":"SEE ALSO"},{"location":"cli/argo_auth_token/","text":"argo auth token \u00b6 Print the auth token Synopsis \u00b6 Print the auth token argo auth token [ flags ] Options \u00b6 - h , -- help help for token Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo auth -","title":"argo auth token"},{"location":"cli/argo_auth_token/#argo-auth-token","text":"Print the auth token","title":"argo auth token"},{"location":"cli/argo_auth_token/#synopsis","text":"Print the auth token argo auth token [ flags ]","title":"Synopsis"},{"location":"cli/argo_auth_token/#options","text":"- h , -- help help for token","title":"Options"},{"location":"cli/argo_auth_token/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_auth_token/#see-also","text":"argo auth -","title":"SEE ALSO"},{"location":"cli/argo_cluster-template/","text":"argo cluster-template \u00b6 manipulate cluster workflow templates Synopsis \u00b6 manipulate cluster workflow templates argo cluster - template [ flags ] Options \u00b6 - h , -- help help for cluster - template Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo argo cluster-template create - create a cluster workflow template argo cluster-template delete - delete a cluster workflow template argo cluster-template get - display details about a cluster workflow template argo cluster-template lint - validate files or directories of cluster workflow template manifests argo cluster-template list - list cluster workflow templates","title":"argo cluster-template"},{"location":"cli/argo_cluster-template/#argo-cluster-template","text":"manipulate cluster workflow templates","title":"argo cluster-template"},{"location":"cli/argo_cluster-template/#synopsis","text":"manipulate cluster workflow templates argo cluster - template [ flags ]","title":"Synopsis"},{"location":"cli/argo_cluster-template/#options","text":"- h , -- help help for cluster - template","title":"Options"},{"location":"cli/argo_cluster-template/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_cluster-template/#see-also","text":"argo - argo is the command line interface to Argo argo cluster-template create - create a cluster workflow template argo cluster-template delete - delete a cluster workflow template argo cluster-template get - display details about a cluster workflow template argo cluster-template lint - validate files or directories of cluster workflow template manifests argo cluster-template list - list cluster workflow templates","title":"SEE ALSO"},{"location":"cli/argo_cluster-template_create/","text":"argo cluster-template create \u00b6 create a cluster workflow template Synopsis \u00b6 create a cluster workflow template argo cluster - template create FILE1 FILE2 ... [ flags ] Options \u00b6 - h , -- help help for create - o , -- output string Output format . One of : name | json | yaml | wide -- strict perform strict workflow validation ( default true ) Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo cluster-template - manipulate cluster workflow templates","title":"argo cluster-template create"},{"location":"cli/argo_cluster-template_create/#argo-cluster-template-create","text":"create a cluster workflow template","title":"argo cluster-template create"},{"location":"cli/argo_cluster-template_create/#synopsis","text":"create a cluster workflow template argo cluster - template create FILE1 FILE2 ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_cluster-template_create/#options","text":"- h , -- help help for create - o , -- output string Output format . One of : name | json | yaml | wide -- strict perform strict workflow validation ( default true )","title":"Options"},{"location":"cli/argo_cluster-template_create/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_cluster-template_create/#see-also","text":"argo cluster-template - manipulate cluster workflow templates","title":"SEE ALSO"},{"location":"cli/argo_cluster-template_delete/","text":"argo cluster-template delete \u00b6 delete a cluster workflow template Synopsis \u00b6 delete a cluster workflow template argo cluster - template delete WORKFLOW_TEMPLATE [ flags ] Options \u00b6 -- all Delete all cluster workflow templates - h , -- help help for delete Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo cluster-template - manipulate cluster workflow templates","title":"argo cluster-template delete"},{"location":"cli/argo_cluster-template_delete/#argo-cluster-template-delete","text":"delete a cluster workflow template","title":"argo cluster-template delete"},{"location":"cli/argo_cluster-template_delete/#synopsis","text":"delete a cluster workflow template argo cluster - template delete WORKFLOW_TEMPLATE [ flags ]","title":"Synopsis"},{"location":"cli/argo_cluster-template_delete/#options","text":"-- all Delete all cluster workflow templates - h , -- help help for delete","title":"Options"},{"location":"cli/argo_cluster-template_delete/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_cluster-template_delete/#see-also","text":"argo cluster-template - manipulate cluster workflow templates","title":"SEE ALSO"},{"location":"cli/argo_cluster-template_get/","text":"argo cluster-template get \u00b6 display details about a cluster workflow template Synopsis \u00b6 display details about a cluster workflow template argo cluster - template get CLUSTER WORKFLOW_TEMPLATE ... [ flags ] Options \u00b6 - h , -- help help for get - o , -- output string Output format . One of : json | yaml | wide Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo cluster-template - manipulate cluster workflow templates","title":"argo cluster-template get"},{"location":"cli/argo_cluster-template_get/#argo-cluster-template-get","text":"display details about a cluster workflow template","title":"argo cluster-template get"},{"location":"cli/argo_cluster-template_get/#synopsis","text":"display details about a cluster workflow template argo cluster - template get CLUSTER WORKFLOW_TEMPLATE ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_cluster-template_get/#options","text":"- h , -- help help for get - o , -- output string Output format . One of : json | yaml | wide","title":"Options"},{"location":"cli/argo_cluster-template_get/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_cluster-template_get/#see-also","text":"argo cluster-template - manipulate cluster workflow templates","title":"SEE ALSO"},{"location":"cli/argo_cluster-template_lint/","text":"argo cluster-template lint \u00b6 validate files or directories of cluster workflow template manifests Synopsis \u00b6 validate files or directories of cluster workflow template manifests argo cluster - template lint FILE ... [ flags ] Options \u00b6 - h , -- help help for lint -- strict perform strict workflow validation ( default true ) Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo cluster-template - manipulate cluster workflow templates","title":"argo cluster-template lint"},{"location":"cli/argo_cluster-template_lint/#argo-cluster-template-lint","text":"validate files or directories of cluster workflow template manifests","title":"argo cluster-template lint"},{"location":"cli/argo_cluster-template_lint/#synopsis","text":"validate files or directories of cluster workflow template manifests argo cluster - template lint FILE ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_cluster-template_lint/#options","text":"- h , -- help help for lint -- strict perform strict workflow validation ( default true )","title":"Options"},{"location":"cli/argo_cluster-template_lint/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_cluster-template_lint/#see-also","text":"argo cluster-template - manipulate cluster workflow templates","title":"SEE ALSO"},{"location":"cli/argo_cluster-template_list/","text":"argo cluster-template list \u00b6 list cluster workflow templates Synopsis \u00b6 list cluster workflow templates argo cluster - template list [ flags ] Options \u00b6 - h , -- help help for list - o , -- output string Output format . One of : wide | name Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo cluster-template - manipulate cluster workflow templates","title":"argo cluster-template list"},{"location":"cli/argo_cluster-template_list/#argo-cluster-template-list","text":"list cluster workflow templates","title":"argo cluster-template list"},{"location":"cli/argo_cluster-template_list/#synopsis","text":"list cluster workflow templates argo cluster - template list [ flags ]","title":"Synopsis"},{"location":"cli/argo_cluster-template_list/#options","text":"- h , -- help help for list - o , -- output string Output format . One of : wide | name","title":"Options"},{"location":"cli/argo_cluster-template_list/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_cluster-template_list/#see-also","text":"argo cluster-template - manipulate cluster workflow templates","title":"SEE ALSO"},{"location":"cli/argo_completion/","text":"argo completion \u00b6 output shell completion code for the specified shell (bash or zsh) Synopsis \u00b6 Write bash or zsh shell completion code to standard output. For bash, ensure you have bash completions installed and enabled. To access completions in your current shell, run $ source <(argo completion bash) Alternatively, write it to a file and source in .bash_profile For zsh, output to a file in a directory referenced by the $fpath shell variable. argo completion SHELL [ flags ] Options \u00b6 - h , -- help help for completion Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo completion"},{"location":"cli/argo_completion/#argo-completion","text":"output shell completion code for the specified shell (bash or zsh)","title":"argo completion"},{"location":"cli/argo_completion/#synopsis","text":"Write bash or zsh shell completion code to standard output. For bash, ensure you have bash completions installed and enabled. To access completions in your current shell, run $ source <(argo completion bash) Alternatively, write it to a file and source in .bash_profile For zsh, output to a file in a directory referenced by the $fpath shell variable. argo completion SHELL [ flags ]","title":"Synopsis"},{"location":"cli/argo_completion/#options","text":"- h , -- help help for completion","title":"Options"},{"location":"cli/argo_completion/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_completion/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_cron/","text":"argo cron \u00b6 manage cron workflows Synopsis \u00b6 manage cron workflows argo cron [ flags ] Options \u00b6 - h , -- help help for cron Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo argo cron create - create a cron workflow argo cron delete - delete a cron workflow argo cron get - display details about a cron workflow argo cron lint - validate files or directories of cron workflow manifests argo cron list - list cron workflows argo cron resume - resume zero or more cron workflows argo cron suspend - suspend zero or more cron workflows","title":"argo cron"},{"location":"cli/argo_cron/#argo-cron","text":"manage cron workflows","title":"argo cron"},{"location":"cli/argo_cron/#synopsis","text":"manage cron workflows argo cron [ flags ]","title":"Synopsis"},{"location":"cli/argo_cron/#options","text":"- h , -- help help for cron","title":"Options"},{"location":"cli/argo_cron/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_cron/#see-also","text":"argo - argo is the command line interface to Argo argo cron create - create a cron workflow argo cron delete - delete a cron workflow argo cron get - display details about a cron workflow argo cron lint - validate files or directories of cron workflow manifests argo cron list - list cron workflows argo cron resume - resume zero or more cron workflows argo cron suspend - suspend zero or more cron workflows","title":"SEE ALSO"},{"location":"cli/argo_cron_create/","text":"argo cron create \u00b6 create a cron workflow Synopsis \u00b6 create a cron workflow argo cron create FILE1 FILE2 ... [ flags ] Options \u00b6 - h , -- help help for create - o , -- output string Output format . One of : name | json | yaml | wide -- schedule string override cron workflow schedule -- strict perform strict workflow validation ( default true ) Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo cron - manage cron workflows","title":"argo cron create"},{"location":"cli/argo_cron_create/#argo-cron-create","text":"create a cron workflow","title":"argo cron create"},{"location":"cli/argo_cron_create/#synopsis","text":"create a cron workflow argo cron create FILE1 FILE2 ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_cron_create/#options","text":"- h , -- help help for create - o , -- output string Output format . One of : name | json | yaml | wide -- schedule string override cron workflow schedule -- strict perform strict workflow validation ( default true )","title":"Options"},{"location":"cli/argo_cron_create/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_cron_create/#see-also","text":"argo cron - manage cron workflows","title":"SEE ALSO"},{"location":"cli/argo_cron_delete/","text":"argo cron delete \u00b6 delete a cron workflow Synopsis \u00b6 delete a cron workflow argo cron delete [ CRON_WORKFLOW... | --all ] [ flags ] Options \u00b6 -- all Delete all workflow templates - h , -- help help for delete Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo cron - manage cron workflows","title":"argo cron delete"},{"location":"cli/argo_cron_delete/#argo-cron-delete","text":"delete a cron workflow","title":"argo cron delete"},{"location":"cli/argo_cron_delete/#synopsis","text":"delete a cron workflow argo cron delete [ CRON_WORKFLOW... | --all ] [ flags ]","title":"Synopsis"},{"location":"cli/argo_cron_delete/#options","text":"-- all Delete all workflow templates - h , -- help help for delete","title":"Options"},{"location":"cli/argo_cron_delete/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_cron_delete/#see-also","text":"argo cron - manage cron workflows","title":"SEE ALSO"},{"location":"cli/argo_cron_get/","text":"argo cron get \u00b6 display details about a cron workflow Synopsis \u00b6 display details about a cron workflow argo cron get CRON_WORKFLOW ... [ flags ] Options \u00b6 - h , -- help help for get - o , -- output string Output format . One of : json | yaml | wide Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo cron - manage cron workflows","title":"argo cron get"},{"location":"cli/argo_cron_get/#argo-cron-get","text":"display details about a cron workflow","title":"argo cron get"},{"location":"cli/argo_cron_get/#synopsis","text":"display details about a cron workflow argo cron get CRON_WORKFLOW ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_cron_get/#options","text":"- h , -- help help for get - o , -- output string Output format . One of : json | yaml | wide","title":"Options"},{"location":"cli/argo_cron_get/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_cron_get/#see-also","text":"argo cron - manage cron workflows","title":"SEE ALSO"},{"location":"cli/argo_cron_lint/","text":"argo cron lint \u00b6 validate files or directories of cron workflow manifests Synopsis \u00b6 validate files or directories of cron workflow manifests argo cron lint FILE ... [ flags ] Options \u00b6 - h , -- help help for lint -- strict perform strict workflow validation ( default true ) Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo cron - manage cron workflows","title":"argo cron lint"},{"location":"cli/argo_cron_lint/#argo-cron-lint","text":"validate files or directories of cron workflow manifests","title":"argo cron lint"},{"location":"cli/argo_cron_lint/#synopsis","text":"validate files or directories of cron workflow manifests argo cron lint FILE ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_cron_lint/#options","text":"- h , -- help help for lint -- strict perform strict workflow validation ( default true )","title":"Options"},{"location":"cli/argo_cron_lint/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_cron_lint/#see-also","text":"argo cron - manage cron workflows","title":"SEE ALSO"},{"location":"cli/argo_cron_list/","text":"argo cron list \u00b6 list cron workflows Synopsis \u00b6 list cron workflows argo cron list [ flags ] Options \u00b6 -- all - namespaces Show workflows from all namespaces - h , -- help help for list - o , -- output string Output format . One of : wide | name Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo cron - manage cron workflows","title":"argo cron list"},{"location":"cli/argo_cron_list/#argo-cron-list","text":"list cron workflows","title":"argo cron list"},{"location":"cli/argo_cron_list/#synopsis","text":"list cron workflows argo cron list [ flags ]","title":"Synopsis"},{"location":"cli/argo_cron_list/#options","text":"-- all - namespaces Show workflows from all namespaces - h , -- help help for list - o , -- output string Output format . One of : wide | name","title":"Options"},{"location":"cli/argo_cron_list/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_cron_list/#see-also","text":"argo cron - manage cron workflows","title":"SEE ALSO"},{"location":"cli/argo_cron_resume/","text":"argo cron resume \u00b6 resume zero or more cron workflows Synopsis \u00b6 resume zero or more cron workflows argo cron resume [ CRON_WORKFLOW... ] [ flags ] Options \u00b6 - h , -- help help for resume Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo cron - manage cron workflows","title":"argo cron resume"},{"location":"cli/argo_cron_resume/#argo-cron-resume","text":"resume zero or more cron workflows","title":"argo cron resume"},{"location":"cli/argo_cron_resume/#synopsis","text":"resume zero or more cron workflows argo cron resume [ CRON_WORKFLOW... ] [ flags ]","title":"Synopsis"},{"location":"cli/argo_cron_resume/#options","text":"- h , -- help help for resume","title":"Options"},{"location":"cli/argo_cron_resume/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_cron_resume/#see-also","text":"argo cron - manage cron workflows","title":"SEE ALSO"},{"location":"cli/argo_cron_suspend/","text":"argo cron suspend \u00b6 suspend zero or more cron workflows Synopsis \u00b6 suspend zero or more cron workflows argo cron suspend CRON_WORKFLOW ... [ flags ] Options \u00b6 - h , -- help help for suspend Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo cron - manage cron workflows","title":"argo cron suspend"},{"location":"cli/argo_cron_suspend/#argo-cron-suspend","text":"suspend zero or more cron workflows","title":"argo cron suspend"},{"location":"cli/argo_cron_suspend/#synopsis","text":"suspend zero or more cron workflows argo cron suspend CRON_WORKFLOW ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_cron_suspend/#options","text":"- h , -- help help for suspend","title":"Options"},{"location":"cli/argo_cron_suspend/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_cron_suspend/#see-also","text":"argo cron - manage cron workflows","title":"SEE ALSO"},{"location":"cli/argo_delete/","text":"argo delete \u00b6 delete workflows Synopsis \u00b6 delete workflows argo delete [ --dry-run ] [ WORKFLOW...|[--all ] [ --older ] [ --completed ] [ --prefix PREFIX ] [ --selector SELECTOR ] ] [ flags ] Examples \u00b6 # Delete a workflow : argo delete my - wf # Delete the latest workflow : argo delete @latest Options \u00b6 -- all Delete all workflows -- all - namespaces Delete workflows from all namespaces -- completed Delete completed workflows -- dry - run Do not delete the workflow , only print what would happen - h , -- help help for delete -- older string Delete completed workflows finished before the specified duration ( e . g . 10 m , 3 h , 1 d ) -- prefix string Delete workflows by prefix - l , -- selector string Selector ( label query ) to filter on , not including uninitialized ones Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo delete"},{"location":"cli/argo_delete/#argo-delete","text":"delete workflows","title":"argo delete"},{"location":"cli/argo_delete/#synopsis","text":"delete workflows argo delete [ --dry-run ] [ WORKFLOW...|[--all ] [ --older ] [ --completed ] [ --prefix PREFIX ] [ --selector SELECTOR ] ] [ flags ]","title":"Synopsis"},{"location":"cli/argo_delete/#examples","text":"# Delete a workflow : argo delete my - wf # Delete the latest workflow : argo delete @latest","title":"Examples"},{"location":"cli/argo_delete/#options","text":"-- all Delete all workflows -- all - namespaces Delete workflows from all namespaces -- completed Delete completed workflows -- dry - run Do not delete the workflow , only print what would happen - h , -- help help for delete -- older string Delete completed workflows finished before the specified duration ( e . g . 10 m , 3 h , 1 d ) -- prefix string Delete workflows by prefix - l , -- selector string Selector ( label query ) to filter on , not including uninitialized ones","title":"Options"},{"location":"cli/argo_delete/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_delete/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_get/","text":"argo get \u00b6 display details about a workflow Synopsis \u00b6 display details about a workflow argo get WORKFLOW ... [ flags ] Examples \u00b6 # Get information about a workflow : argo get my - wf # Get the latest workflow : argo get @latest Options \u00b6 - h , -- help help for get -- no - color Disable colorized output -- node - field - selector string selector of node to display , eg : -- node - field - selector phase = abc - o , -- output string Output format . One of : json | yaml | wide -- status string Filter by status ( Pending , Running , Succeeded , Skipped , Failed , Error ) Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo get"},{"location":"cli/argo_get/#argo-get","text":"display details about a workflow","title":"argo get"},{"location":"cli/argo_get/#synopsis","text":"display details about a workflow argo get WORKFLOW ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_get/#examples","text":"# Get information about a workflow : argo get my - wf # Get the latest workflow : argo get @latest","title":"Examples"},{"location":"cli/argo_get/#options","text":"- h , -- help help for get -- no - color Disable colorized output -- node - field - selector string selector of node to display , eg : -- node - field - selector phase = abc - o , -- output string Output format . One of : json | yaml | wide -- status string Filter by status ( Pending , Running , Succeeded , Skipped , Failed , Error )","title":"Options"},{"location":"cli/argo_get/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_get/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_lint/","text":"argo lint \u00b6 validate files or directories of workflow manifests Synopsis \u00b6 validate files or directories of workflow manifests argo lint FILE ... [ flags ] Options \u00b6 - h , -- help help for lint -- strict perform strict workflow validation ( default true ) Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo lint"},{"location":"cli/argo_lint/#argo-lint","text":"validate files or directories of workflow manifests","title":"argo lint"},{"location":"cli/argo_lint/#synopsis","text":"validate files or directories of workflow manifests argo lint FILE ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_lint/#options","text":"- h , -- help help for lint -- strict perform strict workflow validation ( default true )","title":"Options"},{"location":"cli/argo_lint/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_lint/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_list/","text":"argo list \u00b6 list workflows Synopsis \u00b6 list workflows argo list [ flags ] Options \u00b6 -- all - namespaces Show workflows from all namespaces -- chunk - size int Return large lists in chunks rather than all at once . Pass 0 to disable . -- completed Show only completed workflows -- field - selector string Selector ( field query ) to filter on , supports ' = ' , ' == ' , and ' != ' . ( e . g . -- field - selectorkey1 = value1 , key2 = value2 ) . The server only supports a limited number of field queries per type . - h , -- help help for list -- no - headers Don ' t print headers (default print headers). -- older string List completed workflows finished before the specified duration ( e . g . 10 m , 3 h , 1 d ) - o , -- output string Output format . One of : wide | name -- prefix string Filter workflows by prefix -- running Show only running workflows - l , -- selector string Selector ( label query ) to filter on , supports ' = ' , ' == ' , and ' != ' . ( e . g . - l key1 = value1 , key2 = value2 ) -- since string Show only workflows created after than a relative duration -- status strings Filter by status ( comma separated ) Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo list"},{"location":"cli/argo_list/#argo-list","text":"list workflows","title":"argo list"},{"location":"cli/argo_list/#synopsis","text":"list workflows argo list [ flags ]","title":"Synopsis"},{"location":"cli/argo_list/#options","text":"-- all - namespaces Show workflows from all namespaces -- chunk - size int Return large lists in chunks rather than all at once . Pass 0 to disable . -- completed Show only completed workflows -- field - selector string Selector ( field query ) to filter on , supports ' = ' , ' == ' , and ' != ' . ( e . g . -- field - selectorkey1 = value1 , key2 = value2 ) . The server only supports a limited number of field queries per type . - h , -- help help for list -- no - headers Don ' t print headers (default print headers). -- older string List completed workflows finished before the specified duration ( e . g . 10 m , 3 h , 1 d ) - o , -- output string Output format . One of : wide | name -- prefix string Filter workflows by prefix -- running Show only running workflows - l , -- selector string Selector ( label query ) to filter on , supports ' = ' , ' == ' , and ' != ' . ( e . g . - l key1 = value1 , key2 = value2 ) -- since string Show only workflows created after than a relative duration -- status strings Filter by status ( comma separated )","title":"Options"},{"location":"cli/argo_list/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_list/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_logs/","text":"argo logs \u00b6 view logs of a pod or workflow Synopsis \u00b6 view logs of a pod or workflow argo logs WORKFLOW [ POD ] [ flags ] Examples \u00b6 # Print the logs of a workflow : argo logs my - wf # Follow the logs of a workflows : argo logs my - wf --follow # Print the logs of single container in a pod argo logs my - wf my - pod - c my - container # Print the logs of a workflow ' s pods : argo logs my - wf my - pod # Print the logs of a pods : argo logs --since=1h my-pod # Print the logs of the latest workflow : argo logs @latest Options \u00b6 - c , -- container string Print the logs of this container ( default \" main \" ) - f , -- follow Specify if the logs should be streamed . - h , -- help help for logs -- no - color Disable colorized output -- since duration Only return logs newer than a relative duration like 5 s , 2 m , or 3 h . Defaults to all logs . Only one of since - time / since may be used . -- since - time string Only return logs after a specific date ( RFC3339 ) . Defaults to all logs . Only one of since - time / since may be used . -- tail int If set , the number of lines from the end of the logs to show . If not specified , logs are shown from the creation of the container or sinceSeconds or sinceTime ( default - 1 ) -- timestamps Include timestamps on each line in the log output Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo logs"},{"location":"cli/argo_logs/#argo-logs","text":"view logs of a pod or workflow","title":"argo logs"},{"location":"cli/argo_logs/#synopsis","text":"view logs of a pod or workflow argo logs WORKFLOW [ POD ] [ flags ]","title":"Synopsis"},{"location":"cli/argo_logs/#examples","text":"# Print the logs of a workflow : argo logs my - wf # Follow the logs of a workflows : argo logs my - wf --follow # Print the logs of single container in a pod argo logs my - wf my - pod - c my - container # Print the logs of a workflow ' s pods : argo logs my - wf my - pod # Print the logs of a pods : argo logs --since=1h my-pod # Print the logs of the latest workflow : argo logs @latest","title":"Examples"},{"location":"cli/argo_logs/#options","text":"- c , -- container string Print the logs of this container ( default \" main \" ) - f , -- follow Specify if the logs should be streamed . - h , -- help help for logs -- no - color Disable colorized output -- since duration Only return logs newer than a relative duration like 5 s , 2 m , or 3 h . Defaults to all logs . Only one of since - time / since may be used . -- since - time string Only return logs after a specific date ( RFC3339 ) . Defaults to all logs . Only one of since - time / since may be used . -- tail int If set , the number of lines from the end of the logs to show . If not specified , logs are shown from the creation of the container or sinceSeconds or sinceTime ( default - 1 ) -- timestamps Include timestamps on each line in the log output","title":"Options"},{"location":"cli/argo_logs/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_logs/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_resubmit/","text":"argo resubmit \u00b6 resubmit one or more workflows Synopsis \u00b6 resubmit one or more workflows argo resubmit [ WORKFLOW... ] [ flags ] Examples \u00b6 # Resubmit a workflow : argo resubmit my - wf # Resubmit and wait for completion : argo resubmit -- wait my - wf . yaml # Resubmit and watch until completion : argo resubmit -- watch my - wf . yaml # Resubmit and tail logs until completion : argo resubmit -- log my - wf . yaml # Resubmit the latest workflow : argo resubmit @ latest Options \u00b6 - h , -- help help for resubmit -- log log the workflow until it completes -- memoized re - use successful steps & outputs from the previous run ( experimental ) - o , -- output string Output format . One of : name | json | yaml | wide -- priority int32 workflow priority - w , -- wait wait for the workflow to complete -- watch watch the workflow until it completes Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo resubmit"},{"location":"cli/argo_resubmit/#argo-resubmit","text":"resubmit one or more workflows","title":"argo resubmit"},{"location":"cli/argo_resubmit/#synopsis","text":"resubmit one or more workflows argo resubmit [ WORKFLOW... ] [ flags ]","title":"Synopsis"},{"location":"cli/argo_resubmit/#examples","text":"# Resubmit a workflow : argo resubmit my - wf # Resubmit and wait for completion : argo resubmit -- wait my - wf . yaml # Resubmit and watch until completion : argo resubmit -- watch my - wf . yaml # Resubmit and tail logs until completion : argo resubmit -- log my - wf . yaml # Resubmit the latest workflow : argo resubmit @ latest","title":"Examples"},{"location":"cli/argo_resubmit/#options","text":"- h , -- help help for resubmit -- log log the workflow until it completes -- memoized re - use successful steps & outputs from the previous run ( experimental ) - o , -- output string Output format . One of : name | json | yaml | wide -- priority int32 workflow priority - w , -- wait wait for the workflow to complete -- watch watch the workflow until it completes","title":"Options"},{"location":"cli/argo_resubmit/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_resubmit/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_resume/","text":"argo resume \u00b6 resume zero or more workflows Synopsis \u00b6 resume zero or more workflows argo resume WORKFLOW1 WORKFLOW2 ... [ flags ] Examples \u00b6 # Resume a workflow that has been stopped or suspended : argo resume my - wf # Resume the latest workflow : argo resume @latest Options \u00b6 - h , -- help help for resume -- node - field - selector string selector of node to resume , eg : -- node - field - selector inputs . paramaters . myparam . value = abc Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo resume"},{"location":"cli/argo_resume/#argo-resume","text":"resume zero or more workflows","title":"argo resume"},{"location":"cli/argo_resume/#synopsis","text":"resume zero or more workflows argo resume WORKFLOW1 WORKFLOW2 ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_resume/#examples","text":"# Resume a workflow that has been stopped or suspended : argo resume my - wf # Resume the latest workflow : argo resume @latest","title":"Examples"},{"location":"cli/argo_resume/#options","text":"- h , -- help help for resume -- node - field - selector string selector of node to resume , eg : -- node - field - selector inputs . paramaters . myparam . value = abc","title":"Options"},{"location":"cli/argo_resume/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_resume/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_retry/","text":"argo retry \u00b6 retry zero or more workflows Synopsis \u00b6 retry zero or more workflows argo retry [ WORKFLOW... ] [ flags ] Examples \u00b6 # Retry a workflow : argo retry my - wf # Retry several workflows : argo retry my - wf my - other - wf my - third - wf # Retry and wait for completion : argo retry -- wait my - wf . yaml # Retry and watch until completion : argo retry -- watch my - wf . yaml # Retry and tail logs until completion : argo retry -- log my - wf . yaml # Retry the latest workflow : argo retry @ latest Options \u00b6 - h , -- help help for retry -- log log the workflow until it completes -- node - field - selector string selector of nodes to reset , eg : -- node - field - selector inputs . paramaters . myparam . value = abc - o , -- output string Output format . One of : name | json | yaml | wide -- restart - successful indicates to restart successful nodes matching the -- node - field - selector - w , -- wait wait for the workflow to complete -- watch watch the workflow until it completes Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo retry"},{"location":"cli/argo_retry/#argo-retry","text":"retry zero or more workflows","title":"argo retry"},{"location":"cli/argo_retry/#synopsis","text":"retry zero or more workflows argo retry [ WORKFLOW... ] [ flags ]","title":"Synopsis"},{"location":"cli/argo_retry/#examples","text":"# Retry a workflow : argo retry my - wf # Retry several workflows : argo retry my - wf my - other - wf my - third - wf # Retry and wait for completion : argo retry -- wait my - wf . yaml # Retry and watch until completion : argo retry -- watch my - wf . yaml # Retry and tail logs until completion : argo retry -- log my - wf . yaml # Retry the latest workflow : argo retry @ latest","title":"Examples"},{"location":"cli/argo_retry/#options","text":"- h , -- help help for retry -- log log the workflow until it completes -- node - field - selector string selector of nodes to reset , eg : -- node - field - selector inputs . paramaters . myparam . value = abc - o , -- output string Output format . One of : name | json | yaml | wide -- restart - successful indicates to restart successful nodes matching the -- node - field - selector - w , -- wait wait for the workflow to complete -- watch watch the workflow until it completes","title":"Options"},{"location":"cli/argo_retry/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_retry/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_server/","text":"argo server \u00b6 Start the Argo Server Synopsis \u00b6 Start the Argo Server argo server [ flags ] Examples \u00b6 See https : // github . com / argoproj / argo / blob / master / docs / argo - server . md Options \u00b6 -- auth - mode stringArray API server authentication mode . One of : client | server | sso ( default [ server ]) -- basehref string Value for base href in index . html . Used if the server is running behind reverse proxy under subpath different from / . Defaults to the environment variable BASE_HREF . ( default \"/\" ) - b , -- browser enable automatic launching of the browser [ local mode ] -- configmap string Name of K8s configmap to retrieve workflow controller configuration ( default \"workflow-controller-configmap\" ) - h , -- help help for server -- hsts Whether or not we should add a HTTP Secure Transport Security header . This only has effect if secure is enabled . ( default true ) -- managed - namespace string namespace that watches , default to the installation namespace -- namespaced run as namespaced mode - p , -- port int Port to listen on ( default 2746 ) Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo server"},{"location":"cli/argo_server/#argo-server","text":"Start the Argo Server","title":"argo server"},{"location":"cli/argo_server/#synopsis","text":"Start the Argo Server argo server [ flags ]","title":"Synopsis"},{"location":"cli/argo_server/#examples","text":"See https : // github . com / argoproj / argo / blob / master / docs / argo - server . md","title":"Examples"},{"location":"cli/argo_server/#options","text":"-- auth - mode stringArray API server authentication mode . One of : client | server | sso ( default [ server ]) -- basehref string Value for base href in index . html . Used if the server is running behind reverse proxy under subpath different from / . Defaults to the environment variable BASE_HREF . ( default \"/\" ) - b , -- browser enable automatic launching of the browser [ local mode ] -- configmap string Name of K8s configmap to retrieve workflow controller configuration ( default \"workflow-controller-configmap\" ) - h , -- help help for server -- hsts Whether or not we should add a HTTP Secure Transport Security header . This only has effect if secure is enabled . ( default true ) -- managed - namespace string namespace that watches , default to the installation namespace -- namespaced run as namespaced mode - p , -- port int Port to listen on ( default 2746 )","title":"Options"},{"location":"cli/argo_server/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_server/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_stop/","text":"argo stop \u00b6 stop zero or more workflows Synopsis \u00b6 stop zero or more workflows argo stop WORKFLOW WORKFLOW2 ... [ flags ] Examples \u00b6 # Stop about a workflow : argo stop my - wf # Stop the latest workflow : argo stop @latest Options \u00b6 - h , -- help help for stop -- message string Message to add to previously running nodes -- node - field - selector string selector of node to stop , eg : -- node - field - selector inputs . paramaters . myparam . value = abc Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo stop"},{"location":"cli/argo_stop/#argo-stop","text":"stop zero or more workflows","title":"argo stop"},{"location":"cli/argo_stop/#synopsis","text":"stop zero or more workflows argo stop WORKFLOW WORKFLOW2 ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_stop/#examples","text":"# Stop about a workflow : argo stop my - wf # Stop the latest workflow : argo stop @latest","title":"Examples"},{"location":"cli/argo_stop/#options","text":"- h , -- help help for stop -- message string Message to add to previously running nodes -- node - field - selector string selector of node to stop , eg : -- node - field - selector inputs . paramaters . myparam . value = abc","title":"Options"},{"location":"cli/argo_stop/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_stop/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_submit/","text":"argo submit \u00b6 submit a workflow Synopsis \u00b6 submit a workflow argo submit [ FILE... | --from `kind/name ] [ flags ] Examples \u00b6 # Submit multiple workflows from files : argo submit my - wf . yaml # Submit and wait for completion : argo submit -- wait my - wf . yaml # Submit and watch until completion : argo submit -- watch my - wf . yaml # Submit and tail logs until completion : argo submit -- log my - wf . yaml # Submit a single workflow from an existing resource argo submit -- from cronwf / my - cron - wf Options \u00b6 -- dry - run modify the workflow on the client - side without creating it -- entrypoint string override entrypoint -- from kind / name Submit from an existing kind / name E . g ., -- from = cronwf / hello - world - cwf -- generate - name string override metadata . generateName - h , -- help help for submit - l , -- labels string Comma separated labels to apply to the workflow . Will override previous values . -- log log the workflow until it completes -- name string override metadata . name -- node - field - selector string selector of node to display , eg : -- node - field - selector phase = abc - o , -- output string Output format . One of : name | json | yaml | wide - p , -- parameter stringArray pass an input parameter - f , -- parameter - file string pass a file containing all input parameters -- priority int32 workflow priority -- server - dry - run send request to server with dry - run flag which will modify the workflow without creating it -- serviceaccount string run all pods in the workflow using specified serviceaccount -- status string Filter by status ( Pending , Running , Succeeded , Skipped , Failed , Error ) . Should only be used with -- watch . -- strict perform strict workflow validation ( default true ) - w , -- wait wait for the workflow to complete -- watch watch the workflow until it completes Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo submit"},{"location":"cli/argo_submit/#argo-submit","text":"submit a workflow","title":"argo submit"},{"location":"cli/argo_submit/#synopsis","text":"submit a workflow argo submit [ FILE... | --from `kind/name ] [ flags ]","title":"Synopsis"},{"location":"cli/argo_submit/#examples","text":"# Submit multiple workflows from files : argo submit my - wf . yaml # Submit and wait for completion : argo submit -- wait my - wf . yaml # Submit and watch until completion : argo submit -- watch my - wf . yaml # Submit and tail logs until completion : argo submit -- log my - wf . yaml # Submit a single workflow from an existing resource argo submit -- from cronwf / my - cron - wf","title":"Examples"},{"location":"cli/argo_submit/#options","text":"-- dry - run modify the workflow on the client - side without creating it -- entrypoint string override entrypoint -- from kind / name Submit from an existing kind / name E . g ., -- from = cronwf / hello - world - cwf -- generate - name string override metadata . generateName - h , -- help help for submit - l , -- labels string Comma separated labels to apply to the workflow . Will override previous values . -- log log the workflow until it completes -- name string override metadata . name -- node - field - selector string selector of node to display , eg : -- node - field - selector phase = abc - o , -- output string Output format . One of : name | json | yaml | wide - p , -- parameter stringArray pass an input parameter - f , -- parameter - file string pass a file containing all input parameters -- priority int32 workflow priority -- server - dry - run send request to server with dry - run flag which will modify the workflow without creating it -- serviceaccount string run all pods in the workflow using specified serviceaccount -- status string Filter by status ( Pending , Running , Succeeded , Skipped , Failed , Error ) . Should only be used with -- watch . -- strict perform strict workflow validation ( default true ) - w , -- wait wait for the workflow to complete -- watch watch the workflow until it completes","title":"Options"},{"location":"cli/argo_submit/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_submit/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_suspend/","text":"argo suspend \u00b6 suspend zero or more workflow Synopsis \u00b6 suspend zero or more workflow argo suspend WORKFLOW1 WORKFLOW2 ... [ flags ] Examples \u00b6 # Suspend a workflow : argo suspend my - wf # Suspend the latest workflow : argo suspend @latest Options \u00b6 - h , -- help help for suspend Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo suspend"},{"location":"cli/argo_suspend/#argo-suspend","text":"suspend zero or more workflow","title":"argo suspend"},{"location":"cli/argo_suspend/#synopsis","text":"suspend zero or more workflow argo suspend WORKFLOW1 WORKFLOW2 ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_suspend/#examples","text":"# Suspend a workflow : argo suspend my - wf # Suspend the latest workflow : argo suspend @latest","title":"Examples"},{"location":"cli/argo_suspend/#options","text":"- h , -- help help for suspend","title":"Options"},{"location":"cli/argo_suspend/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_suspend/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_template/","text":"argo template \u00b6 manipulate workflow templates Synopsis \u00b6 manipulate workflow templates argo template [ flags ] Options \u00b6 - h , -- help help for template Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo argo template create - create a workflow template argo template delete - delete a workflow template argo template get - display details about a workflow template argo template lint - validate a file or directory of workflow template manifests argo template list - list workflow templates","title":"argo template"},{"location":"cli/argo_template/#argo-template","text":"manipulate workflow templates","title":"argo template"},{"location":"cli/argo_template/#synopsis","text":"manipulate workflow templates argo template [ flags ]","title":"Synopsis"},{"location":"cli/argo_template/#options","text":"- h , -- help help for template","title":"Options"},{"location":"cli/argo_template/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_template/#see-also","text":"argo - argo is the command line interface to Argo argo template create - create a workflow template argo template delete - delete a workflow template argo template get - display details about a workflow template argo template lint - validate a file or directory of workflow template manifests argo template list - list workflow templates","title":"SEE ALSO"},{"location":"cli/argo_template_create/","text":"argo template create \u00b6 create a workflow template Synopsis \u00b6 create a workflow template argo template create FILE1 FILE2 ... [ flags ] Options \u00b6 - h , -- help help for create - o , -- output string Output format . One of : name | json | yaml | wide -- strict perform strict workflow validation ( default true ) Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo template - manipulate workflow templates","title":"argo template create"},{"location":"cli/argo_template_create/#argo-template-create","text":"create a workflow template","title":"argo template create"},{"location":"cli/argo_template_create/#synopsis","text":"create a workflow template argo template create FILE1 FILE2 ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_template_create/#options","text":"- h , -- help help for create - o , -- output string Output format . One of : name | json | yaml | wide -- strict perform strict workflow validation ( default true )","title":"Options"},{"location":"cli/argo_template_create/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_template_create/#see-also","text":"argo template - manipulate workflow templates","title":"SEE ALSO"},{"location":"cli/argo_template_delete/","text":"argo template delete \u00b6 delete a workflow template Synopsis \u00b6 delete a workflow template argo template delete WORKFLOW_TEMPLATE [ flags ] Options \u00b6 -- all Delete all workflow templates - h , -- help help for delete Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo template - manipulate workflow templates","title":"argo template delete"},{"location":"cli/argo_template_delete/#argo-template-delete","text":"delete a workflow template","title":"argo template delete"},{"location":"cli/argo_template_delete/#synopsis","text":"delete a workflow template argo template delete WORKFLOW_TEMPLATE [ flags ]","title":"Synopsis"},{"location":"cli/argo_template_delete/#options","text":"-- all Delete all workflow templates - h , -- help help for delete","title":"Options"},{"location":"cli/argo_template_delete/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_template_delete/#see-also","text":"argo template - manipulate workflow templates","title":"SEE ALSO"},{"location":"cli/argo_template_get/","text":"argo template get \u00b6 display details about a workflow template Synopsis \u00b6 display details about a workflow template argo template get WORKFLOW_TEMPLATE ... [ flags ] Options \u00b6 - h , -- help help for get - o , -- output string Output format . One of : json | yaml | wide Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo template - manipulate workflow templates","title":"argo template get"},{"location":"cli/argo_template_get/#argo-template-get","text":"display details about a workflow template","title":"argo template get"},{"location":"cli/argo_template_get/#synopsis","text":"display details about a workflow template argo template get WORKFLOW_TEMPLATE ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_template_get/#options","text":"- h , -- help help for get - o , -- output string Output format . One of : json | yaml | wide","title":"Options"},{"location":"cli/argo_template_get/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_template_get/#see-also","text":"argo template - manipulate workflow templates","title":"SEE ALSO"},{"location":"cli/argo_template_lint/","text":"argo template lint \u00b6 validate a file or directory of workflow template manifests Synopsis \u00b6 validate a file or directory of workflow template manifests argo template lint ( DIRECTORY | FILE1 FILE2 FILE3 ...) [ flags ] Options \u00b6 - h , -- help help for lint -- strict perform strict workflow validation ( default true ) Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo template - manipulate workflow templates","title":"argo template lint"},{"location":"cli/argo_template_lint/#argo-template-lint","text":"validate a file or directory of workflow template manifests","title":"argo template lint"},{"location":"cli/argo_template_lint/#synopsis","text":"validate a file or directory of workflow template manifests argo template lint ( DIRECTORY | FILE1 FILE2 FILE3 ...) [ flags ]","title":"Synopsis"},{"location":"cli/argo_template_lint/#options","text":"- h , -- help help for lint -- strict perform strict workflow validation ( default true )","title":"Options"},{"location":"cli/argo_template_lint/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_template_lint/#see-also","text":"argo template - manipulate workflow templates","title":"SEE ALSO"},{"location":"cli/argo_template_list/","text":"argo template list \u00b6 list workflow templates Synopsis \u00b6 list workflow templates argo template list [ flags ] Options \u00b6 -- all - namespaces Show workflows from all namespaces - h , -- help help for list - o , -- output string Output format . One of : wide | name Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo template - manipulate workflow templates","title":"argo template list"},{"location":"cli/argo_template_list/#argo-template-list","text":"list workflow templates","title":"argo template list"},{"location":"cli/argo_template_list/#synopsis","text":"list workflow templates argo template list [ flags ]","title":"Synopsis"},{"location":"cli/argo_template_list/#options","text":"-- all - namespaces Show workflows from all namespaces - h , -- help help for list - o , -- output string Output format . One of : wide | name","title":"Options"},{"location":"cli/argo_template_list/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_template_list/#see-also","text":"argo template - manipulate workflow templates","title":"SEE ALSO"},{"location":"cli/argo_terminate/","text":"argo terminate \u00b6 terminate zero or more workflows Synopsis \u00b6 terminate zero or more workflows argo terminate WORKFLOW WORKFLOW2 ... [ flags ] Examples \u00b6 # Terminate a workflow : argo terminate my - wf # Terminate the latest workflow : argo terminate @latest Options \u00b6 - h , -- help help for terminate Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo terminate"},{"location":"cli/argo_terminate/#argo-terminate","text":"terminate zero or more workflows","title":"argo terminate"},{"location":"cli/argo_terminate/#synopsis","text":"terminate zero or more workflows argo terminate WORKFLOW WORKFLOW2 ... [ flags ]","title":"Synopsis"},{"location":"cli/argo_terminate/#examples","text":"# Terminate a workflow : argo terminate my - wf # Terminate the latest workflow : argo terminate @latest","title":"Examples"},{"location":"cli/argo_terminate/#options","text":"- h , -- help help for terminate","title":"Options"},{"location":"cli/argo_terminate/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_terminate/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_version/","text":"argo version \u00b6 Print version information Synopsis \u00b6 Print version information argo version [ flags ] Options \u00b6 - h , -- help help for version -- short print just the version number Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo version"},{"location":"cli/argo_version/#argo-version","text":"Print version information","title":"argo version"},{"location":"cli/argo_version/#synopsis","text":"Print version information argo version [ flags ]","title":"Synopsis"},{"location":"cli/argo_version/#options","text":"- h , -- help help for version -- short print just the version number","title":"Options"},{"location":"cli/argo_version/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_version/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_wait/","text":"argo wait \u00b6 waits for workflows to complete Synopsis \u00b6 waits for workflows to complete argo wait [ WORKFLOW ...] [ flags ] Examples \u00b6 # Wait on a workflow : argo wait my - wf # Wait on the latest workflow : argo wait @ latest Options \u00b6 - h , -- help help for wait -- ignore - not - found Ignore the wait if the workflow is not found Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo wait"},{"location":"cli/argo_wait/#argo-wait","text":"waits for workflows to complete","title":"argo wait"},{"location":"cli/argo_wait/#synopsis","text":"waits for workflows to complete argo wait [ WORKFLOW ...] [ flags ]","title":"Synopsis"},{"location":"cli/argo_wait/#examples","text":"# Wait on a workflow : argo wait my - wf # Wait on the latest workflow : argo wait @ latest","title":"Examples"},{"location":"cli/argo_wait/#options","text":"- h , -- help help for wait -- ignore - not - found Ignore the wait if the workflow is not found","title":"Options"},{"location":"cli/argo_wait/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_wait/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"cli/argo_watch/","text":"argo watch \u00b6 watch a workflow until it completes Synopsis \u00b6 watch a workflow until it completes argo watch WORKFLOW [ flags ] Examples \u00b6 # Watch a workflow : argo watch my - wf # Watch the latest workflow : argo watch @latest Options \u00b6 - h , -- help help for watch -- node - field - selector string selector of node to display , eg : -- node - field - selector phase = abc -- status string Filter by status ( Pending , Running , Succeeded , Skipped , Failed , Error ) Options inherited from parent commands \u00b6 - s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug SEE ALSO \u00b6 argo - argo is the command line interface to Argo","title":"argo watch"},{"location":"cli/argo_watch/#argo-watch","text":"watch a workflow until it completes","title":"argo watch"},{"location":"cli/argo_watch/#synopsis","text":"watch a workflow until it completes argo watch WORKFLOW [ flags ]","title":"Synopsis"},{"location":"cli/argo_watch/#examples","text":"# Watch a workflow : argo watch my - wf # Watch the latest workflow : argo watch @latest","title":"Examples"},{"location":"cli/argo_watch/#options","text":"- h , -- help help for watch -- node - field - selector string selector of node to display , eg : -- node - field - selector phase = abc -- status string Filter by status ( Pending , Running , Succeeded , Skipped , Failed , Error )","title":"Options"},{"location":"cli/argo_watch/#options-inherited-from-parent-commands","text":"- s , -- argo - server host : port API server host : port . e . g . localhost : 2746 . Defaults to the ARGO_SERVER environment variable . -- as string Username to impersonate for the operation -- as - group stringArray Group to impersonate for the operation , this flag can be repeated to specify multiple groups . -- certificate - authority string Path to a cert file for the certificate authority -- client - certificate string Path to a client certificate file for TLS -- client - key string Path to a client key file for TLS -- cluster string The name of the kubeconfig cluster to use -- context string The name of the kubeconfig context to use -- insecure - skip - tls - verify If true , the server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure - k , -- insecure - skip - verify If true , the Argo Server ' s certificate will not be checked for validity. This will make your HTTPS connections insecure. Defaults to the ARGO_INSECURE_SKIP_VERIFY environment variable. -- instanceid string submit with a specific controller ' s instance id label. Default to the ARGO_INSTANCEID environment variable. -- kubeconfig string Path to a kube config . Only required if out - of - cluster -- loglevel string Set the logging level . One of : debug | info | warn | error ( default \" info \" ) - n , -- namespace string If present , the namespace scope for this CLI request -- password string Password for basic authentication to the API server -- request - timeout string The length of time to wait before giving up on a single server request . Non - zero values should contain a corresponding time unit ( e . g . 1 s , 2 m , 3 h ) . A value of zero means don ' t timeout requests. (default \"0\") - e , -- secure Whether or not the server is using TLS with the Argo Server . Defaults to the ARGO_SECURE environment variable . -- server string The address and port of the Kubernetes API server -- token string Bearer token for authentication to the API server -- user string The name of the kubeconfig user to use -- username string Username for basic authentication to the API server - v , -- verbose Enabled verbose logging , i . e . -- loglevel debug","title":"Options inherited from parent commands"},{"location":"cli/argo_watch/#see-also","text":"argo - argo is the command line interface to Argo","title":"SEE ALSO"},{"location":"examples/","text":"Documentation by Example \u00b6 Welcome! \u00b6 Argo is an open source project that provides container-native workflows for Kubernetes. Each step in an Argo workflow is defined as a container. Argo is implemented as a Kubernetes CRD (Custom Resource Definition). As a result, Argo workflows can be managed using kubectl and natively integrates with other Kubernetes services such as volumes, secrets, and RBAC. The new Argo software is light-weight and installs in under a minute, and provides complete workflow features including parameter substitution, artifacts, fixtures, loops and recursive workflows. Many of the Argo examples used in this walkthrough are available at in this directory. If you like this project, please give us a star! For a complete description of the Argo workflow spec, please refer to our spec definitions . Table of Contents \u00b6 Argo CLI Hello World! Parameters Steps DAG Artifacts The Structure of Workflow Specs Secrets Scripts & Results Output Parameters Loops Conditionals Retrying Failed or Errored Steps Recursion Exit Handlers Timeouts Volumes Suspending Daemon Containers Sidecars Hardwired Artifacts Kubernetes Resources Docker-in-Docker Using Sidecars Custom Template Variable Reference Continuous Integration Example Argo CLI \u00b6 In case you want to follow along with this walkthrough, here's a quick overview of the most useful argo command line interface (CLI) commands. argo submit hello-world.yaml # submit a workflow spec to Kubernetes argo list # list current workflows argo get hello-world-xxx # get info about a specific workflow argo logs -w hello-world-xxx # get logs from all steps in a workflow argo logs hello-world-xxx-yyy # get logs from a specific step in a workflow argo delete hello-world-xxx # delete workflow You can also run workflow specs directly using kubectl but the Argo CLI provides syntax checking, nicer output, and requires less typing. kubectl create -f hello-world.yaml kubectl get wf kubectl get wf hello-world-xxx kubectl get po --selector = workflows.argoproj.io/workflow = hello-world-xxx --show-all # similar to argo kubectl logs hello-world-xxx-yyy -c main kubectl delete wf hello-world-xxx Hello World! \u00b6 Let's start by creating a very simple workflow template to echo \"hello world\" using the docker/whalesay container image from DockerHub. You can run this directly from your shell with a simple docker command: $ docker run docker/whalesay cowsay \"hello world\" _____________ < hello world > ------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___/ === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / === - ~~~ \\_ _____ o __/ \\ \\ __/ \\_ ___ \\_ _____/ Hello from Docker! This message shows that your installation appears to be working correctly. Below, we run the same container on a Kubernetes cluster using an Argo workflow template. Be sure to read the comments as they provide useful explanations. apiVersion : argoproj.io/v1alpha1 kind : Workflow # new type of k8s spec metadata : generateName : hello-world- # name of the workflow spec spec : entrypoint : whalesay # invoke the whalesay template templates : - name : whalesay # name of the template container : image : docker/whalesay command : [ cowsay ] args : [ \"hello world\" ] resources : # limit the resources limits : memory : 32Mi cpu : 100m Argo adds a new kind of Kubernetes spec called a Workflow . The above spec contains a single template called whalesay which runs the docker/whalesay container and invokes cowsay \"hello world\" . The whalesay template is the entrypoint for the spec. The entrypoint specifies the initial template that should be invoked when the workflow spec is executed by Kubernetes. Being able to specify the entrypoint is more useful when there is more than one template defined in the Kubernetes workflow spec. :-) Parameters \u00b6 Let's look at a slightly more complex workflow spec with parameters. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : hello-world-parameters- spec : # invoke the whalesay template with # \"hello world\" as the argument # to the message parameter entrypoint : whalesay arguments : parameters : - name : message value : hello world templates : - name : whalesay inputs : parameters : - name : message # parameter declaration container : # run cowsay with that message input parameter as args image : docker/whalesay command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] This time, the whalesay template takes an input parameter named message that is passed as the args to the cowsay command. In order to reference parameters (e.g., \"{{inputs.parameters.message}}\" ), the parameters must be enclosed in double quotes to escape the curly braces in YAML. The argo CLI provides a convenient way to override parameters used to invoke the entrypoint. For example, the following command would bind the message parameter to \"goodbye world\" instead of the default \"hello world\". argo submit arguments-parameters.yaml -p message = \"goodbye world\" In case of multiple parameters that can be overriten, the argo CLI provides a command to load parameters files in YAML or JSON format. Here is an example of that kind of parameter file: message : goodbye world To run use following command: argo submit arguments-parameters.yaml --parameter-file params.yaml Command-line parameters can also be used to override the default entrypoint and invoke any template in the workflow spec. For example, if you add a new version of the whalesay template called whalesay-caps but you don't want to change the default entrypoint, you can invoke this from the command line as follows: argo submit arguments-parameters.yaml --entrypoint whalesay-caps By using a combination of the --entrypoint and -p parameters, you can call any template in the workflow spec with any parameter that you like. The values set in the spec.arguments.parameters are globally scoped and can be accessed via {{workflow.parameters.parameter_name}} . This can be useful to pass information to multiple steps in a workflow. For example, if you wanted to run your workflows with different logging levels that are set in the environment of each container, you could have a YAML file similar to this one: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : global-parameters- spec : entrypoint : A arguments : parameters : - name : log-level value : INFO templates : - name : A container : image : containerA env : - name : LOG_LEVEL value : \"{{workflow.parameters.log-level}}\" command : [ runA ] - name : B container : image : containerB env : - name : LOG_LEVEL value : \"{{workflow.parameters.log-level}}\" command : [ runB ] In this workflow, both steps A and B would have the same log-level set to INFO and can easily be changed between workflow submissions using the -p flag. Steps \u00b6 In this example, we'll see how to create multi-step workflows, how to define more than one template in a workflow spec, and how to create nested workflows. Be sure to read the comments as they provide useful explanations. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : steps- spec : entrypoint : hello-hello-hello # This spec contains two templates: hello-hello-hello and whalesay templates : - name : hello-hello-hello # Instead of just running a container # This template has a sequence of steps steps : - - name : hello1 # hello1 is run before the following steps template : whalesay arguments : parameters : - name : message value : \"hello1\" - - name : hello2a # double dash => run after previous step template : whalesay arguments : parameters : - name : message value : \"hello2a\" - name : hello2b # single dash => run in parallel with previous step template : whalesay arguments : parameters : - name : message value : \"hello2b\" # This is the same template as from the previous example - name : whalesay inputs : parameters : - name : message container : image : docker/whalesay command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] The above workflow spec prints three different flavors of \"hello\". The hello-hello-hello template consists of three steps . The first step named hello1 will be run in sequence whereas the next two steps named hello2a and hello2b will be run in parallel with each other. Using the argo CLI command, we can graphically display the execution history of this workflow spec, which shows that the steps named hello2a and hello2b ran in parallel with each other. STEP PODNAME \u2714 arguments-parameters-rbm92 \u251c---\u2714 hello1 steps-rbm92-2023062412 \u2514-\u00b7-\u2714 hello2a steps-rbm92-685171357 \u2514-\u2714 hello2b steps-rbm92-634838500 DAG \u00b6 As an alternative to specifying sequences of steps, you can define the workflow as a directed-acyclic graph (DAG) by specifying the dependencies of each task. This can be simpler to maintain for complex workflows and allows for maximum parallelism when running tasks. In the following workflow, step A runs first, as it has no dependencies. Once A has finished, steps B and C run in parallel. Finally, once B and C have completed, step D can run. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : dag-diamond- spec : entrypoint : diamond templates : - name : echo inputs : parameters : - name : message container : image : alpine:3.7 command : [ echo , \"{{inputs.parameters.message}}\" ] - name : diamond dag : tasks : - name : A template : echo arguments : parameters : [{ name : message , value : A }] - name : B dependencies : [ A ] template : echo arguments : parameters : [{ name : message , value : B }] - name : C dependencies : [ A ] template : echo arguments : parameters : [{ name : message , value : C }] - name : D dependencies : [ B , C ] template : echo arguments : parameters : [{ name : message , value : D }] The dependency graph may have multiple roots . The templates called from a DAG or steps template can themselves be DAG or steps templates. This can allow for complex workflows to be split into manageable pieces. The DAG logic has a built-in fail fast feature to stop scheduling new steps, as soon as it detects that one of the DAG nodes is failed. Then it waits until all DAG nodes are completed before failing the DAG itself. The FailFast flag default is true , if set to false , it will allow a DAG to run all branches of the DAG to completion (either success or failure), regardless of the failed outcomes of branches in the DAG. More info and example about this feature at here . Artifacts \u00b6 Note: You will need to configure an artifact repository to run this example. Configuring an artifact repository here . When running workflows, it is very common to have steps that generate or consume artifacts. Often, the output artifacts of one step may be used as input artifacts to a subsequent step. The below workflow spec consists of two steps that run in sequence. The first step named generate-artifact will generate an artifact using the whalesay template that will be consumed by the second step named print-message that then consumes the generated artifact. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : artifact-passing- spec : entrypoint : artifact-example templates : - name : artifact-example steps : - - name : generate-artifact template : whalesay - - name : consume-artifact template : print-message arguments : artifacts : # bind message to the hello-art artifact # generated by the generate-artifact step - name : message from : \"{{steps.generate-artifact.outputs.artifacts.hello-art}}\" - name : whalesay container : image : docker/whalesay:latest command : [ sh , -c ] args : [ \"cowsay hello world | tee /tmp/hello_world.txt\" ] outputs : artifacts : # generate hello-art artifact from /tmp/hello_world.txt # artifacts can be directories as well as files - name : hello-art path : /tmp/hello_world.txt - name : print-message inputs : artifacts : # unpack the message input artifact # and put it at /tmp/message - name : message path : /tmp/message container : image : alpine:latest command : [ sh , -c ] args : [ \"cat /tmp/message\" ] The whalesay template uses the cowsay command to generate a file named /tmp/hello-world.txt . It then outputs this file as an artifact named hello-art . In general, the artifact's path may be a directory rather than just a file. The print-message template takes an input artifact named message , unpacks it at the path named /tmp/message and then prints the contents of /tmp/message using the cat command. The artifact-example template passes the hello-art artifact generated as an output of the generate-artifact step as the message input artifact to the print-message step. DAG templates use the tasks prefix to refer to another task, for example {{tasks.generate-artifact.outputs.artifacts.hello-art}} . Artifacts are packaged as Tarballs and gzipped by default. You may customize this behavior by specifying an archive strategy, using the archive field. For example: <... snipped ...> outputs : artifacts : # default behavior - tar+gzip default compression. - name : hello-art-1 path : /tmp/hello_world.txt # disable archiving entirely - upload the file / directory as is. # this is useful when the container layout matches the desired target repository layout. - name : hello-art-2 path : /tmp/hello_world.txt archive : none : {} # customize the compression behavior (disabling it here). # this is useful for files with varying compression benefits, # e.g. disabling compression for a cached build workspace and large binaries, # or increasing compression for \"perfect\" textual data - like a json/xml export of a large database. - name : hello-art-3 path : /tmp/hello_world.txt archive : tar : # no compression (also accepts the standard gzip 1 to 9 values) compressionLevel : 0 <... snipped ...> The Structure of Workflow Specs \u00b6 We now know enough about the basic components of a workflow spec to review its basic structure: Kubernetes header including metadata Spec body Entrypoint invocation with optionally arguments List of template definitions For each template definition Name of the template Optionally a list of inputs Optionally a list of outputs Container invocation (leaf template) or a list of steps For each step, a template invocation To summarize, workflow specs are composed of a set of Argo templates where each template consists of an optional input section, an optional output section and either a container invocation or a list of steps where each step invokes another template. Note that the container section of the workflow spec will accept the same options as the container section of a pod spec, including but not limited to environment variables, secrets, and volume mounts. Similarly, for volume claims and volumes. Secrets \u00b6 Argo supports the same secrets syntax and mechanisms as Kubernetes Pod specs, which allows access to secrets as environment variables or volume mounts. See the Kubernetes documentation for more information. # To run this example, first create the secret by running: # kubectl create secret generic my-secret --from-literal=mypassword=S00perS3cretPa55word apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : secret-example- spec : entrypoint : whalesay # To access secrets as files, add a volume entry in spec.volumes[] and # then in the container template spec, add a mount using volumeMounts. volumes : - name : my-secret-vol secret : secretName : my-secret # name of an existing k8s secret templates : - name : whalesay container : image : alpine:3.7 command : [ sh , -c ] args : [ ' echo \"secret from env: $MYSECRETPASSWORD\"; echo \"secret from file: `cat /secret/mountpath/mypassword`\" ' ] # To access secrets as environment variables, use the k8s valueFrom and # secretKeyRef constructs. env : - name : MYSECRETPASSWORD # name of env var valueFrom : secretKeyRef : name : my-secret # name of an existing k8s secret key : mypassword # 'key' subcomponent of the secret volumeMounts : - name : my-secret-vol # mount file containing secret at /secret/mountpath mountPath : \"/secret/mountpath\" Scripts & Results \u00b6 Often, we just want a template that executes a script specified as a here-script (also known as a here document ) in the workflow spec. This example shows how to do that: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : scripts-bash- spec : entrypoint : bash-script-example templates : - name : bash-script-example steps : - - name : generate template : gen-random-int-bash - - name : print template : print-message arguments : parameters : - name : message value : \"{{steps.generate.outputs.result}}\" # The result of the here-script - name : gen-random-int-bash script : image : debian:9.4 command : [ bash ] source : | # Contents of the here-script cat /dev/urandom | od -N2 -An -i | awk -v f=1 -v r=100 '{printf \"%i\\n\", f + r * $1 / 65536}' - name : gen-random-int-python script : image : python:alpine3.6 command : [ python ] source : | import random i = random.randint(1, 100) print(i) - name : gen-random-int-javascript script : image : node:9.1-alpine command : [ node ] source : | var rand = Math.floor(Math.random() * 100); console.log(rand); - name : print-message inputs : parameters : - name : message container : image : alpine:latest command : [ sh , -c ] args : [ \"echo result was: {{inputs.parameters.message}}\" ] The script keyword allows the specification of the script body using the source tag. This creates a temporary file containing the script body and then passes the name of the temporary file as the final parameter to command , which should be an interpreter that executes the script body. The use of the script feature also assigns the standard output of running the script to a special output parameter named result . This allows you to use the result of running the script itself in the rest of the workflow spec. In this example, the result is simply echoed by the print-message template. Output Parameters \u00b6 Output parameters provide a general mechanism to use the result of a step as a parameter rather than as an artifact. This allows you to use the result from any type of step, not just a script , for conditional tests, loops, and arguments. Output parameters work similarly to script result except that the value of the output parameter is set to the contents of a generated file rather than the contents of stdout . apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : output-parameter- spec : entrypoint : output-parameter templates : - name : output-parameter steps : - - name : generate-parameter template : whalesay - - name : consume-parameter template : print-message arguments : parameters : # Pass the hello-param output from the generate-parameter step as the message input to print-message - name : message value : \"{{steps.generate-parameter.outputs.parameters.hello-param}}\" - name : whalesay container : image : docker/whalesay:latest command : [ sh , -c ] args : [ \"echo -n hello world > /tmp/hello_world.txt\" ] # generate the content of hello_world.txt outputs : parameters : - name : hello-param # name of output parameter valueFrom : path : /tmp/hello_world.txt # set the value of hello-param to the contents of this hello-world.txt - name : print-message inputs : parameters : - name : message container : image : docker/whalesay:latest command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] DAG templates use the tasks prefix to refer to another task, for example {{tasks.generate-parameter.outputs.parameters.hello-param}} . Loops \u00b6 When writing workflows, it is often very useful to be able to iterate over a set of inputs as shown in this example: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : loops- spec : entrypoint : loop-example templates : - name : loop-example steps : - - name : print-message template : whalesay arguments : parameters : - name : message value : \"{{item}}\" withItems : # invoke whalesay once for each item in parallel - hello world # item 1 - goodbye world # item 2 - name : whalesay inputs : parameters : - name : message container : image : docker/whalesay:latest command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] We can also iterate over sets of items: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : loops-maps- spec : entrypoint : loop-map-example templates : - name : loop-map-example steps : - - name : test-linux template : cat-os-release arguments : parameters : - name : image value : \"{{item.image}}\" - name : tag value : \"{{item.tag}}\" withItems : - { image : 'debian' , tag : '9.1' } #item set 1 - { image : 'debian' , tag : '8.9' } #item set 2 - { image : 'alpine' , tag : '3.6' } #item set 3 - { image : 'ubuntu' , tag : '17.10' } #item set 4 - name : cat-os-release inputs : parameters : - name : image - name : tag container : image : \"{{inputs.parameters.image}}:{{inputs.parameters.tag}}\" command : [ cat ] args : [ /etc/os-release ] We can pass lists of items as parameters: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : loops-param-arg- spec : entrypoint : loop-param-arg-example arguments : parameters : - name : os-list # a list of items value : | [ { \"image\": \"debian\", \"tag\": \"9.1\" }, { \"image\": \"debian\", \"tag\": \"8.9\" }, { \"image\": \"alpine\", \"tag\": \"3.6\" }, { \"image\": \"ubuntu\", \"tag\": \"17.10\" } ] templates : - name : loop-param-arg-example inputs : parameters : - name : os-list steps : - - name : test-linux template : cat-os-release arguments : parameters : - name : image value : \"{{item.image}}\" - name : tag value : \"{{item.tag}}\" withParam : \"{{inputs.parameters.os-list}}\" # parameter specifies the list to iterate over # This template is the same as in the previous example - name : cat-os-release inputs : parameters : - name : image - name : tag container : image : \"{{inputs.parameters.image}}:{{inputs.parameters.tag}}\" command : [ cat ] args : [ /etc/os-release ] We can even dynamically generate the list of items to iterate over! apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : loops-param-result- spec : entrypoint : loop-param-result-example templates : - name : loop-param-result-example steps : - - name : generate template : gen-number-list # Iterate over the list of numbers generated by the generate step above - - name : sleep template : sleep-n-sec arguments : parameters : - name : seconds value : \"{{item}}\" withParam : \"{{steps.generate.outputs.result}}\" # Generate a list of numbers in JSON format - name : gen-number-list script : image : python:alpine3.6 command : [ python ] source : | import json import sys json.dump([i for i in range(20, 31)], sys.stdout) - name : sleep-n-sec inputs : parameters : - name : seconds container : image : alpine:latest command : [ sh , -c ] args : [ \"echo sleeping for {{inputs.parameters.seconds}} seconds; sleep {{inputs.parameters.seconds}}; echo done\" ] Conditionals \u00b6 We also support conditional execution as shown in this example: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : coinflip- spec : entrypoint : coinflip templates : - name : coinflip steps : # flip a coin - - name : flip-coin template : flip-coin # evaluate the result in parallel - - name : heads template : heads # call heads template if \"heads\" when : \"{{steps.flip-coin.outputs.result}} == heads\" - name : tails template : tails # call tails template if \"tails\" when : \"{{steps.flip-coin.outputs.result}} == tails\" # Return heads or tails based on a random number - name : flip-coin script : image : python:alpine3.6 command : [ python ] source : | import random result = \"heads\" if random.randint(0,1) == 0 else \"tails\" print(result) - name : heads container : image : alpine:3.6 command : [ sh , -c ] args : [ \"echo \\\"it was heads\\\"\" ] - name : tails container : image : alpine:3.6 command : [ sh , -c ] args : [ \"echo \\\"it was tails\\\"\" ] Retrying Failed or Errored Steps \u00b6 You can specify a retryStrategy that will dictate how failed or errored steps are retried: # This example demonstrates the use of retry back offs apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : retry-backoff- spec : entrypoint : retry-backoff templates : - name : retry-backoff retryStrategy : limit : 10 retryPolicy : \"Always\" backoff : duration : \"1\" # Must be a string. Default unit is seconds. Could also be a Duration, e.g.: \"2m\", \"6h\", \"1d\" factor : 2 maxDuration : \"1m\" # Must be a string. Default unit is seconds. Could also be a Duration, e.g.: \"2m\", \"6h\", \"1d\" container : image : python:alpine3.6 command : [ \"python\" , -c ] # fail with a 66% probability args : [ \"import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)\" ] limit is the maximum number of times the container will be retried. retryPolicy specifies if a container will be retried on failure, error, or both. \"Always\" retries on both errors and failures. Also available: \"OnFailure\" (default), \"OnError\" backoff is an exponential backoff Providing an empty retryStrategy (i.e. retryStrategy: {} ) will cause a container to retry until completion. Recursion \u00b6 Templates can recursively invoke each other! In this variation of the above coin-flip template, we continue to flip coins until it comes up heads. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : coinflip-recursive- spec : entrypoint : coinflip templates : - name : coinflip steps : # flip a coin - - name : flip-coin template : flip-coin # evaluate the result in parallel - - name : heads template : heads # call heads template if \"heads\" when : \"{{steps.flip-coin.outputs.result}} == heads\" - name : tails # keep flipping coins if \"tails\" template : coinflip when : \"{{steps.flip-coin.outputs.result}} == tails\" - name : flip-coin script : image : python:alpine3.6 command : [ python ] source : | import random result = \"heads\" if random.randint(0,1) == 0 else \"tails\" print(result) - name : heads container : image : alpine:3.6 command : [ sh , -c ] args : [ \"echo \\\"it was heads\\\"\" ] Here's the result of a couple of runs of coinflip for comparison. argo get coinflip-recursive-tzcb5 STEP PODNAME MESSAGE \u2714 coinflip-recursive-vhph5 \u251c---\u2714 flip-coin coinflip-recursive-vhph5-2123890397 \u2514-\u00b7-\u2714 heads coinflip-recursive-vhph5-128690560 \u2514-\u25cb tails STEP PODNAME MESSAGE \u2714 coinflip-recursive-tzcb5 \u251c---\u2714 flip-coin coinflip-recursive-tzcb5-322836820 \u2514-\u00b7-\u25cb heads \u2514-\u2714 tails \u251c---\u2714 flip-coin coinflip-recursive-tzcb5-1863890320 \u2514-\u00b7-\u25cb heads \u2514-\u2714 tails \u251c---\u2714 flip-coin coinflip-recursive-tzcb5-1768147140 \u2514-\u00b7-\u25cb heads \u2514-\u2714 tails \u251c---\u2714 flip-coin coinflip-recursive-tzcb5-4080411136 \u2514-\u00b7-\u2714 heads coinflip-recursive-tzcb5-4080323273 \u2514-\u25cb tails In the first run, the coin immediately comes up heads and we stop. In the second run, the coin comes up tail three times before it finally comes up heads and we stop. Exit handlers \u00b6 An exit handler is a template that always executes, irrespective of success or failure, at the end of the workflow. Some common use cases of exit handlers are: cleaning up after a workflow runs sending notifications of workflow status (e.g., e-mail/Slack) posting the pass/fail status to a webhook result (e.g. GitHub build result) resubmitting or submitting another workflow apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : exit-handlers- spec : entrypoint : intentional-fail onExit : exit-handler # invoke exit-hander template at end of the workflow templates : # primary workflow template - name : intentional-fail container : image : alpine:latest command : [ sh , -c ] args : [ \"echo intentional failure; exit 1\" ] # Exit handler templates # After the completion of the entrypoint template, the status of the # workflow is made available in the global variable {{workflow.status}}. # {{workflow.status}} will be one of: Succeeded, Failed, Error - name : exit-handler steps : - - name : notify template : send-email - name : celebrate template : celebrate when : \"{{workflow.status}} == Succeeded\" - name : cry template : cry when : \"{{workflow.status}} != Succeeded\" - name : send-email container : image : alpine:latest command : [ sh , -c ] args : [ \"echo send e-mail: {{workflow.name}} {{workflow.status}}\" ] - name : celebrate container : image : alpine:latest command : [ sh , -c ] args : [ \"echo hooray!\" ] - name : cry container : image : alpine:latest command : [ sh , -c ] args : [ \"echo boohoo!\" ] Timeouts \u00b6 To limit the elapsed time for a workflow, you can set the variable activeDeadlineSeconds . # To enforce a timeout for a container template, specify a value for activeDeadlineSeconds. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : timeouts- spec : entrypoint : sleep templates : - name : sleep container : image : alpine:latest command : [ sh , -c ] args : [ \"echo sleeping for 1m; sleep 60; echo done\" ] activeDeadlineSeconds : 10 # terminate container template after 10 seconds Volumes \u00b6 The following example dynamically creates a volume and then uses the volume in a two step workflow. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : volumes-pvc- spec : entrypoint : volumes-pvc-example volumeClaimTemplates : # define volume, same syntax as k8s Pod spec - metadata : name : workdir # name of volume claim spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi # Gi => 1024 * 1024 * 1024 templates : - name : volumes-pvc-example steps : - - name : generate template : whalesay - - name : print template : print-message - name : whalesay container : image : docker/whalesay:latest command : [ sh , -c ] args : [ \"echo generating message in volume; cowsay hello world | tee /mnt/vol/hello_world.txt\" ] # Mount workdir volume at /mnt/vol before invoking docker/whalesay volumeMounts : # same syntax as k8s Pod spec - name : workdir mountPath : /mnt/vol - name : print-message container : image : alpine:latest command : [ sh , -c ] args : [ \"echo getting message from volume; find /mnt/vol; cat /mnt/vol/hello_world.txt\" ] # Mount workdir volume at /mnt/vol before invoking docker/whalesay volumeMounts : # same syntax as k8s Pod spec - name : workdir mountPath : /mnt/vol Volumes are a very useful way to move large amounts of data from one step in a workflow to another. Depending on the system, some volumes may be accessible concurrently from multiple steps. In some cases, you want to access an already existing volume rather than creating/destroying one dynamically. # Define Kubernetes PVC kind : PersistentVolumeClaim apiVersion : v1 metadata : name : my-existing-volume spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi --- apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : volumes-existing- spec : entrypoint : volumes-existing-example volumes : # Pass my-existing-volume as an argument to the volumes-existing-example template # Same syntax as k8s Pod spec - name : workdir persistentVolumeClaim : claimName : my-existing-volume templates : - name : volumes-existing-example steps : - - name : generate template : whalesay - - name : print template : print-message - name : whalesay container : image : docker/whalesay:latest command : [ sh , -c ] args : [ \"echo generating message in volume; cowsay hello world | tee /mnt/vol/hello_world.txt\" ] volumeMounts : - name : workdir mountPath : /mnt/vol - name : print-message container : image : alpine:latest command : [ sh , -c ] args : [ \"echo getting message from volume; find /mnt/vol; cat /mnt/vol/hello_world.txt\" ] volumeMounts : - name : workdir mountPath : /mnt/vol It's also possible to declare existing volumes at the template level, instead of the workflow level. This can be useful workflows that generate volumes using a resource step. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : template-level-volume- spec : entrypoint : generate-and-use-volume templates : - name : generate-and-use-volume steps : - - name : generate-volume template : generate-volume arguments : parameters : - name : pvc-size # In a real-world example, this could be generated by a previous workfow step. value : '1Gi' - - name : generate template : whalesay arguments : parameters : - name : pvc-name value : '{{ steps.generate-volume.outputs.parameters.pvc-name }}' - - name : print template : print-message arguments : parameters : - name : pvc-name value : '{{ steps.generate-volume.outputs.parameters.pvc-name }}' - name : generate-volume inputs : parameters : - name : pvc-size resource : action : create setOwnerReference : true manifest : | apiVersion: v1 kind: PersistentVolumeClaim metadata: generateName: pvc-example- spec: accessModes: ['ReadWriteOnce', 'ReadOnlyMany'] resources: requests: storage: '{{inputs.parameters.pvc-size}}' outputs : parameters : - name : pvc-name valueFrom : jsonPath : '{.metadata.name}' - name : whalesay inputs : parameters : - name : pvc-name volumes : - name : workdir persistentVolumeClaim : claimName : '{{inputs.parameters.pvc-name}}' container : image : docker/whalesay:latest command : [ sh , -c ] args : [ \"echo generating message in volume; cowsay hello world | tee /mnt/vol/hello_world.txt\" ] volumeMounts : - name : workdir mountPath : /mnt/vol - name : print-message inputs : parameters : - name : pvc-name volumes : - name : workdir persistentVolumeClaim : claimName : '{{inputs.parameters.pvc-name}}' container : image : alpine:latest command : [ sh , -c ] args : [ \"echo getting message from volume; find /mnt/vol; cat /mnt/vol/hello_world.txt\" ] volumeMounts : - name : workdir mountPath : /mnt/vol Suspending \u00b6 Workflows can be suspended by argo suspend WORKFLOW Or by specifying a suspend step on the workflow: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : suspend-template- spec : entrypoint : suspend templates : - name : suspend steps : - - name : build template : whalesay - - name : approve template : approve - - name : delay template : delay - - name : release template : whalesay - name : approve suspend : {} - name : delay suspend : duration : 20 # Default unit is seconds. Could also be a Duration, e.g.: \"2m\", \"6h\", \"1d\" - name : whalesay container : image : docker/whalesay command : [ cowsay ] args : [ \"hello world\" ] Once suspended, a Workflow will not schedule any new steps until it is resumed. It can be resumed manually by argo resume WORKFLOW Or automatically with a duration limit as the example above. Daemon Containers \u00b6 Argo workflows can start containers that run in the background (also known as daemon containers ) while the workflow itself continues execution. Note that the daemons will be automatically destroyed when the workflow exits the template scope in which the daemon was invoked. Daemon containers are useful for starting up services to be tested or to be used in testing (e.g., fixtures). We also find it very useful when running large simulations to spin up a database as a daemon for collecting and organizing the results. The big advantage of daemons compared with sidecars is that their existence can persist across multiple steps or even the entire workflow. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : daemon-step- spec : entrypoint : daemon-example templates : - name : daemon-example steps : - - name : influx template : influxdb # start an influxdb as a daemon (see the influxdb template spec below) - - name : init-database # initialize influxdb template : influxdb-client arguments : parameters : - name : cmd value : curl -XPOST 'http://{{steps.influx.ip}}:8086/query' --data-urlencode \"q=CREATE DATABASE mydb\" - - name : producer-1 # add entries to influxdb template : influxdb-client arguments : parameters : - name : cmd value : for i in $(seq 1 20); do curl -XPOST 'http://{{steps.influx.ip}}:8086/write?db=mydb' -d \"cpu,host=server01,region=uswest load=$i\" ; sleep .5 ; done - name : producer-2 # add entries to influxdb template : influxdb-client arguments : parameters : - name : cmd value : for i in $(seq 1 20); do curl -XPOST 'http://{{steps.influx.ip}}:8086/write?db=mydb' -d \"cpu,host=server02,region=uswest load=$((RANDOM % 100))\" ; sleep .5 ; done - name : producer-3 # add entries to influxdb template : influxdb-client arguments : parameters : - name : cmd value : curl -XPOST 'http://{{steps.influx.ip}}:8086/write?db=mydb' -d 'cpu,host=server03,region=useast load=15.4' - - name : consumer # consume intries from influxdb template : influxdb-client arguments : parameters : - name : cmd value : curl --silent -G http://{{steps.influx.ip}}:8086/query?pretty=true --data-urlencode \"db=mydb\" --data-urlencode \"q=SELECT * FROM cpu\" - name : influxdb daemon : true # start influxdb as a daemon retryStrategy : limit : 10 # retry container if it fails container : image : influxdb:1.2 readinessProbe : # wait for readinessProbe to succeed httpGet : path : /ping port : 8086 - name : influxdb-client inputs : parameters : - name : cmd container : image : appropriate/curl:latest command : [ \"/bin/sh\" , \"-c\" ] args : [ \"{{inputs.parameters.cmd}}\" ] resources : requests : memory : 32Mi cpu : 100m DAG templates use the tasks prefix to refer to another task, for example {{tasks.influx.ip}} . Sidecars \u00b6 A sidecar is another container that executes concurrently in the same pod as the main container and is useful in creating multi-container pods. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : sidecar-nginx- spec : entrypoint : sidecar-nginx-example templates : - name : sidecar-nginx-example container : image : appropriate/curl command : [ sh , -c ] # Try to read from nginx web server until it comes up args : [ \"until `curl -G 'http://127.0.0.1/' >& /tmp/out`; do echo sleep && sleep 1; done && cat /tmp/out\" ] # Create a simple nginx web server sidecars : - name : nginx image : nginx:1.13 In the above example, we create a sidecar container that runs nginx as a simple web server. The order in which containers come up is random, so in this example the main container polls the nginx container until it is ready to service requests. This is a good design pattern when designing multi-container systems: always wait for any services you need to come up before running your main code. Hardwired Artifacts \u00b6 With Argo, you can use any container image that you like to generate any kind of artifact. In practice, however, we find certain types of artifacts are very common, so there is built-in support for git, http, gcs and s3 artifacts. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : hardwired-artifact- spec : entrypoint : hardwired-artifact templates : - name : hardwired-artifact inputs : artifacts : # Check out the master branch of the argo repo and place it at /src # revision can be anything that git checkout accepts: branch, commit, tag, etc. - name : argo-source path : /src git : repo : https://github.com/argoproj/argo.git revision : \"master\" # Download kubectl 1.8.0 and place it at /bin/kubectl - name : kubectl path : /bin/kubectl mode : 0755 http : url : https://storage.googleapis.com/kubernetes-release/release/v1.8.0/bin/linux/amd64/kubectl # Copy an s3 compatible artifact repository bucket (such as AWS, GCS and Minio) and place it at /s3 - name : objects path : /s3 s3 : endpoint : storage.googleapis.com bucket : my-bucket-name key : path/in/bucket accessKeySecret : name : my-s3-credentials key : accessKey secretKeySecret : name : my-s3-credentials key : secretKey container : image : debian command : [ sh , -c ] args : [ \"ls -l /src /bin/kubectl /s3\" ] Kubernetes Resources \u00b6 In many cases, you will want to manage Kubernetes resources from Argo workflows. The resource template allows you to create, delete or updated any type of Kubernetes resource. # in a workflow. The resource template type accepts any k8s manifest # (including CRDs) and can perform any kubectl action against it (e.g. create, # apply, delete, patch). apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : k8s-jobs- spec : entrypoint : pi-tmpl templates : - name : pi-tmpl resource : # indicates that this is a resource template action : create # can be any kubectl action (e.g. create, delete, apply, patch) # The successCondition and failureCondition are optional expressions. # If failureCondition is true, the step is considered failed. # If successCondition is true, the step is considered successful. # They use kubernetes label selection syntax and can be applied against any field # of the resource (not just labels). Multiple AND conditions can be represented by comma # delimited expressions. # For more details: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ successCondition : status.succeeded > 0 failureCondition : status.failed > 3 manifest : | #put your kubernetes spec here apiVersion: batch/v1 kind: Job metadata: generateName: pi-job- spec: template: metadata: name: pi spec: containers: - name: pi image: perl command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"] restartPolicy: Never backoffLimit: 4 Resources created in this way are independent of the workflow. If you want the resource to be deleted when the workflow is deleted then you can use Kubernetes garbage collection with the workflow resource as an owner reference ( example ). Note: When patching, the resource will accept another attribute, mergeStrategy , which can either be strategic , merge , or json . If this attribute is not supplied, it will default to strategic . Keep in mind that Custom Resources cannot be patched with strategic , so a different strategy must be chosen. For example, suppose you have the CronTab CustomResourceDefinition defined, and the following instance of a CronTab: apiVersion : \"stable.example.com/v1\" kind : CronTab spec : cronSpec : \"* * * * */5\" image : my-awesome-cron-image This Crontab can be modified using the following Argo Workflow: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : k8s-patch- spec : entrypoint : cront-tmpl templates : - name : cront-tmpl resource : action : patch mergeStrategy : merge # Must be one of [strategic merge json] manifest : | apiVersion: \"stable.example.com/v1\" kind: CronTab spec: cronSpec: \"* * * * */10\" image: my-awesome-cron-image Docker-in-Docker Using Sidecars \u00b6 An application of sidecars is to implement Docker-in-Docker (DinD). DinD is useful when you want to run Docker commands from inside a container. For example, you may want to build and push a container image from inside your build container. In the following example, we use the docker:dind container to run a Docker daemon in a sidecar and give the main container access to the daemon. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : sidecar-dind- spec : entrypoint : dind-sidecar-example templates : - name : dind-sidecar-example container : image : docker:17.10 command : [ sh , -c ] args : [ \"until docker ps; do sleep 3; done; docker run --rm debian:latest cat /etc/os-release\" ] env : - name : DOCKER_HOST # the docker daemon can be access on the standard port on localhost value : 127.0.0.1 sidecars : - name : dind image : docker:17.10-dind # Docker already provides an image for running a Docker daemon securityContext : privileged : true # the Docker daemon can only run in a privileged container # mirrorVolumeMounts will mount the same volumes specified in the main container # to the sidecar (including artifacts), at the same mountPaths. This enables # dind daemon to (partially) see the same filesystem as the main container in # order to use features such as docker volume binding. mirrorVolumeMounts : true Custom Template Variable Reference \u00b6 In this example, we can see how we can use the other template language variable reference (E.g: Jinja) in Argo workflow template. Argo will validate and resolve only the variable that starts with Argo allowed prefix { \"item\", \"steps\", \"inputs\", \"outputs\", \"workflow\", \"tasks\" } apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : custom-template-variable- spec : entrypoint : hello-hello-hello templates : - name : hello-hello-hello steps : - - name : hello1 template : whalesay arguments : parameters : [{ name : message , value : \"hello1\" }] - - name : hello2a template : whalesay arguments : parameters : [{ name : message , value : \"hello2a\" }] - name : hello2b template : whalesay arguments : parameters : [{ name : message , value : \"hello2b\" }] - name : whalesay inputs : parameters : - name : message container : image : docker/whalesay command : [ cowsay ] args : [ \"{{user.username}}\" ] Continuous Integration Example \u00b6 Continuous integration is a popular application for workflows. Currently, Argo does not provide event triggers for automatically kicking off your CI jobs, but we plan to do so in the near future. Until then, you can easily write a cron job that checks for new commits and kicks off the needed workflow, or use your existing Jenkins server to kick off the workflow. A good example of a CI workflow spec is provided at https://github.com/argoproj/argo/tree/master/examples/influxdb-ci.yaml. Because it just uses the concepts that we've already covered and is somewhat long, we don't go into details here.","title":"Examples"},{"location":"examples/#documentation-by-example","text":"","title":"Documentation by Example"},{"location":"examples/#welcome","text":"Argo is an open source project that provides container-native workflows for Kubernetes. Each step in an Argo workflow is defined as a container. Argo is implemented as a Kubernetes CRD (Custom Resource Definition). As a result, Argo workflows can be managed using kubectl and natively integrates with other Kubernetes services such as volumes, secrets, and RBAC. The new Argo software is light-weight and installs in under a minute, and provides complete workflow features including parameter substitution, artifacts, fixtures, loops and recursive workflows. Many of the Argo examples used in this walkthrough are available at in this directory. If you like this project, please give us a star! For a complete description of the Argo workflow spec, please refer to our spec definitions .","title":"Welcome!"},{"location":"examples/#table-of-contents","text":"Argo CLI Hello World! Parameters Steps DAG Artifacts The Structure of Workflow Specs Secrets Scripts & Results Output Parameters Loops Conditionals Retrying Failed or Errored Steps Recursion Exit Handlers Timeouts Volumes Suspending Daemon Containers Sidecars Hardwired Artifacts Kubernetes Resources Docker-in-Docker Using Sidecars Custom Template Variable Reference Continuous Integration Example","title":"Table of Contents"},{"location":"examples/#argo-cli","text":"In case you want to follow along with this walkthrough, here's a quick overview of the most useful argo command line interface (CLI) commands. argo submit hello-world.yaml # submit a workflow spec to Kubernetes argo list # list current workflows argo get hello-world-xxx # get info about a specific workflow argo logs -w hello-world-xxx # get logs from all steps in a workflow argo logs hello-world-xxx-yyy # get logs from a specific step in a workflow argo delete hello-world-xxx # delete workflow You can also run workflow specs directly using kubectl but the Argo CLI provides syntax checking, nicer output, and requires less typing. kubectl create -f hello-world.yaml kubectl get wf kubectl get wf hello-world-xxx kubectl get po --selector = workflows.argoproj.io/workflow = hello-world-xxx --show-all # similar to argo kubectl logs hello-world-xxx-yyy -c main kubectl delete wf hello-world-xxx","title":"Argo CLI"},{"location":"examples/#hello-world","text":"Let's start by creating a very simple workflow template to echo \"hello world\" using the docker/whalesay container image from DockerHub. You can run this directly from your shell with a simple docker command: $ docker run docker/whalesay cowsay \"hello world\" _____________ < hello world > ------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___/ === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / === - ~~~ \\_ _____ o __/ \\ \\ __/ \\_ ___ \\_ _____/ Hello from Docker! This message shows that your installation appears to be working correctly. Below, we run the same container on a Kubernetes cluster using an Argo workflow template. Be sure to read the comments as they provide useful explanations. apiVersion : argoproj.io/v1alpha1 kind : Workflow # new type of k8s spec metadata : generateName : hello-world- # name of the workflow spec spec : entrypoint : whalesay # invoke the whalesay template templates : - name : whalesay # name of the template container : image : docker/whalesay command : [ cowsay ] args : [ \"hello world\" ] resources : # limit the resources limits : memory : 32Mi cpu : 100m Argo adds a new kind of Kubernetes spec called a Workflow . The above spec contains a single template called whalesay which runs the docker/whalesay container and invokes cowsay \"hello world\" . The whalesay template is the entrypoint for the spec. The entrypoint specifies the initial template that should be invoked when the workflow spec is executed by Kubernetes. Being able to specify the entrypoint is more useful when there is more than one template defined in the Kubernetes workflow spec. :-)","title":"Hello World!"},{"location":"examples/#parameters","text":"Let's look at a slightly more complex workflow spec with parameters. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : hello-world-parameters- spec : # invoke the whalesay template with # \"hello world\" as the argument # to the message parameter entrypoint : whalesay arguments : parameters : - name : message value : hello world templates : - name : whalesay inputs : parameters : - name : message # parameter declaration container : # run cowsay with that message input parameter as args image : docker/whalesay command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] This time, the whalesay template takes an input parameter named message that is passed as the args to the cowsay command. In order to reference parameters (e.g., \"{{inputs.parameters.message}}\" ), the parameters must be enclosed in double quotes to escape the curly braces in YAML. The argo CLI provides a convenient way to override parameters used to invoke the entrypoint. For example, the following command would bind the message parameter to \"goodbye world\" instead of the default \"hello world\". argo submit arguments-parameters.yaml -p message = \"goodbye world\" In case of multiple parameters that can be overriten, the argo CLI provides a command to load parameters files in YAML or JSON format. Here is an example of that kind of parameter file: message : goodbye world To run use following command: argo submit arguments-parameters.yaml --parameter-file params.yaml Command-line parameters can also be used to override the default entrypoint and invoke any template in the workflow spec. For example, if you add a new version of the whalesay template called whalesay-caps but you don't want to change the default entrypoint, you can invoke this from the command line as follows: argo submit arguments-parameters.yaml --entrypoint whalesay-caps By using a combination of the --entrypoint and -p parameters, you can call any template in the workflow spec with any parameter that you like. The values set in the spec.arguments.parameters are globally scoped and can be accessed via {{workflow.parameters.parameter_name}} . This can be useful to pass information to multiple steps in a workflow. For example, if you wanted to run your workflows with different logging levels that are set in the environment of each container, you could have a YAML file similar to this one: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : global-parameters- spec : entrypoint : A arguments : parameters : - name : log-level value : INFO templates : - name : A container : image : containerA env : - name : LOG_LEVEL value : \"{{workflow.parameters.log-level}}\" command : [ runA ] - name : B container : image : containerB env : - name : LOG_LEVEL value : \"{{workflow.parameters.log-level}}\" command : [ runB ] In this workflow, both steps A and B would have the same log-level set to INFO and can easily be changed between workflow submissions using the -p flag.","title":"Parameters"},{"location":"examples/#steps","text":"In this example, we'll see how to create multi-step workflows, how to define more than one template in a workflow spec, and how to create nested workflows. Be sure to read the comments as they provide useful explanations. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : steps- spec : entrypoint : hello-hello-hello # This spec contains two templates: hello-hello-hello and whalesay templates : - name : hello-hello-hello # Instead of just running a container # This template has a sequence of steps steps : - - name : hello1 # hello1 is run before the following steps template : whalesay arguments : parameters : - name : message value : \"hello1\" - - name : hello2a # double dash => run after previous step template : whalesay arguments : parameters : - name : message value : \"hello2a\" - name : hello2b # single dash => run in parallel with previous step template : whalesay arguments : parameters : - name : message value : \"hello2b\" # This is the same template as from the previous example - name : whalesay inputs : parameters : - name : message container : image : docker/whalesay command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] The above workflow spec prints three different flavors of \"hello\". The hello-hello-hello template consists of three steps . The first step named hello1 will be run in sequence whereas the next two steps named hello2a and hello2b will be run in parallel with each other. Using the argo CLI command, we can graphically display the execution history of this workflow spec, which shows that the steps named hello2a and hello2b ran in parallel with each other. STEP PODNAME \u2714 arguments-parameters-rbm92 \u251c---\u2714 hello1 steps-rbm92-2023062412 \u2514-\u00b7-\u2714 hello2a steps-rbm92-685171357 \u2514-\u2714 hello2b steps-rbm92-634838500","title":"Steps"},{"location":"examples/#dag","text":"As an alternative to specifying sequences of steps, you can define the workflow as a directed-acyclic graph (DAG) by specifying the dependencies of each task. This can be simpler to maintain for complex workflows and allows for maximum parallelism when running tasks. In the following workflow, step A runs first, as it has no dependencies. Once A has finished, steps B and C run in parallel. Finally, once B and C have completed, step D can run. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : dag-diamond- spec : entrypoint : diamond templates : - name : echo inputs : parameters : - name : message container : image : alpine:3.7 command : [ echo , \"{{inputs.parameters.message}}\" ] - name : diamond dag : tasks : - name : A template : echo arguments : parameters : [{ name : message , value : A }] - name : B dependencies : [ A ] template : echo arguments : parameters : [{ name : message , value : B }] - name : C dependencies : [ A ] template : echo arguments : parameters : [{ name : message , value : C }] - name : D dependencies : [ B , C ] template : echo arguments : parameters : [{ name : message , value : D }] The dependency graph may have multiple roots . The templates called from a DAG or steps template can themselves be DAG or steps templates. This can allow for complex workflows to be split into manageable pieces. The DAG logic has a built-in fail fast feature to stop scheduling new steps, as soon as it detects that one of the DAG nodes is failed. Then it waits until all DAG nodes are completed before failing the DAG itself. The FailFast flag default is true , if set to false , it will allow a DAG to run all branches of the DAG to completion (either success or failure), regardless of the failed outcomes of branches in the DAG. More info and example about this feature at here .","title":"DAG"},{"location":"examples/#artifacts","text":"Note: You will need to configure an artifact repository to run this example. Configuring an artifact repository here . When running workflows, it is very common to have steps that generate or consume artifacts. Often, the output artifacts of one step may be used as input artifacts to a subsequent step. The below workflow spec consists of two steps that run in sequence. The first step named generate-artifact will generate an artifact using the whalesay template that will be consumed by the second step named print-message that then consumes the generated artifact. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : artifact-passing- spec : entrypoint : artifact-example templates : - name : artifact-example steps : - - name : generate-artifact template : whalesay - - name : consume-artifact template : print-message arguments : artifacts : # bind message to the hello-art artifact # generated by the generate-artifact step - name : message from : \"{{steps.generate-artifact.outputs.artifacts.hello-art}}\" - name : whalesay container : image : docker/whalesay:latest command : [ sh , -c ] args : [ \"cowsay hello world | tee /tmp/hello_world.txt\" ] outputs : artifacts : # generate hello-art artifact from /tmp/hello_world.txt # artifacts can be directories as well as files - name : hello-art path : /tmp/hello_world.txt - name : print-message inputs : artifacts : # unpack the message input artifact # and put it at /tmp/message - name : message path : /tmp/message container : image : alpine:latest command : [ sh , -c ] args : [ \"cat /tmp/message\" ] The whalesay template uses the cowsay command to generate a file named /tmp/hello-world.txt . It then outputs this file as an artifact named hello-art . In general, the artifact's path may be a directory rather than just a file. The print-message template takes an input artifact named message , unpacks it at the path named /tmp/message and then prints the contents of /tmp/message using the cat command. The artifact-example template passes the hello-art artifact generated as an output of the generate-artifact step as the message input artifact to the print-message step. DAG templates use the tasks prefix to refer to another task, for example {{tasks.generate-artifact.outputs.artifacts.hello-art}} . Artifacts are packaged as Tarballs and gzipped by default. You may customize this behavior by specifying an archive strategy, using the archive field. For example: <... snipped ...> outputs : artifacts : # default behavior - tar+gzip default compression. - name : hello-art-1 path : /tmp/hello_world.txt # disable archiving entirely - upload the file / directory as is. # this is useful when the container layout matches the desired target repository layout. - name : hello-art-2 path : /tmp/hello_world.txt archive : none : {} # customize the compression behavior (disabling it here). # this is useful for files with varying compression benefits, # e.g. disabling compression for a cached build workspace and large binaries, # or increasing compression for \"perfect\" textual data - like a json/xml export of a large database. - name : hello-art-3 path : /tmp/hello_world.txt archive : tar : # no compression (also accepts the standard gzip 1 to 9 values) compressionLevel : 0 <... snipped ...>","title":"Artifacts"},{"location":"examples/#the-structure-of-workflow-specs","text":"We now know enough about the basic components of a workflow spec to review its basic structure: Kubernetes header including metadata Spec body Entrypoint invocation with optionally arguments List of template definitions For each template definition Name of the template Optionally a list of inputs Optionally a list of outputs Container invocation (leaf template) or a list of steps For each step, a template invocation To summarize, workflow specs are composed of a set of Argo templates where each template consists of an optional input section, an optional output section and either a container invocation or a list of steps where each step invokes another template. Note that the container section of the workflow spec will accept the same options as the container section of a pod spec, including but not limited to environment variables, secrets, and volume mounts. Similarly, for volume claims and volumes.","title":"The Structure of Workflow Specs"},{"location":"examples/#secrets","text":"Argo supports the same secrets syntax and mechanisms as Kubernetes Pod specs, which allows access to secrets as environment variables or volume mounts. See the Kubernetes documentation for more information. # To run this example, first create the secret by running: # kubectl create secret generic my-secret --from-literal=mypassword=S00perS3cretPa55word apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : secret-example- spec : entrypoint : whalesay # To access secrets as files, add a volume entry in spec.volumes[] and # then in the container template spec, add a mount using volumeMounts. volumes : - name : my-secret-vol secret : secretName : my-secret # name of an existing k8s secret templates : - name : whalesay container : image : alpine:3.7 command : [ sh , -c ] args : [ ' echo \"secret from env: $MYSECRETPASSWORD\"; echo \"secret from file: `cat /secret/mountpath/mypassword`\" ' ] # To access secrets as environment variables, use the k8s valueFrom and # secretKeyRef constructs. env : - name : MYSECRETPASSWORD # name of env var valueFrom : secretKeyRef : name : my-secret # name of an existing k8s secret key : mypassword # 'key' subcomponent of the secret volumeMounts : - name : my-secret-vol # mount file containing secret at /secret/mountpath mountPath : \"/secret/mountpath\"","title":"Secrets"},{"location":"examples/#scripts-results","text":"Often, we just want a template that executes a script specified as a here-script (also known as a here document ) in the workflow spec. This example shows how to do that: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : scripts-bash- spec : entrypoint : bash-script-example templates : - name : bash-script-example steps : - - name : generate template : gen-random-int-bash - - name : print template : print-message arguments : parameters : - name : message value : \"{{steps.generate.outputs.result}}\" # The result of the here-script - name : gen-random-int-bash script : image : debian:9.4 command : [ bash ] source : | # Contents of the here-script cat /dev/urandom | od -N2 -An -i | awk -v f=1 -v r=100 '{printf \"%i\\n\", f + r * $1 / 65536}' - name : gen-random-int-python script : image : python:alpine3.6 command : [ python ] source : | import random i = random.randint(1, 100) print(i) - name : gen-random-int-javascript script : image : node:9.1-alpine command : [ node ] source : | var rand = Math.floor(Math.random() * 100); console.log(rand); - name : print-message inputs : parameters : - name : message container : image : alpine:latest command : [ sh , -c ] args : [ \"echo result was: {{inputs.parameters.message}}\" ] The script keyword allows the specification of the script body using the source tag. This creates a temporary file containing the script body and then passes the name of the temporary file as the final parameter to command , which should be an interpreter that executes the script body. The use of the script feature also assigns the standard output of running the script to a special output parameter named result . This allows you to use the result of running the script itself in the rest of the workflow spec. In this example, the result is simply echoed by the print-message template.","title":"Scripts &amp; Results"},{"location":"examples/#output-parameters","text":"Output parameters provide a general mechanism to use the result of a step as a parameter rather than as an artifact. This allows you to use the result from any type of step, not just a script , for conditional tests, loops, and arguments. Output parameters work similarly to script result except that the value of the output parameter is set to the contents of a generated file rather than the contents of stdout . apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : output-parameter- spec : entrypoint : output-parameter templates : - name : output-parameter steps : - - name : generate-parameter template : whalesay - - name : consume-parameter template : print-message arguments : parameters : # Pass the hello-param output from the generate-parameter step as the message input to print-message - name : message value : \"{{steps.generate-parameter.outputs.parameters.hello-param}}\" - name : whalesay container : image : docker/whalesay:latest command : [ sh , -c ] args : [ \"echo -n hello world > /tmp/hello_world.txt\" ] # generate the content of hello_world.txt outputs : parameters : - name : hello-param # name of output parameter valueFrom : path : /tmp/hello_world.txt # set the value of hello-param to the contents of this hello-world.txt - name : print-message inputs : parameters : - name : message container : image : docker/whalesay:latest command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] DAG templates use the tasks prefix to refer to another task, for example {{tasks.generate-parameter.outputs.parameters.hello-param}} .","title":"Output Parameters"},{"location":"examples/#loops","text":"When writing workflows, it is often very useful to be able to iterate over a set of inputs as shown in this example: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : loops- spec : entrypoint : loop-example templates : - name : loop-example steps : - - name : print-message template : whalesay arguments : parameters : - name : message value : \"{{item}}\" withItems : # invoke whalesay once for each item in parallel - hello world # item 1 - goodbye world # item 2 - name : whalesay inputs : parameters : - name : message container : image : docker/whalesay:latest command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] We can also iterate over sets of items: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : loops-maps- spec : entrypoint : loop-map-example templates : - name : loop-map-example steps : - - name : test-linux template : cat-os-release arguments : parameters : - name : image value : \"{{item.image}}\" - name : tag value : \"{{item.tag}}\" withItems : - { image : 'debian' , tag : '9.1' } #item set 1 - { image : 'debian' , tag : '8.9' } #item set 2 - { image : 'alpine' , tag : '3.6' } #item set 3 - { image : 'ubuntu' , tag : '17.10' } #item set 4 - name : cat-os-release inputs : parameters : - name : image - name : tag container : image : \"{{inputs.parameters.image}}:{{inputs.parameters.tag}}\" command : [ cat ] args : [ /etc/os-release ] We can pass lists of items as parameters: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : loops-param-arg- spec : entrypoint : loop-param-arg-example arguments : parameters : - name : os-list # a list of items value : | [ { \"image\": \"debian\", \"tag\": \"9.1\" }, { \"image\": \"debian\", \"tag\": \"8.9\" }, { \"image\": \"alpine\", \"tag\": \"3.6\" }, { \"image\": \"ubuntu\", \"tag\": \"17.10\" } ] templates : - name : loop-param-arg-example inputs : parameters : - name : os-list steps : - - name : test-linux template : cat-os-release arguments : parameters : - name : image value : \"{{item.image}}\" - name : tag value : \"{{item.tag}}\" withParam : \"{{inputs.parameters.os-list}}\" # parameter specifies the list to iterate over # This template is the same as in the previous example - name : cat-os-release inputs : parameters : - name : image - name : tag container : image : \"{{inputs.parameters.image}}:{{inputs.parameters.tag}}\" command : [ cat ] args : [ /etc/os-release ] We can even dynamically generate the list of items to iterate over! apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : loops-param-result- spec : entrypoint : loop-param-result-example templates : - name : loop-param-result-example steps : - - name : generate template : gen-number-list # Iterate over the list of numbers generated by the generate step above - - name : sleep template : sleep-n-sec arguments : parameters : - name : seconds value : \"{{item}}\" withParam : \"{{steps.generate.outputs.result}}\" # Generate a list of numbers in JSON format - name : gen-number-list script : image : python:alpine3.6 command : [ python ] source : | import json import sys json.dump([i for i in range(20, 31)], sys.stdout) - name : sleep-n-sec inputs : parameters : - name : seconds container : image : alpine:latest command : [ sh , -c ] args : [ \"echo sleeping for {{inputs.parameters.seconds}} seconds; sleep {{inputs.parameters.seconds}}; echo done\" ]","title":"Loops"},{"location":"examples/#conditionals","text":"We also support conditional execution as shown in this example: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : coinflip- spec : entrypoint : coinflip templates : - name : coinflip steps : # flip a coin - - name : flip-coin template : flip-coin # evaluate the result in parallel - - name : heads template : heads # call heads template if \"heads\" when : \"{{steps.flip-coin.outputs.result}} == heads\" - name : tails template : tails # call tails template if \"tails\" when : \"{{steps.flip-coin.outputs.result}} == tails\" # Return heads or tails based on a random number - name : flip-coin script : image : python:alpine3.6 command : [ python ] source : | import random result = \"heads\" if random.randint(0,1) == 0 else \"tails\" print(result) - name : heads container : image : alpine:3.6 command : [ sh , -c ] args : [ \"echo \\\"it was heads\\\"\" ] - name : tails container : image : alpine:3.6 command : [ sh , -c ] args : [ \"echo \\\"it was tails\\\"\" ]","title":"Conditionals"},{"location":"examples/#retrying-failed-or-errored-steps","text":"You can specify a retryStrategy that will dictate how failed or errored steps are retried: # This example demonstrates the use of retry back offs apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : retry-backoff- spec : entrypoint : retry-backoff templates : - name : retry-backoff retryStrategy : limit : 10 retryPolicy : \"Always\" backoff : duration : \"1\" # Must be a string. Default unit is seconds. Could also be a Duration, e.g.: \"2m\", \"6h\", \"1d\" factor : 2 maxDuration : \"1m\" # Must be a string. Default unit is seconds. Could also be a Duration, e.g.: \"2m\", \"6h\", \"1d\" container : image : python:alpine3.6 command : [ \"python\" , -c ] # fail with a 66% probability args : [ \"import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)\" ] limit is the maximum number of times the container will be retried. retryPolicy specifies if a container will be retried on failure, error, or both. \"Always\" retries on both errors and failures. Also available: \"OnFailure\" (default), \"OnError\" backoff is an exponential backoff Providing an empty retryStrategy (i.e. retryStrategy: {} ) will cause a container to retry until completion.","title":"Retrying Failed or Errored Steps"},{"location":"examples/#recursion","text":"Templates can recursively invoke each other! In this variation of the above coin-flip template, we continue to flip coins until it comes up heads. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : coinflip-recursive- spec : entrypoint : coinflip templates : - name : coinflip steps : # flip a coin - - name : flip-coin template : flip-coin # evaluate the result in parallel - - name : heads template : heads # call heads template if \"heads\" when : \"{{steps.flip-coin.outputs.result}} == heads\" - name : tails # keep flipping coins if \"tails\" template : coinflip when : \"{{steps.flip-coin.outputs.result}} == tails\" - name : flip-coin script : image : python:alpine3.6 command : [ python ] source : | import random result = \"heads\" if random.randint(0,1) == 0 else \"tails\" print(result) - name : heads container : image : alpine:3.6 command : [ sh , -c ] args : [ \"echo \\\"it was heads\\\"\" ] Here's the result of a couple of runs of coinflip for comparison. argo get coinflip-recursive-tzcb5 STEP PODNAME MESSAGE \u2714 coinflip-recursive-vhph5 \u251c---\u2714 flip-coin coinflip-recursive-vhph5-2123890397 \u2514-\u00b7-\u2714 heads coinflip-recursive-vhph5-128690560 \u2514-\u25cb tails STEP PODNAME MESSAGE \u2714 coinflip-recursive-tzcb5 \u251c---\u2714 flip-coin coinflip-recursive-tzcb5-322836820 \u2514-\u00b7-\u25cb heads \u2514-\u2714 tails \u251c---\u2714 flip-coin coinflip-recursive-tzcb5-1863890320 \u2514-\u00b7-\u25cb heads \u2514-\u2714 tails \u251c---\u2714 flip-coin coinflip-recursive-tzcb5-1768147140 \u2514-\u00b7-\u25cb heads \u2514-\u2714 tails \u251c---\u2714 flip-coin coinflip-recursive-tzcb5-4080411136 \u2514-\u00b7-\u2714 heads coinflip-recursive-tzcb5-4080323273 \u2514-\u25cb tails In the first run, the coin immediately comes up heads and we stop. In the second run, the coin comes up tail three times before it finally comes up heads and we stop.","title":"Recursion"},{"location":"examples/#exit-handlers","text":"An exit handler is a template that always executes, irrespective of success or failure, at the end of the workflow. Some common use cases of exit handlers are: cleaning up after a workflow runs sending notifications of workflow status (e.g., e-mail/Slack) posting the pass/fail status to a webhook result (e.g. GitHub build result) resubmitting or submitting another workflow apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : exit-handlers- spec : entrypoint : intentional-fail onExit : exit-handler # invoke exit-hander template at end of the workflow templates : # primary workflow template - name : intentional-fail container : image : alpine:latest command : [ sh , -c ] args : [ \"echo intentional failure; exit 1\" ] # Exit handler templates # After the completion of the entrypoint template, the status of the # workflow is made available in the global variable {{workflow.status}}. # {{workflow.status}} will be one of: Succeeded, Failed, Error - name : exit-handler steps : - - name : notify template : send-email - name : celebrate template : celebrate when : \"{{workflow.status}} == Succeeded\" - name : cry template : cry when : \"{{workflow.status}} != Succeeded\" - name : send-email container : image : alpine:latest command : [ sh , -c ] args : [ \"echo send e-mail: {{workflow.name}} {{workflow.status}}\" ] - name : celebrate container : image : alpine:latest command : [ sh , -c ] args : [ \"echo hooray!\" ] - name : cry container : image : alpine:latest command : [ sh , -c ] args : [ \"echo boohoo!\" ]","title":"Exit handlers"},{"location":"examples/#timeouts","text":"To limit the elapsed time for a workflow, you can set the variable activeDeadlineSeconds . # To enforce a timeout for a container template, specify a value for activeDeadlineSeconds. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : timeouts- spec : entrypoint : sleep templates : - name : sleep container : image : alpine:latest command : [ sh , -c ] args : [ \"echo sleeping for 1m; sleep 60; echo done\" ] activeDeadlineSeconds : 10 # terminate container template after 10 seconds","title":"Timeouts"},{"location":"examples/#volumes","text":"The following example dynamically creates a volume and then uses the volume in a two step workflow. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : volumes-pvc- spec : entrypoint : volumes-pvc-example volumeClaimTemplates : # define volume, same syntax as k8s Pod spec - metadata : name : workdir # name of volume claim spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi # Gi => 1024 * 1024 * 1024 templates : - name : volumes-pvc-example steps : - - name : generate template : whalesay - - name : print template : print-message - name : whalesay container : image : docker/whalesay:latest command : [ sh , -c ] args : [ \"echo generating message in volume; cowsay hello world | tee /mnt/vol/hello_world.txt\" ] # Mount workdir volume at /mnt/vol before invoking docker/whalesay volumeMounts : # same syntax as k8s Pod spec - name : workdir mountPath : /mnt/vol - name : print-message container : image : alpine:latest command : [ sh , -c ] args : [ \"echo getting message from volume; find /mnt/vol; cat /mnt/vol/hello_world.txt\" ] # Mount workdir volume at /mnt/vol before invoking docker/whalesay volumeMounts : # same syntax as k8s Pod spec - name : workdir mountPath : /mnt/vol Volumes are a very useful way to move large amounts of data from one step in a workflow to another. Depending on the system, some volumes may be accessible concurrently from multiple steps. In some cases, you want to access an already existing volume rather than creating/destroying one dynamically. # Define Kubernetes PVC kind : PersistentVolumeClaim apiVersion : v1 metadata : name : my-existing-volume spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi --- apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : volumes-existing- spec : entrypoint : volumes-existing-example volumes : # Pass my-existing-volume as an argument to the volumes-existing-example template # Same syntax as k8s Pod spec - name : workdir persistentVolumeClaim : claimName : my-existing-volume templates : - name : volumes-existing-example steps : - - name : generate template : whalesay - - name : print template : print-message - name : whalesay container : image : docker/whalesay:latest command : [ sh , -c ] args : [ \"echo generating message in volume; cowsay hello world | tee /mnt/vol/hello_world.txt\" ] volumeMounts : - name : workdir mountPath : /mnt/vol - name : print-message container : image : alpine:latest command : [ sh , -c ] args : [ \"echo getting message from volume; find /mnt/vol; cat /mnt/vol/hello_world.txt\" ] volumeMounts : - name : workdir mountPath : /mnt/vol It's also possible to declare existing volumes at the template level, instead of the workflow level. This can be useful workflows that generate volumes using a resource step. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : template-level-volume- spec : entrypoint : generate-and-use-volume templates : - name : generate-and-use-volume steps : - - name : generate-volume template : generate-volume arguments : parameters : - name : pvc-size # In a real-world example, this could be generated by a previous workfow step. value : '1Gi' - - name : generate template : whalesay arguments : parameters : - name : pvc-name value : '{{ steps.generate-volume.outputs.parameters.pvc-name }}' - - name : print template : print-message arguments : parameters : - name : pvc-name value : '{{ steps.generate-volume.outputs.parameters.pvc-name }}' - name : generate-volume inputs : parameters : - name : pvc-size resource : action : create setOwnerReference : true manifest : | apiVersion: v1 kind: PersistentVolumeClaim metadata: generateName: pvc-example- spec: accessModes: ['ReadWriteOnce', 'ReadOnlyMany'] resources: requests: storage: '{{inputs.parameters.pvc-size}}' outputs : parameters : - name : pvc-name valueFrom : jsonPath : '{.metadata.name}' - name : whalesay inputs : parameters : - name : pvc-name volumes : - name : workdir persistentVolumeClaim : claimName : '{{inputs.parameters.pvc-name}}' container : image : docker/whalesay:latest command : [ sh , -c ] args : [ \"echo generating message in volume; cowsay hello world | tee /mnt/vol/hello_world.txt\" ] volumeMounts : - name : workdir mountPath : /mnt/vol - name : print-message inputs : parameters : - name : pvc-name volumes : - name : workdir persistentVolumeClaim : claimName : '{{inputs.parameters.pvc-name}}' container : image : alpine:latest command : [ sh , -c ] args : [ \"echo getting message from volume; find /mnt/vol; cat /mnt/vol/hello_world.txt\" ] volumeMounts : - name : workdir mountPath : /mnt/vol","title":"Volumes"},{"location":"examples/#suspending","text":"Workflows can be suspended by argo suspend WORKFLOW Or by specifying a suspend step on the workflow: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : suspend-template- spec : entrypoint : suspend templates : - name : suspend steps : - - name : build template : whalesay - - name : approve template : approve - - name : delay template : delay - - name : release template : whalesay - name : approve suspend : {} - name : delay suspend : duration : 20 # Default unit is seconds. Could also be a Duration, e.g.: \"2m\", \"6h\", \"1d\" - name : whalesay container : image : docker/whalesay command : [ cowsay ] args : [ \"hello world\" ] Once suspended, a Workflow will not schedule any new steps until it is resumed. It can be resumed manually by argo resume WORKFLOW Or automatically with a duration limit as the example above.","title":"Suspending"},{"location":"examples/#daemon-containers","text":"Argo workflows can start containers that run in the background (also known as daemon containers ) while the workflow itself continues execution. Note that the daemons will be automatically destroyed when the workflow exits the template scope in which the daemon was invoked. Daemon containers are useful for starting up services to be tested or to be used in testing (e.g., fixtures). We also find it very useful when running large simulations to spin up a database as a daemon for collecting and organizing the results. The big advantage of daemons compared with sidecars is that their existence can persist across multiple steps or even the entire workflow. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : daemon-step- spec : entrypoint : daemon-example templates : - name : daemon-example steps : - - name : influx template : influxdb # start an influxdb as a daemon (see the influxdb template spec below) - - name : init-database # initialize influxdb template : influxdb-client arguments : parameters : - name : cmd value : curl -XPOST 'http://{{steps.influx.ip}}:8086/query' --data-urlencode \"q=CREATE DATABASE mydb\" - - name : producer-1 # add entries to influxdb template : influxdb-client arguments : parameters : - name : cmd value : for i in $(seq 1 20); do curl -XPOST 'http://{{steps.influx.ip}}:8086/write?db=mydb' -d \"cpu,host=server01,region=uswest load=$i\" ; sleep .5 ; done - name : producer-2 # add entries to influxdb template : influxdb-client arguments : parameters : - name : cmd value : for i in $(seq 1 20); do curl -XPOST 'http://{{steps.influx.ip}}:8086/write?db=mydb' -d \"cpu,host=server02,region=uswest load=$((RANDOM % 100))\" ; sleep .5 ; done - name : producer-3 # add entries to influxdb template : influxdb-client arguments : parameters : - name : cmd value : curl -XPOST 'http://{{steps.influx.ip}}:8086/write?db=mydb' -d 'cpu,host=server03,region=useast load=15.4' - - name : consumer # consume intries from influxdb template : influxdb-client arguments : parameters : - name : cmd value : curl --silent -G http://{{steps.influx.ip}}:8086/query?pretty=true --data-urlencode \"db=mydb\" --data-urlencode \"q=SELECT * FROM cpu\" - name : influxdb daemon : true # start influxdb as a daemon retryStrategy : limit : 10 # retry container if it fails container : image : influxdb:1.2 readinessProbe : # wait for readinessProbe to succeed httpGet : path : /ping port : 8086 - name : influxdb-client inputs : parameters : - name : cmd container : image : appropriate/curl:latest command : [ \"/bin/sh\" , \"-c\" ] args : [ \"{{inputs.parameters.cmd}}\" ] resources : requests : memory : 32Mi cpu : 100m DAG templates use the tasks prefix to refer to another task, for example {{tasks.influx.ip}} .","title":"Daemon Containers"},{"location":"examples/#sidecars","text":"A sidecar is another container that executes concurrently in the same pod as the main container and is useful in creating multi-container pods. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : sidecar-nginx- spec : entrypoint : sidecar-nginx-example templates : - name : sidecar-nginx-example container : image : appropriate/curl command : [ sh , -c ] # Try to read from nginx web server until it comes up args : [ \"until `curl -G 'http://127.0.0.1/' >& /tmp/out`; do echo sleep && sleep 1; done && cat /tmp/out\" ] # Create a simple nginx web server sidecars : - name : nginx image : nginx:1.13 In the above example, we create a sidecar container that runs nginx as a simple web server. The order in which containers come up is random, so in this example the main container polls the nginx container until it is ready to service requests. This is a good design pattern when designing multi-container systems: always wait for any services you need to come up before running your main code.","title":"Sidecars"},{"location":"examples/#hardwired-artifacts","text":"With Argo, you can use any container image that you like to generate any kind of artifact. In practice, however, we find certain types of artifacts are very common, so there is built-in support for git, http, gcs and s3 artifacts. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : hardwired-artifact- spec : entrypoint : hardwired-artifact templates : - name : hardwired-artifact inputs : artifacts : # Check out the master branch of the argo repo and place it at /src # revision can be anything that git checkout accepts: branch, commit, tag, etc. - name : argo-source path : /src git : repo : https://github.com/argoproj/argo.git revision : \"master\" # Download kubectl 1.8.0 and place it at /bin/kubectl - name : kubectl path : /bin/kubectl mode : 0755 http : url : https://storage.googleapis.com/kubernetes-release/release/v1.8.0/bin/linux/amd64/kubectl # Copy an s3 compatible artifact repository bucket (such as AWS, GCS and Minio) and place it at /s3 - name : objects path : /s3 s3 : endpoint : storage.googleapis.com bucket : my-bucket-name key : path/in/bucket accessKeySecret : name : my-s3-credentials key : accessKey secretKeySecret : name : my-s3-credentials key : secretKey container : image : debian command : [ sh , -c ] args : [ \"ls -l /src /bin/kubectl /s3\" ]","title":"Hardwired Artifacts"},{"location":"examples/#kubernetes-resources","text":"In many cases, you will want to manage Kubernetes resources from Argo workflows. The resource template allows you to create, delete or updated any type of Kubernetes resource. # in a workflow. The resource template type accepts any k8s manifest # (including CRDs) and can perform any kubectl action against it (e.g. create, # apply, delete, patch). apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : k8s-jobs- spec : entrypoint : pi-tmpl templates : - name : pi-tmpl resource : # indicates that this is a resource template action : create # can be any kubectl action (e.g. create, delete, apply, patch) # The successCondition and failureCondition are optional expressions. # If failureCondition is true, the step is considered failed. # If successCondition is true, the step is considered successful. # They use kubernetes label selection syntax and can be applied against any field # of the resource (not just labels). Multiple AND conditions can be represented by comma # delimited expressions. # For more details: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ successCondition : status.succeeded > 0 failureCondition : status.failed > 3 manifest : | #put your kubernetes spec here apiVersion: batch/v1 kind: Job metadata: generateName: pi-job- spec: template: metadata: name: pi spec: containers: - name: pi image: perl command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"] restartPolicy: Never backoffLimit: 4 Resources created in this way are independent of the workflow. If you want the resource to be deleted when the workflow is deleted then you can use Kubernetes garbage collection with the workflow resource as an owner reference ( example ). Note: When patching, the resource will accept another attribute, mergeStrategy , which can either be strategic , merge , or json . If this attribute is not supplied, it will default to strategic . Keep in mind that Custom Resources cannot be patched with strategic , so a different strategy must be chosen. For example, suppose you have the CronTab CustomResourceDefinition defined, and the following instance of a CronTab: apiVersion : \"stable.example.com/v1\" kind : CronTab spec : cronSpec : \"* * * * */5\" image : my-awesome-cron-image This Crontab can be modified using the following Argo Workflow: apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : k8s-patch- spec : entrypoint : cront-tmpl templates : - name : cront-tmpl resource : action : patch mergeStrategy : merge # Must be one of [strategic merge json] manifest : | apiVersion: \"stable.example.com/v1\" kind: CronTab spec: cronSpec: \"* * * * */10\" image: my-awesome-cron-image","title":"Kubernetes Resources"},{"location":"examples/#docker-in-docker-using-sidecars","text":"An application of sidecars is to implement Docker-in-Docker (DinD). DinD is useful when you want to run Docker commands from inside a container. For example, you may want to build and push a container image from inside your build container. In the following example, we use the docker:dind container to run a Docker daemon in a sidecar and give the main container access to the daemon. apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : sidecar-dind- spec : entrypoint : dind-sidecar-example templates : - name : dind-sidecar-example container : image : docker:17.10 command : [ sh , -c ] args : [ \"until docker ps; do sleep 3; done; docker run --rm debian:latest cat /etc/os-release\" ] env : - name : DOCKER_HOST # the docker daemon can be access on the standard port on localhost value : 127.0.0.1 sidecars : - name : dind image : docker:17.10-dind # Docker already provides an image for running a Docker daemon securityContext : privileged : true # the Docker daemon can only run in a privileged container # mirrorVolumeMounts will mount the same volumes specified in the main container # to the sidecar (including artifacts), at the same mountPaths. This enables # dind daemon to (partially) see the same filesystem as the main container in # order to use features such as docker volume binding. mirrorVolumeMounts : true","title":"Docker-in-Docker Using Sidecars"},{"location":"examples/#custom-template-variable-reference","text":"In this example, we can see how we can use the other template language variable reference (E.g: Jinja) in Argo workflow template. Argo will validate and resolve only the variable that starts with Argo allowed prefix { \"item\", \"steps\", \"inputs\", \"outputs\", \"workflow\", \"tasks\" } apiVersion : argoproj.io/v1alpha1 kind : Workflow metadata : generateName : custom-template-variable- spec : entrypoint : hello-hello-hello templates : - name : hello-hello-hello steps : - - name : hello1 template : whalesay arguments : parameters : [{ name : message , value : \"hello1\" }] - - name : hello2a template : whalesay arguments : parameters : [{ name : message , value : \"hello2a\" }] - name : hello2b template : whalesay arguments : parameters : [{ name : message , value : \"hello2b\" }] - name : whalesay inputs : parameters : - name : message container : image : docker/whalesay command : [ cowsay ] args : [ \"{{user.username}}\" ]","title":"Custom Template Variable Reference"},{"location":"examples/#continuous-integration-example","text":"Continuous integration is a popular application for workflows. Currently, Argo does not provide event triggers for automatically kicking off your CI jobs, but we plan to do so in the near future. Until then, you can easily write a cron job that checks for new commits and kicks off the needed workflow, or use your existing Jenkins server to kick off the workflow. A good example of a CI workflow spec is provided at https://github.com/argoproj/argo/tree/master/examples/influxdb-ci.yaml. Because it just uses the concepts that we've already covered and is somewhat long, we don't go into details here.","title":"Continuous Integration Example"}]}
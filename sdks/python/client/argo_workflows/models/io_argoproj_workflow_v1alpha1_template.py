# coding: utf-8

"""
    Argo Workflows API

    Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. For more information, please see https://argo-workflows.readthedocs.io/en/latest/

    The version of the OpenAPI document: VERSION
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from argo_workflows.models.affinity import Affinity
from argo_workflows.models.container import Container
from argo_workflows.models.host_alias import HostAlias
from argo_workflows.models.io_argoproj_workflow_v1alpha1_artifact_location import IoArgoprojWorkflowV1alpha1ArtifactLocation
from argo_workflows.models.io_argoproj_workflow_v1alpha1_container_set_template import IoArgoprojWorkflowV1alpha1ContainerSetTemplate
from argo_workflows.models.io_argoproj_workflow_v1alpha1_data import IoArgoprojWorkflowV1alpha1Data
from argo_workflows.models.io_argoproj_workflow_v1alpha1_executor_config import IoArgoprojWorkflowV1alpha1ExecutorConfig
from argo_workflows.models.io_argoproj_workflow_v1alpha1_http import IoArgoprojWorkflowV1alpha1HTTP
from argo_workflows.models.io_argoproj_workflow_v1alpha1_inputs import IoArgoprojWorkflowV1alpha1Inputs
from argo_workflows.models.io_argoproj_workflow_v1alpha1_memoize import IoArgoprojWorkflowV1alpha1Memoize
from argo_workflows.models.io_argoproj_workflow_v1alpha1_metadata import IoArgoprojWorkflowV1alpha1Metadata
from argo_workflows.models.io_argoproj_workflow_v1alpha1_metrics import IoArgoprojWorkflowV1alpha1Metrics
from argo_workflows.models.io_argoproj_workflow_v1alpha1_outputs import IoArgoprojWorkflowV1alpha1Outputs
from argo_workflows.models.io_argoproj_workflow_v1alpha1_parallel_steps import IoArgoprojWorkflowV1alpha1ParallelSteps
from argo_workflows.models.io_argoproj_workflow_v1alpha1_resource_template import IoArgoprojWorkflowV1alpha1ResourceTemplate
from argo_workflows.models.io_argoproj_workflow_v1alpha1_retry_strategy import IoArgoprojWorkflowV1alpha1RetryStrategy
from argo_workflows.models.io_argoproj_workflow_v1alpha1_script_template import IoArgoprojWorkflowV1alpha1ScriptTemplate
from argo_workflows.models.io_argoproj_workflow_v1alpha1_suspend_template import IoArgoprojWorkflowV1alpha1SuspendTemplate
from argo_workflows.models.io_argoproj_workflow_v1alpha1_synchronization import IoArgoprojWorkflowV1alpha1Synchronization
from argo_workflows.models.io_argoproj_workflow_v1alpha1_user_container import IoArgoprojWorkflowV1alpha1UserContainer
from argo_workflows.models.pod_security_context import PodSecurityContext
from argo_workflows.models.toleration import Toleration
from argo_workflows.models.volume import Volume
from typing import Optional, Set
from typing_extensions import Self

class IoArgoprojWorkflowV1alpha1Template(BaseModel):
    """
    Template is a reusable and composable unit of execution in a workflow
    """ # noqa: E501
    active_deadline_seconds: Optional[StrictStr] = Field(default=None, alias="activeDeadlineSeconds")
    affinity: Optional[Affinity] = None
    archive_location: Optional[IoArgoprojWorkflowV1alpha1ArtifactLocation] = Field(default=None, alias="archiveLocation")
    automount_service_account_token: Optional[StrictBool] = Field(default=None, description="AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in pods. ServiceAccountName of ExecutorConfig must be specified if this value is false.", alias="automountServiceAccountToken")
    container: Optional[Container] = None
    container_set: Optional[IoArgoprojWorkflowV1alpha1ContainerSetTemplate] = Field(default=None, alias="containerSet")
    daemon: Optional[StrictBool] = Field(default=None, description="Daemon will allow a workflow to proceed to the next step so long as the container reaches readiness")
    dag: Optional[IoArgoprojWorkflowV1alpha1DAGTemplate] = None
    data: Optional[IoArgoprojWorkflowV1alpha1Data] = None
    executor: Optional[IoArgoprojWorkflowV1alpha1ExecutorConfig] = None
    fail_fast: Optional[StrictBool] = Field(default=None, description="FailFast, if specified, will fail this template if any of its child pods has failed. This is useful for when this template is expanded with `withItems`, etc.", alias="failFast")
    host_aliases: Optional[List[HostAlias]] = Field(default=None, description="HostAliases is an optional list of hosts and IPs that will be injected into the pod spec", alias="hostAliases")
    http: Optional[IoArgoprojWorkflowV1alpha1HTTP] = None
    init_containers: Optional[List[IoArgoprojWorkflowV1alpha1UserContainer]] = Field(default=None, description="InitContainers is a list of containers which run before the main container.", alias="initContainers")
    inputs: Optional[IoArgoprojWorkflowV1alpha1Inputs] = None
    memoize: Optional[IoArgoprojWorkflowV1alpha1Memoize] = None
    metadata: Optional[IoArgoprojWorkflowV1alpha1Metadata] = None
    metrics: Optional[IoArgoprojWorkflowV1alpha1Metrics] = None
    name: Optional[StrictStr] = Field(default=None, description="Name is the name of the template")
    node_selector: Optional[Dict[str, StrictStr]] = Field(default=None, description="NodeSelector is a selector to schedule this step of the workflow to be run on the selected node(s). Overrides the selector set at the workflow level.", alias="nodeSelector")
    outputs: Optional[IoArgoprojWorkflowV1alpha1Outputs] = None
    parallelism: Optional[StrictInt] = Field(default=None, description="Parallelism limits the max total parallel pods that can execute at the same time within the boundaries of this template invocation. If additional steps/dag templates are invoked, the pods created by those templates will not be counted towards this total.")
    plugin: Optional[Dict[str, Any]] = Field(default=None, description="Plugin is an Object with exactly one key")
    pod_spec_patch: Optional[StrictStr] = Field(default=None, description="PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of container fields which are not strings (e.g. resource limits).", alias="podSpecPatch")
    priority: Optional[StrictInt] = Field(default=None, description="Priority to apply to workflow pods.")
    priority_class_name: Optional[StrictStr] = Field(default=None, description="PriorityClassName to apply to workflow pods.", alias="priorityClassName")
    resource: Optional[IoArgoprojWorkflowV1alpha1ResourceTemplate] = None
    retry_strategy: Optional[IoArgoprojWorkflowV1alpha1RetryStrategy] = Field(default=None, alias="retryStrategy")
    scheduler_name: Optional[StrictStr] = Field(default=None, description="If specified, the pod will be dispatched by specified scheduler. Or it will be dispatched by workflow scope scheduler if specified. If neither specified, the pod will be dispatched by default scheduler.", alias="schedulerName")
    script: Optional[IoArgoprojWorkflowV1alpha1ScriptTemplate] = None
    security_context: Optional[PodSecurityContext] = Field(default=None, alias="securityContext")
    service_account_name: Optional[StrictStr] = Field(default=None, description="ServiceAccountName to apply to workflow pods", alias="serviceAccountName")
    sidecars: Optional[List[IoArgoprojWorkflowV1alpha1UserContainer]] = Field(default=None, description="Sidecars is a list of containers which run alongside the main container Sidecars are automatically killed when the main container completes")
    steps: Optional[List[IoArgoprojWorkflowV1alpha1ParallelSteps]] = Field(default=None, description="Steps define a series of sequential/parallel workflow steps")
    suspend: Optional[IoArgoprojWorkflowV1alpha1SuspendTemplate] = None
    synchronization: Optional[IoArgoprojWorkflowV1alpha1Synchronization] = None
    timeout: Optional[StrictStr] = Field(default=None, description="Timeout allows to set the total node execution timeout duration counting from the node's start time. This duration also includes time in which the node spends in Pending state. This duration may not be applied to Step or DAG templates.")
    tolerations: Optional[List[Toleration]] = Field(default=None, description="Tolerations to apply to workflow pods.")
    volumes: Optional[List[Volume]] = Field(default=None, description="Volumes is a list of volumes that can be mounted by containers in a template.")
    __properties: ClassVar[List[str]] = ["activeDeadlineSeconds", "affinity", "archiveLocation", "automountServiceAccountToken", "container", "containerSet", "daemon", "dag", "data", "executor", "failFast", "hostAliases", "http", "initContainers", "inputs", "memoize", "metadata", "metrics", "name", "nodeSelector", "outputs", "parallelism", "plugin", "podSpecPatch", "priority", "priorityClassName", "resource", "retryStrategy", "schedulerName", "script", "securityContext", "serviceAccountName", "sidecars", "steps", "suspend", "synchronization", "timeout", "tolerations", "volumes"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of IoArgoprojWorkflowV1alpha1Template from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of affinity
        if self.affinity:
            _dict['affinity'] = self.affinity.to_dict()
        # override the default output from pydantic by calling `to_dict()` of archive_location
        if self.archive_location:
            _dict['archiveLocation'] = self.archive_location.to_dict()
        # override the default output from pydantic by calling `to_dict()` of container
        if self.container:
            _dict['container'] = self.container.to_dict()
        # override the default output from pydantic by calling `to_dict()` of container_set
        if self.container_set:
            _dict['containerSet'] = self.container_set.to_dict()
        # override the default output from pydantic by calling `to_dict()` of dag
        if self.dag:
            _dict['dag'] = self.dag.to_dict()
        # override the default output from pydantic by calling `to_dict()` of data
        if self.data:
            _dict['data'] = self.data.to_dict()
        # override the default output from pydantic by calling `to_dict()` of executor
        if self.executor:
            _dict['executor'] = self.executor.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in host_aliases (list)
        _items = []
        if self.host_aliases:
            for _item in self.host_aliases:
                if _item:
                    _items.append(_item.to_dict())
            _dict['hostAliases'] = _items
        # override the default output from pydantic by calling `to_dict()` of http
        if self.http:
            _dict['http'] = self.http.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in init_containers (list)
        _items = []
        if self.init_containers:
            for _item in self.init_containers:
                if _item:
                    _items.append(_item.to_dict())
            _dict['initContainers'] = _items
        # override the default output from pydantic by calling `to_dict()` of inputs
        if self.inputs:
            _dict['inputs'] = self.inputs.to_dict()
        # override the default output from pydantic by calling `to_dict()` of memoize
        if self.memoize:
            _dict['memoize'] = self.memoize.to_dict()
        # override the default output from pydantic by calling `to_dict()` of metadata
        if self.metadata:
            _dict['metadata'] = self.metadata.to_dict()
        # override the default output from pydantic by calling `to_dict()` of metrics
        if self.metrics:
            _dict['metrics'] = self.metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of outputs
        if self.outputs:
            _dict['outputs'] = self.outputs.to_dict()
        # override the default output from pydantic by calling `to_dict()` of resource
        if self.resource:
            _dict['resource'] = self.resource.to_dict()
        # override the default output from pydantic by calling `to_dict()` of retry_strategy
        if self.retry_strategy:
            _dict['retryStrategy'] = self.retry_strategy.to_dict()
        # override the default output from pydantic by calling `to_dict()` of script
        if self.script:
            _dict['script'] = self.script.to_dict()
        # override the default output from pydantic by calling `to_dict()` of security_context
        if self.security_context:
            _dict['securityContext'] = self.security_context.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in sidecars (list)
        _items = []
        if self.sidecars:
            for _item in self.sidecars:
                if _item:
                    _items.append(_item.to_dict())
            _dict['sidecars'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in steps (list)
        _items = []
        if self.steps:
            for _item in self.steps:
                if _item:
                    _items.append(_item.to_dict())
            _dict['steps'] = _items
        # override the default output from pydantic by calling `to_dict()` of suspend
        if self.suspend:
            _dict['suspend'] = self.suspend.to_dict()
        # override the default output from pydantic by calling `to_dict()` of synchronization
        if self.synchronization:
            _dict['synchronization'] = self.synchronization.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in tolerations (list)
        _items = []
        if self.tolerations:
            for _item in self.tolerations:
                if _item:
                    _items.append(_item.to_dict())
            _dict['tolerations'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in volumes (list)
        _items = []
        if self.volumes:
            for _item in self.volumes:
                if _item:
                    _items.append(_item.to_dict())
            _dict['volumes'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of IoArgoprojWorkflowV1alpha1Template from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "activeDeadlineSeconds": obj.get("activeDeadlineSeconds"),
            "affinity": Affinity.from_dict(obj["affinity"]) if obj.get("affinity") is not None else None,
            "archiveLocation": IoArgoprojWorkflowV1alpha1ArtifactLocation.from_dict(obj["archiveLocation"]) if obj.get("archiveLocation") is not None else None,
            "automountServiceAccountToken": obj.get("automountServiceAccountToken"),
            "container": Container.from_dict(obj["container"]) if obj.get("container") is not None else None,
            "containerSet": IoArgoprojWorkflowV1alpha1ContainerSetTemplate.from_dict(obj["containerSet"]) if obj.get("containerSet") is not None else None,
            "daemon": obj.get("daemon"),
            "dag": IoArgoprojWorkflowV1alpha1DAGTemplate.from_dict(obj["dag"]) if obj.get("dag") is not None else None,
            "data": IoArgoprojWorkflowV1alpha1Data.from_dict(obj["data"]) if obj.get("data") is not None else None,
            "executor": IoArgoprojWorkflowV1alpha1ExecutorConfig.from_dict(obj["executor"]) if obj.get("executor") is not None else None,
            "failFast": obj.get("failFast"),
            "hostAliases": [HostAlias.from_dict(_item) for _item in obj["hostAliases"]] if obj.get("hostAliases") is not None else None,
            "http": IoArgoprojWorkflowV1alpha1HTTP.from_dict(obj["http"]) if obj.get("http") is not None else None,
            "initContainers": [IoArgoprojWorkflowV1alpha1UserContainer.from_dict(_item) for _item in obj["initContainers"]] if obj.get("initContainers") is not None else None,
            "inputs": IoArgoprojWorkflowV1alpha1Inputs.from_dict(obj["inputs"]) if obj.get("inputs") is not None else None,
            "memoize": IoArgoprojWorkflowV1alpha1Memoize.from_dict(obj["memoize"]) if obj.get("memoize") is not None else None,
            "metadata": IoArgoprojWorkflowV1alpha1Metadata.from_dict(obj["metadata"]) if obj.get("metadata") is not None else None,
            "metrics": IoArgoprojWorkflowV1alpha1Metrics.from_dict(obj["metrics"]) if obj.get("metrics") is not None else None,
            "name": obj.get("name"),
            "nodeSelector": obj.get("nodeSelector"),
            "outputs": IoArgoprojWorkflowV1alpha1Outputs.from_dict(obj["outputs"]) if obj.get("outputs") is not None else None,
            "parallelism": obj.get("parallelism"),
            "plugin": obj.get("plugin"),
            "podSpecPatch": obj.get("podSpecPatch"),
            "priority": obj.get("priority"),
            "priorityClassName": obj.get("priorityClassName"),
            "resource": IoArgoprojWorkflowV1alpha1ResourceTemplate.from_dict(obj["resource"]) if obj.get("resource") is not None else None,
            "retryStrategy": IoArgoprojWorkflowV1alpha1RetryStrategy.from_dict(obj["retryStrategy"]) if obj.get("retryStrategy") is not None else None,
            "schedulerName": obj.get("schedulerName"),
            "script": IoArgoprojWorkflowV1alpha1ScriptTemplate.from_dict(obj["script"]) if obj.get("script") is not None else None,
            "securityContext": PodSecurityContext.from_dict(obj["securityContext"]) if obj.get("securityContext") is not None else None,
            "serviceAccountName": obj.get("serviceAccountName"),
            "sidecars": [IoArgoprojWorkflowV1alpha1UserContainer.from_dict(_item) for _item in obj["sidecars"]] if obj.get("sidecars") is not None else None,
            "steps": [IoArgoprojWorkflowV1alpha1ParallelSteps.from_dict(_item) for _item in obj["steps"]] if obj.get("steps") is not None else None,
            "suspend": IoArgoprojWorkflowV1alpha1SuspendTemplate.from_dict(obj["suspend"]) if obj.get("suspend") is not None else None,
            "synchronization": IoArgoprojWorkflowV1alpha1Synchronization.from_dict(obj["synchronization"]) if obj.get("synchronization") is not None else None,
            "timeout": obj.get("timeout"),
            "tolerations": [Toleration.from_dict(_item) for _item in obj["tolerations"]] if obj.get("tolerations") is not None else None,
            "volumes": [Volume.from_dict(_item) for _item in obj["volumes"]] if obj.get("volumes") is not None else None
        })
        return _obj

from argo_workflows.models.io_argoproj_workflow_v1alpha1_dag_template import IoArgoprojWorkflowV1alpha1DAGTemplate
# TODO: Rewrite to not use raise_errors
IoArgoprojWorkflowV1alpha1Template.model_rebuild(raise_errors=False)

